\documentclass{article}

\usepackage{amsmath,amssymb,amsthm}
\usepackage[shortlabels]{enumitem}
\usepackage{physics}

\newtheorem{theorem}{Theorem}
\numberwithin{theorem}{section}
\newtheorem*{theorem*}{Theorem}
\newtheorem{corollary}{Corollary}
\numberwithin{corollary}{section}
\newtheorem*{corollary*}{Corollary}
\newtheorem{postulate}{Postulate}
\numberwithin{postulate}{section}
\newtheorem*{postulate*}{Postulate}

\DeclareMathOperator{\E}{\mathbb{E}}

\title{Notes to ``Quantum Computation and Quantum Information'', 10th
anniversary edition (Nielsen, Chung)}
\author{Ng Wei En}

\begin{document}

\maketitle
\tableofcontents
\newpage

\setcounter{section}{1}
\section{Introduction to quantum mechanics}

\subsection{Linear algebra}

\subsubsection{Bases and linear independence}

\paragraph{Spanning set.} A \emph{spanning set} for a vector space is a set of
vectors $\ket{v_1}, \ldots, \ket{v_n}$ such that any vector $\ket{v}$ in the
vector space can be written as a linear combination $\ket{v} = \sum_i
a_i\ket{v_i}$ of vectors in that set.

\paragraph{Linear dependence.} A set of non-zero vectors $\ket{v_1}, \ldots,
\ket{v_n}$ are \emph{linearly dependent} if there exists a set of complex
numbers $a_1, \ldots, a_n$ with $a_i \neq 0$ for at least one value of $i$,
such that \[
  a_1\ket{v_1} + a_2\ket{v_2} + \cdots + a_n\ket{v_n} = 0.
\]

\paragraph{Basis.} A set of linearly independent vectors which span a vector
space $V$ is called the \emph{basis} for $V$. Such a basis set always exists,
and all bases for $V$ have the same number of elements, defined to be the
\emph{dimension} of $V$.

\subsubsection{Linear operators and matrices}

\paragraph{Linear operator.} A \emph{linear operator} between vector spaces $V$
and $W$ is defined to be any function $A: V \to W$ which is linear in its
inputs, \[
  A\sum_i a_i\ket{v_i} = \sum_i a_iA\ket{v_i}.
\] Suppose $\ket{v_1}, \ldots, \ket{v_m}$ is a basis for $V$ and $\ket{w_1},
\ldots, \ket{v_n}$ is a basis for $W$. Then for each $j$ in the range $1,
\ldots, m$, there exist complex numbers $A_{1j}$ through $A_{nj}$ such that \[
  A\ket{v_j} = \sum_i A_{ij}\ket{w_i}.
\] The matrix whose entries are the values $A_{ij}$ is said to form a
\emph{matrix representation} of the operator $A$.

\subsubsection{The Pauli matrices}

We define the following matrices, known as the Pauli matrices:
\begin{align*}
  \sigma_0 \equiv I \equiv
  \begin{bmatrix}
    1 & 0 \\
    0 & 1
  \end{bmatrix}&
  &\sigma_1 \equiv \sigma_x \equiv X \equiv
  \begin{bmatrix}
    0 & 1 \\
    1 & 0
  \end{bmatrix} \\
  \sigma_2 \equiv \sigma_y \equiv Y \equiv
  \begin{bmatrix}
    0 & -i \\
    i & 0
  \end{bmatrix}&
  &\sigma_3 \equiv \sigma_z \equiv Z \equiv
  \begin{bmatrix}
    1 & 0 \\
    0 & -1
  \end{bmatrix}
\end{align*}

\subsubsection{Inner products}

\paragraph{Inner product.} A function $(\cdot, \cdot)$ from $V \times V$ to
$\mathbb{C}$ is an \emph{inner product} if it satisfies the requirements that:
\begin{enumerate}[(1)]
  \item $(\cdot, \cdot)$ is linear in the second argument, \[
      \left(\ket{v}, \sum_i \lambda_i\ket{w_i}\right)
      = \sum_i \lambda_i(\ket{v}, \ket{w_i}).
    \]
  \item $(\ket{v}, \ket{w}) = (\ket{w}, \ket{v})^*$.
  \item $(\ket{v}, \ket{v}) \geq 0$ with equality if and only if $\ket{v} = 0$.
\end{enumerate}

\paragraph{Orthogonality.} Vectors $\ket{w}$ and $\ket{v}$ are
\emph{orthogonal} if $\ip{w}{v} = 0$.

\paragraph{Norm.} We define the \emph{norm} of a vector $\ket{v}$ by \[
  \|\ket{v}\| \equiv \sqrt{\ip{v}}.
\] A unit vector is a vector $\ket{v}$ with norm 1.

\paragraph{Orthonormality.} A set $\ket{i}$ of vectors with index $i$ is
\emph{orthonormal} if each vector is a unit vector, and distinct vectors in the
set are orthogonal, that is, $\ip{i}{j} = \delta_{ij}$.

\paragraph{Gram-Schmidt procedure.} Suppose $\ket{w_1}, \ldots, \ket{w_d}$ is a
basis set for some vector space $V$ with an inner product. Define $\ket{v_1}
\equiv \ket{w_1}/\|\ket{w_1}\|$, and for $1 \leq k \leq d - 1$ define
$\ket{v_{k+1}}$ inductively by \[
  \ket{v_{k+1}} \equiv \frac{
    \ket{w_{k+1}} - \sum_{i=1}^k \ip{v_i}{w_{k+1}}\ket{v_i}
  }{
    \|\ket{w_{k+1}} - \sum_{i=1}^k \ip{v_i}{w_{k+1}}\ket{v_i}\|
  }.
\] The vectors $\ket{v_1}, \ldots, \ket{v_d}$ form an orthonormal set which is
also a basis for $V$.

\paragraph{Inner product on a Hilbert space.} Let $\ket{w} = \sum_i w_i\ket{i}$
and $\ket{v} = \sum_j v_j\ket{j}$ be representations of vectors $\ket{w}$ and
$\ket{v}$ with respect to some orthonormal basis $\ket{i}$. Then, since
$\ip{i}{j} = \delta_{ij}$,
\begin{align*}
  \ip{v}{w}
  &= \left(\sum_i v_i\ket{i}, \sum_j w_j\ket{j}\right)
  = \sum_{ij} v_i^*w_j\delta_{ij}
  = \sum_i v_i^*w_i \\
  &=
  \begin{bmatrix}
    v_1^* & \cdots & v_n^*
  \end{bmatrix}
  \begin{bmatrix}
    w_1 \\
    \vdots \\
    w_n
  \end{bmatrix}.
\end{align*}

\paragraph{Outer product.} Suppose $\ket{v}$ is a vector in an inner product
space $V$, and $\ket{w}$ is a vector in an inner product space $W$. Define
$\op{w}{v}$ to be the linear operator from $V$ to $W$ whose action is defined
by \[
  (\op{w}{v})(\ket{v'}) \equiv \ket{w}\ip{v}{v'} = \ip{v}{v'}\ket{w}.
\]

\paragraph{Completeness relation.} Let $\ket{i}$ be any orthonormal basis for
the vector space $V$, so an arbitrary vector $\ket{v}$ can be written $\ket{v}
= \sum_i v_i\ket{i}$ for some set of complex numbers $v_i$. Note that
$\ip{i}{v} = v_i$ and therefore \[
  \left(\sum_i \op{i}{i}\right)\ket{v} = \sum_i \ket{i}\ip{i}{v} =
  \sum_i v_i\ket{i} = \ket{v}.
\] Since the last equation is true for all $\ket{v}$ it follows that \[
  \sum_i \op{i}{i} = I.
\]
This equation is known as the \emph{completeness relation}.

Suppose $A: V \to W$ is a linear operator, $\ket{v_i}$ is an orthonormal basis
for $V$, and $\ket{w_j}$ an orthonormal basis for $W$. Using the completeness
relation twice we obtain
\begin{align*}
  A &= I_WAI_V \\
    &= \sum_{ij} \op{w_j}{w_j}A\op{v_i}{v_i} \\
    &= \sum_{ij} \mel{w_j}{A}{v_i}\op{w_j}{v_i}.
\end{align*}
which is the outer product representation for $A$. We also see that $A$ has
matrix element $\mel{w_j}{A}{v_i}$ in the $i$th column and $j$th row, with
respect to the input basis $\ket{v_i}$ and output basis $\ket{w_j}$.

\paragraph{Cauchy-Schwarz inequality.} The \emph{Cauchy-Schwarz inequality}
states that for any two vectors $\ket{v}$ and $\ket{w}$, $|\ip{v}{w}|^2 \leq
\ip{v}\ip{w}$.

To see this, use the Gram-Schmidt procedure to construct an orthonormal basis
$\ket{i}$ for the vector space such that the first member of the basis
$\ket{i}$ is $\ket{w}/\sqrt{\ip{w}}$. Using the completeness relation $\sum_i
\ip{i} = I$, and dropping some non-negative terms gives
\begin{align*}
  \ip{v}\ip{w}
  &= \sum_i \ip{v}{i}\ip{i}{v}\ip{w} \\
  &\geq \frac{\ip{v}{w}\ip{w}{v}}{\ip{w}}\ip{w} \\
  &= \ip{v}{w}\ip{w}{v} = |\ip{v}{w}|^2,
\end{align*}
as required. Equality occurs if and only if $\ket{v}$ and $\ket{w}$ are
linearly related.

\subsubsection{Eigenvectors and eigenvalues}

\paragraph{Eigenvectors and eigenvalues.} An \emph{eigenvector} of a linear
operator $A$ on a vector space is a non-zero vector $\ket{v}$ such that
$A\ket{v} = v\ket{v}$, where $v$ is a complex number known as the
\emph{eigenvalue} of $A$ corresponding to $\ket{v}$.

\paragraph{Characteristic equation.} The \emph{characteristic function} is
defined to be $c(\lambda) \equiv \det|A - \lambda I|$, where $\det$ is the
\emph{determinant} function for matrices; it can be shown that the
characteristic function depends only upon the operator $A$, and not on the
specific matrix representation used for $A$. The solutions of the
\emph{charateristic equation} $c(\lambda) = 0$ are the eigenvalues of the
operator $A$.

\paragraph{Eigenspace.} The \emph{eigenspace} corresponding to an eigenvalue
$v$ is the set of vectors which have eigenvalue $v$. When an eigenspace is more
than one dimensional we say that it is \emph{degenerate}.

\paragraph{Diagonal representation.} A \emph{diagonal representation} for an
operator $A$ on a vector space $V$ is a representation $A = \sum_i
\lambda_i\op{i}{i}$, where the vectors $\ket{i}$ form an orthonormal set of
eigenvectors for $A$, with corresponding eigenvalues $\lambda_i$.

\subsubsection{Adjoints and Hermitian operators}

\paragraph{Adjoint.} Supose $A$ is any linear operator on a Hilbert space, $V$.
There exists a unique linear operator $A^{\dagger}$ on $V$ such that for all
vectors $\ket{v}, \ket{w} \in V$, \[
  (\ket{v}, A\ket{w}) = (A^{\dagger}\ket{v}, \ket{w}).
\] If $\ket{v}$ is a vector, then we define $\ket{v}^{\dagger} = \bra{v}$.

In matrix representation of the operator $A$, the action of the Hermitian
conjugation operator is to take the matrix of $A$ to the conjugate-transpose
matrix, $A^{\dagger} = (A^*)^T$.

\paragraph{Hermitian operators.} An operator $A$ whose adjoint is $A$ is known
as a \emph{Hermitian} or \emph{self-adjoint} operator.

\paragraph{Projectors.} Suppose $W$ is a $k$-dimensional vector subspace of the
$d$-dimensional vector space $V$. Using the Gram-Schmidt procedure it is
possible to construct an orthonormal basis $\ket{1}, \ldots, \ket{d}$ for $V$
such that $\ket{1}, \ldots, \ket{k}$ is an orthonormal basis for $W$. By
definition, \[
  P \equiv \sum_{i=1}^k \op{i}{i}
\] is the projector onto the subspace $W$. This definition is independent of
the orthonormal basis $\ket{1}, \ldots, \ket{k}$ used for $W$. As $\op{v}{v}$
is Hermitian for any vector $\ket{v}$, $P$ is Hermitian too. The
\emph{orthogonal complement} of $P$ is the operator $Q \equiv I - P$, which is
a projector onto the vector space spanned by $\ket{k+1}, \ldots, \ket{d}$.

\paragraph{Normal operators.} An operator $A$ is said to be \emph{normal} if
$AA^{\dagger} = A^{\dagger}A$. Clearly, an operator which is Hermitian is also
normal.

\begin{theorem}[Spectral decomposition]
  Any normal operator $M$ on a vector space $V$ is diagonal with respect to
  some orthonormal basis for $V$. Conversely, any diagonalizable operator is
  normal.
\end{theorem}
\begin{proof}
  The converse is a simple exercise, so we prove merely the forward
  implication, by induction on the dimension $d$ of $V$. The case $d = 1$ is
  trivial. Let $\lambda$ be an eigenvalue of $M$, $P$ the projector onto the
  $\lambda$ eigenspace, and $Q$ the projector onto the orthogonal complement.
  Then $M = (P + Q)M(P + Q) = PMP + QMP + PMQ + QMQ$.

  As $MP\ket{v} = \lambda P \ket{v}$ for any vector $\ket{v}$, $PMP = P(\lambda
  P) = \lambda P^2 = \lambda P$. Furthermore, $QMP = (I - P)MP = MP - PMP =
  \lambda P - \lambda P = 0$.

  We claim that $PMQ = 0$ also. To see this, let $\ket{v}$ be an element of the
  subspace $P$. Then $M M^\dagger \ket{v} = M^\dagger M \ket{v} = \lambda
  M^\dagger \ket{v}$. Thus, $M^\dagger \ket{v}$ has eigenvalue $\lambda$ and
  therefore is an element of the subspace $P$. It follows that $Q M^\dagger P =
  0$. Taking the adjoint of this equation gives $PMQ = 0$, as the projectors
  are Hermitian i.e. $P = P^\dagger$ and $Q = Q^\dagger$. Thus $M = PMP + QMQ$.

  Next, we prove that $QMQ$ is normal. To see this, note that $QM = QM(P + Q) =
  QMQ$, and $Q M^\dagger = Q M^\dagger (P+Q) = Q M^\dagger Q$. Therefore, by
  the normality of $M$, and the observation that $Q^2 = Q$, \begin{align*}
    Q M Q Q M^\dagger Q
    &= Q M Q M^\dagger Q \\
    &= Q M M^\dagger Q \\
    &= Q M^\dagger M Q \\
    &= Q M^\dagger Q M Q \\
    &= Q M^\dagger Q Q M Q,
  \end{align*} so $QMQ$ is normal.

  By induction, $QMQ$ is diagonal with respect to some orthonormal basis for
  the subspace $Q$, and $PMP$ is already diagonal with respect to some
  orthonormal basis for the subspace $P$. It follows that $M = PMP + QMQ$ is
  diagonal with respect to some orthonormal basis for the total vector space.
\end{proof}

$M$ can therefore be written as $M = \sum_i \lambda_i\op{i}{i}$, where
$\lambda_i$ are the eigenvalues of $M$, $\ket{i}$ is an orthonormal basis for
$V$, and each $\ket{i}$ an eigenvector of $M$ with eigenvalue $\lambda_i$.

\paragraph{Unitary operators.} An operator $U$ is said to be unitary if
$U^{\dagger}U = I$. A unitary operator also satisfies $UU^{\dagger} = I$, and
therefore $U$ is normal and has a spectral decomposition.

Geometrically, unitary operators preserve inner products between vectors. For
any two vectors $\ket{v}$ and $\ket{w}$, \[
  (U\ket{v}, U\ket{w}) = \mel{v}{U^{\dagger}U}{w} = \mel{v}{I}{w} =
  \ip{v}{w}.
\]

\paragraph{Positive operators.} A \emph{positive} operator $A$ is defined to be
an operator such that for any vector $\ket{v}$, $(\ket{v}, A\ket{v})$ is a
real, non-negative number. If $(\ket{v}, A\ket{v})$ is \emph{strictly} greater
than zero for all $\ket{v} \neq 0$ then we say that $A$ is \emph{positive
definite}.

It can be shown that any positive operator is Hermitian and therefore has a
diagonal representation by spectral decomposition.

\subsubsection{Tensor product}

\paragraph{Tensor product.} Suppose $V$ and $W$ are Hilbert spaces of dimension
$m$ and $n$ respectively. Then $V \otimes W$ is an $mn$ dimensional vector
space, with the elements of $V \otimes W$ being linear combinations of tensor
products $\ket{v} \otimes \ket{w}$ of elements $\ket{v}$ of $V$ and $\ket{w}$
of $W$.

By definition the tensor product satisfies the following basic properties:
\begin{enumerate}
  \item For an arbitrary scalar $z$ and elements $\ket{v}$ of $V$ and $\ket{w}$
    of $W$, \[
      z(\ket{v} \otimes \ket{w}) = (z\ket{v}) \otimes \ket{W} = \ket{v} \otimes
      (z\ket{w}).
    \]
  \item For arbitrary $\ket{v_1}$ and $\ket{v_2}$ in $V$ and $\ket{w}$ in $W$,
    \[
      (\ket{v_1} + \ket{v_2}) \otimes \ket{w} = \ket{v_1} \otimes \ket{w} +
      \ket{v_2} \otimes \ket{w}.
    \]
  \item For arbitrary $\ket{v}$ in $V$ and $\ket{w_1}$ and $\ket{w_2}$ in $W$,
    \[
      \ket{v} \otimes (\ket{w_1} + \ket{w_2}) = \ket{v} \otimes \ket{w_1} +
      \ket{v} \otimes \ket{w_2}.
    \]
\end{enumerate}

\paragraph{Kronecker product.} Suppose $A$ is a $m$ by $n$ matrix, and $B$ is a
$p$ by $q$ matrix. Then we have the matrix representation: \[
  A \otimes B \equiv
  \begin{bmatrix}
    A_{11}B & A_{12}B & \cdots & A_{1n}B \\
    A_{21}B & A_{22}B & \cdots & A_{2n}B \\
    \vdots & \vdots & \ddots & \vdots \\
    A_{m1}B & A_{m2}B & \cdots & A_{mn}B
  \end{bmatrix}.
\]

\subsubsection{Operator functions}

\paragraph{Operator functions.} Let $A = \sum_a a\op{a}{a}$ be a spectral
decomposition for a normal operator $A$. Given a function $f$ on the complex
numbers, we may (uniquely) define $f(A) \equiv \sum_a f(a)\op{a}{a}$.

\paragraph{Trace.} The \emph{trace} of $A$ is defined to be the sum of its
diagonal elements, $\tr(A) \equiv \sum_i A_{ii}$.

The trace is easily seen to be \emph{cyclic} ($\tr(AB) = \tr(BA)$) and
\emph{linear} ($\tr(A + B) = \tr(A) + \tr(B), \tr(zA) = z\tr(A)$, where $A$ and
$B$ are arbitrary matrices, and $z$ is a complex number. Furthermore, it
follows that the trace of a matrix is invariant under the unitary
\emph{similarity transformation} $A \to UAU^{\dagger}$, so the trace of an
operator $A$ can be defined as the trace of any matrix representation of $A$.

Suppose $\ket{\psi}$ is a unit vector and $A$ is an arbitrary operator. To
evaluate $\tr(A\op{\psi}{\psi})$ use the Gram-Schmidt procedure to extend
$\ket{\psi}$ to an orthonormal basis $\ket{i}$ which includes $\ket{\psi}$ as
the first element. Then we have \[
  \tr(A\op{\psi}{\psi}) = \sum_i \mel{i}{A}{\psi}\ip{\psi}{i} =
  \mel{\psi}{A}{\psi}.
\]

\subsubsection{The commutator and anti-commutator}

\paragraph{Commutator.} The \emph{commutator} between two operators $A$ and $B$
is defined to be \[
  [A, B] \equiv AB - BA.
\] If $[A, B] = 0$, that is, $AB = BA$, then we say $A$ commutes with $B$.

\paragraph{Anti-commutator.} The \emph{anti-commutator} of two operators $A$
and $B$ is defined by \[
  \{A, B\} \equiv AB + BA;
\] we say $A$ \emph{anti-commutes} with $B$ if $\{A, B\} = 0$.

\begin{theorem}[Simultaneous diagonalization theorem]
  Suppose $A$ and $B$ are Hermitian operators. Then $[A, B] = 0$ if and only if
  there exists an orthonormal basis such that both $A$ and $B$ are diagonal
  with respect to that basis.
\end{theorem}

\begin{proof}
  It can be easily verified that if $A$ and $B$ are diagonal in the same
  orthonormal basis then $[A, B] = 0$. To show the converse, let $\ket{a, j}$
  be an orthonormal basis for the eigenspace $V_a$ of $A$ with eigenvalue $a$;
  the index $j$ is used to label possible degeneracies. Note that \[
    AB\ket{a, j} = BA\ket{a, j} = aB\ket{a, j},
  \] and therefore $B\ket{a, j}$ is an element of the eigenspace $V_a$. Let
  $P_a$ denote the projector onto the space $V_a$ and define $B_a \equiv
  P_aBP_a$. It is easy to see that the restriction of $B_a$ to the space $V_a$
  is Hermitian on $V_a$, and therefore has a spectral decomposition in terms of
  an orthonormal set of eigenvectors which span the space $V_a$. Let's call
  these eigenvectors $\ket{a, b, k}$, where the indices $a$ and $b$ label the
  eigenvalues of $A$ and $B_a$, and $k$ is an extra index to allow for the
  possibility of a degenerate $B_a$. Note that $B\ket{a, b, k}$ is an element
  of $V_a$, so $B\ket{a, b, k} = P_aB\ket{a, b, k}$. Moreover we have
  $P_a\ket{a, b, k} = \ket{a, b, k}$, so \[
    B\ket{a, b, k} = P_aBP_a\ket{a, b, k} = B_a\ket{a, b, k} = b\ket{a, b, k}.
  \] It folows that $\ket{a, b, k}$ is also an eigenvector of $B$ with
  eigenvalue $b$, and therefore $\ket{a, b, k}$ is an orthonormal set of
  eigenvectors of both $A$ and $B$, spanning the entire vector space on which
  $A$ and $B$ are defined. That is, $A$ and $B$ are simultaneously
  diagonalizable.
\end{proof}

\subsubsection{The polar and singular value decompositions}

\begin{theorem}[Polar decomposition]
  Let $A$ be a linear operator on a vector space $V$. Then there exists unitary
  $U$ and positive operators $J$ and $K$ such that \[
    A = UJ = KU,
  \] where the unique positive operators $J$ and $K$ satisfying these equations
  are defined by $J \equiv \sqrt{A^{\dagger}A}$ and $K \equiv
  \sqrt{AA^{\dagger}}$. Moreover, if $A$ is invertible then $U$ is unique.
\end{theorem}

We call $A = UJ$ the \emph{left polar decomposition} of $A$, and $A = KU$ the
\emph{right polar decomposition} of $A$.

\begin{proof}
  $J \equiv \sqrt{A^{\dagger}A}$ is a positive operator, so it can be given a
  spectral decomposition, $J = \sum_i \lambda_i\op{i}{i}$ ($\lambda_i \geq 0$).
  Define $\ket{\psi_i} \equiv A\ket{i}$. From the definition, we see that
  $\ip{\psi_i} = \mel{i}{A^{\dagger}A}{i} = \mel{i}{J^2}{i} =
  \lambda_i^2$. Consider for now only those $i$ for which $\lambda_i \neq 0$.
  For those $i$ define $\ket{e_i} \equiv \ket{\psi_i}/\lambda_i$, so the
  $\ket{e_i}$ are normalized and orthogonal.

  Now use the Gram-Schmidt procedure to extend the orthonormal set $\ket{e_i}$
  so it forms an orthonormal basis, which we also label $\ket{e_i}$. Define a
  unitary operator $U \equiv \sum_i \op{e_i}{i}$. When $\lambda_i \neq 0$ we
  have $UJ\ket{i} = \lambda_i\ket{e_i} = \ket{\psi_i}$. When $\lambda_i = 0$ we
  have $UJ\ket{i} = 0 = \ket{\psi_i}$. We have proved that the action of $A$
  and $UJ$ agree on the basis $\ket{i}$, and thus that $A = UJ$.

  $J$ is unique, since multiplying $A = UJ$ on the left by the adjoint equation
  $A^{\dagger} = JU^{\dagger}$ gives $J^2 = A^{\dagger}A$, from which we see
  that $J = \sqrt{A^{\dagger}A}$, uniquely. A little thought shows that if $A$
  is invertible, then so is $J$, so $U$ is uniquely determined by the equation
  $U = AJ^{-1}$. The proof of the right polar decomposition follows, since $A =
  UJ = UJU^{\dagger}U = KU$, where $K \equiv UJU^{\dagger}$ is a positive
  operator. Since $AA^{\dagger} = KUU^{\dagger}K = K^2$ we must have $K =
  \sqrt{AA^{\dagger}}$, as claimed.
\end{proof}

\begin{corollary}[Singular value decomposition]
  Let $A$ be a square matrix. Then there exist unitary matrices $U$ and $V$,
  and a diagonal matrix $D$ with non-negative entries such that \[
    A = UDV.
  \] The diagonal elements of $D$ are called the \emph{singular values} of $A$.
\end{corollary}
\begin{proof}
  By the polar decomposition, $A = SJ$ for unitary $S$ and positive $J$. By the
  spectral theoerm, $J = TDT^{\dagger}$, for unitary $T$ and diagonal $D$ with
  non-negative entries. Setting $U \equiv ST$ and $V \equiv T^{\dagger}$
  completes the proof.
\end{proof}

\subsection{The postulates of quantum mechanics}

\subsubsection{State sapce}

\begin{postulate}
  Associated to any isolated physical system is a complex vector space with
  inner product (that is, a Hilbert space) known as the \emph{state space} of
  the system. The system is completely described by its \emph{state vector},
  which is a unit vector in the system's state space.
\end{postulate}

\subsubsection{Evolution}

\begin{postulate}
  The evolution of a \emph{closed} quantum system is described by a
  \emph{unitary transformation}. That is, the state $\ket{\psi}$ of the system
  at time $t_1$ is related to the state $\ket{\psi'}$ of the system at time
  $t_2$ by a unitary operator $U$ which depends only on the times $t_1$ and
  $t_2$, \[
    \ket{\psi'} = U\ket{\psi}.
  \]

  The time evolution of the state of a closed quantum system can also be
  described by the \emph{Schr\"{o}dinger equation}, \[
    i\hbar\frac{d\ket{\psi}}{dt} = H\ket{\psi}.
  \]
\end{postulate}

Writing down the solution to Schr\"{o}dinger's equation, \[
  \ket{\psi(t_2)} = \exp\left[\frac{-iH(t_2 - t_1)}{\hbar}\right] =
  U(t_1, t_2)\ket{\psi(t_1)},
\] where \[
  U(t_1, t_2) \equiv \exp\left[\frac{-iH(t_2 - t_1)}{\hbar}\right]
\] is unitary. Furthermore, any unitary operator $U$ can be realized in the
form $U = \exp(iK)$ for some Hermitian operator $K$. There is therefore a
one-to-one correspondence between the discrete-time description of dynamics
using unitary operators, and the continuous time description using
Hamiltonians.

\subsubsection{Quantum measurement}

\begin{postulate}
  Quantum measurements are described by a collection $\{M_m\}$ of
  \emph{measurement operators}. These are operators acting on the state space
  of the system being measured. The index $m$ refers to the measurement
  outcomes that may occur in the experiment. If the state of the quantum system
  is $\ket{\psi}$ immediately before the measurement then the probability that
  result $m$ occurs is given by \[
    p(m) = \ev{M_m^{\dagger}M_m}{\psi},
  \] and the state of the system after the measurement is \[
    \frac{M_m\ket{\psi}}{\sqrt{\ev{M_m^{\dagger}M_m}{\psi}}}.
  \] The measurement operators satisfy the \emph{completeness equation}, \[
    \sum_m M_m^{\dagger}M_m = I.
  \]
\end{postulate}

The completeness equation expresses the fact that probabilities sum to one: \[
  I = \sum_m p(m) = \sum_m \ev{M_m^{\dagger}M_m}{\psi}.
\]

\subsubsection{Distinguishing quantum states}

It is possible to reliably distinguish orthonormal states, but non-orthonormal
states cannot be reliably distinguished.

\begin{proof}[Proof that non-orthogonal states can't be reliably
distinguished]
  A proof by contradiction shows that no measurement distinguishing the
  non-orthogonal states $\ket{\psi_1}$ and $\ket{\psi_2}$ is possible. Suppose
  such a measurement is possible. If the state $\ket{\psi_1}$ ($\ket{\psi_2}$)
  is prepared then the probability of measuring $j$ such that $f(j) = 1$ ($f(j)
  = 2$) must be 1. Defining $E_i \equiv \sum_{j: f(j) = i} M_j^{\dagger}M_j$,
  these observations may be written as: \[
    \ev{E}{\psi_1} = 1; \quad
    \ev{E_2}{\psi_2} = 1.
  \] Since $\sum_i E_i = I$ it follows that $\sum_i \ev{E_i}{\psi_1} = 1$, and
  since $\ev{E_1}{\psi_1} = 1$ we must have $\ev{E_2}{\psi_1} = 0$, and thus
  $\sqrt{E_2}\ket{\psi_1} = 0$. Suppose we decompose $\ket{\psi_2} =
  \alpha\ket{\psi_1} + \beta\ket{\phi}$, where $\ket{\phi}$ is orthonormal to
  $\ket{\psi_1}$, $|\alpha|^2 + |\beta|^2 = 1$, and $|\beta| < 1$ since
  $\ket{\psi_1}$ and $\ket{\psi_2}$ are not orthogonal. Then
  $\sqrt{E_2}\ket{\psi_2} = \beta\sqrt{E_2}\ket{\phi}$, which implies a
  contradiction, as \[
    \ev{E_2}{\psi_2} = |\beta|^2\ev{E_2}{\phi} \leq |\beta|^2 < 1,
  \] where the second last inequality follows from the observation that \[
    \ev{E_2}{\phi} \leq \sum_i \ev{E_i}{\phi} = \ip{\phi} = 1.
  \]
\end{proof}

\subsubsection{Projective measurements}

\paragraph{Projective measurements.} A projective measurement is described by
an \emph{observable}, $M$, a Hermitian operator on the state space of the
system being observed. The observable has a spectral decomposition, \[
  M = \sum_m mP_m,
\] where $P_m$ is the projector onto the eigenspace of $M$ with eigenvalue $m$.
The possible outcomes of the measurement correspond to the eigenvalues, $m$, of
the observable. Upon measuring the state $\ket{\psi}$, the probabiltiy of
getting result $m$ is given by \[
  p(m) = \ev{P_m}{\psi}.
\] Given that outcome $m$ occurred, the state of the quantum system immediately
after the measurement is \[
  \frac{P_m\ket{\psi}}{\sqrt{p(m)}}.
\]

Suppose the measurement operators in Postulate 3, in addition to satisfying the
completeness relation $\sum_m M_m^{\dagger}M_m = I$, also satisfy the
conditions that $M_m$ are orthogonal projectors, that is, the $M_m$ are
Hermitian, and $M_mM_{m'} = \delta_{m, m'}M_m$. With these additional
restrictions, Postulate 3 reduces to a projective measurement as just defined.

\paragraph{Expectation and standard deviation.} By definition, the expectation
value of a measurement operator $M$ is \[
  \E(M) = \sum_m mp(m) = \sum_m m\ev{P_m}{\psi} =
  \ev{\left(\sum_m mP_m\right)}{\psi} = \ev{M}{\psi}.
\] From this formula follows the variance associated to observations of $M$, \[
  [\Delta(M)]^2 = \langle (M - \langle M \rangle)^2 \rangle =
  \langle M^2 \rangle - \langle M \rangle^2.
\] and the standard deviation $\Delta(M)$ is determined by $\Delta(M) =
\sqrt{\langle M^2 \rangle - \langle M \rangle^2}$.

\begin{proof}[Proof of Heisenberg uncertainty principle.]
  Suppose $A$ and $B$ are two Hermitian operators, and $\ket{\psi}$ is a
  quantum state. Suppose $\ev{AB}{\psi} = x + iy$, where $x$ and $y$ are real.
  Note that $\ev{[A, B]}{\psi} = 2iy$ and $\ev{\{A, B\}}{\psi} = 2x$. This
  implies that \[
    |\ev{[A, B]}{\psi}|^2 + |\ev{\{A, B\}}{\psi}|^2 = 4|\ev{AB}{\psi}|^2.
  \] By the Cauchy-Schwarz inequality \[
    |\ev{AB}{\psi}|^2 \leq \ev{A^2}{\psi}\ev{B^2}{\psi},
  \] which combined with the the previous equation and dropping a non-negative
  term gives \[
    |\ev{[A, B]}{\psi}|^2 \leq 4\ev{A^2}{\psi}\ev{B^2}{\psi}.
  \] Suppose $C$ and $D$ are two observables. Substituting $A = C -
  \langle C \rangle$ and $B = D - \langle D \rangle$ into the last equation, we
  obtain Heisenberg's uncertainty principle as it is usually stated: \[
    \Delta(C)\Delta(D) \geq \frac{|\ev{[C, D]}{\psi}|}{2}.
  \]
\end{proof}

\subsubsection{POVM measurements}

For some applications where the post-measurement state of the system is of
little interest, the \emph{POVM formalism} is especially well adapted to the
analysis of the measurements.

\paragraph{Positive Operator-Valued Measure (POVM).} Suppose a measurement
described by measurement operators $M_m$ is performed upon a quantum system in
the state $\ket{\psi}$. Then the probability of outcome $m$ is given by $p(m) =
\ev{M_m^{\dagger}M_m}{\psi}$. Suppose we define \[
  E_m \equiv M_m^{\dagger}M_m.
\] Then from Postulate 3 and elementary linear algebra, $E_m$ is a positive
operator such that $\sum_m E_m = I$ and $p(m) = \ev{E_m}{\psi}$. Thus the set
of operators $E_m$ are sufficient to determine the probabilities of the
different measurement outcomes. The operators $E_m$ are known as the \emph{POVM
elements} associated with the measurement. The complete set $\{E_m\}$ is known
as a \emph{POVM}.

\subsubsection{Phase}

\paragraph{Global phase.} Consider the state $e^{i\theta}\ket{\psi}$, where
$\ket{\psi}$ is a state vector, and $\theta$ is a real number. We say that the
state $e^{i\theta}\ket{\psi}$ is equal to $\ket{\psi}$, up to the \emph{global
phase factor} $e^{i\theta}$.

Suppose $M_m$ is a measurement operator associated to some quantum measurement;
then the respective probabilities for outcome $m$ occurring are
$\ev{M_m^{\dagger}M_m}{\psi}$ and
$\ev{e^{-i\theta}M_m^{\dagger}M_me^{i\theta}}{\psi} =
\ev{M_m^{\dagger}M_m}{\psi}$, so the statistics of measurement predicted for
these two states are the same.

\paragraph{Relative phase.} We say that two amplitudes $a$ and $b$ \emph{differ
by a relative phase} if there is a real $\theta$ such that $a =
\exp(i\theta)b$. More generally, two states are said to \emph{differ by a
relative phase} in some basis if each of the amplitudes in that basis is
related by such a phase factor.

\subsubsection{Composite systems}

\begin{postulate}
  The state space of a composite physical system is the tensor product of the
  state spaces of the component physical systems. Moreover, if we have systems
  numbered 1 through $n$, and system number $i$ is prepared in the state
  $\ket{\psi_i}$, then the joint state of the system is $\ket{\psi_1} \otimes
  \ket{\psi_2} \otimes \cdots \otimes \ket{\psi_n}$.
\end{postulate}

Suppose we have a quantum system with state space $Q$, and we want to perform a
measurement described by measurement operators $M_m$ on the system $Q$. To do
this, we introduce an \emph{aniclla system}, with state space $M$, having an
orthonormal basis $\ket{m}$ in one-to-one correspondence with the possible
outcomes of the measurement we wish to implement.

Letting $\ket{0}$ be any fixed state of $M$, define an operator $U$ on products
$\ket{\psi}\ket{0}$ of states $\ket{\psi}$ from $Q$ with the state $\ket{0}$ by
\[
  U(\ket{\psi}\ket{0}) \equiv \sum_m (M_m\ket{\psi})\ket{m}.
\] Using the orthonormality of the states $\ket{m}$ and the completeness
relation $\sum_m M_m^{\dagger}M_m = I$, we can see that $U$ preserves inner
products between states of the form $\ket{\psi}\ket{0}$,
\begin{align*}
  (\bra{\phi}\bra{0})U^{\dagger}U(\ket{\psi}\ket{0})
  &= \sum_{m, m'} \ev{M_m^{\dagger}M_{m'}}{\psi}\braket{m}{m'} \\
  &= \sum_m \ev{M_m^{\dagger}M_m}{\psi} \\
  &= \braket{\phi}{\psi}.
\end{align*}

By Exercise 2.67 it follows that $U$ can be extended to a unitary operator on
the space $Q \otimes M$, which we also denote by $U$.

After letting $U$ act on $\ket{\psi}\ket{0}$, suppose we perform a projective
measurement on the two systems described by projectors $P_m \equiv I_Q \otimes
\op{m}{m}$. Outcome $m$ occurs with probability
\begin{align*}
  p(m)
  &= \bra{\psi}\bra{0}U^{\dagger}P_mU\ket{\psi}\ket{0} \\
  &= \sum_{m', m''} \bra{\psi}M_{m'}^{\dagger}\bra{m'} (I_Q \otimes \op{m}{m})
    M_{m''}\ket{\psi}\ket{m''} \\
  &= \ev{M_m^{\dagger}M_m}{\psi},
\end{align*}
just as given in Postulate 3. The joint state of the system $QM$ after
measurement, conditional on result $m$ occurring, is given by \[
  \frac{P_mU\ket{\psi}\ket{0}}{\sqrt{
    \bra{0}\bra{\psi}U^{\dagger}P_mU\ket{\psi}\ket{0}
  }} = \frac{M_m\ket{\psi}\ket{m}}{\sqrt{
    \bra{0}\bra{\psi}M_m^{\dagger}M_m\ket{\psi}\ket{0}
  }}
\] It follows that the state of system $M$ after the measurement is $\ket{m}$,
and the state of system $Q$ is \[
  \frac{M_m\ket{\psi}}{\sqrt{\ev{M_m^{\dagger}M_m}{\psi}}}
\] just as prescribed by Postulate 3. Thus unitary dynamics, projective
measurements, and the ability to introduce ancillary systems, together allow
any measurement of the form described in Postulate 3 to be realised.

\paragraph{Entanglement.} A state of a composite system which cannot be written
as a product of states of its component systems is an \emph{entangled} state.

\subsection{Application: superdense coding}

\emph{Superdense coding} involves two parties, conventionally known as 'Alice'
and 'Bob', who are a long way away from one another. Their goal is to transmit
some classical information from Alice to Bob. Suppose Alice is in posession of
two classical bits of information which she wishes to send Bob, but is only
allowed to send a single qubit to Bob.

Suppose Alice and Bob initially share a pair of qubits in the entangled state
\[
  \ket{\psi} = \frac{\ket{00} + \ket{11}}{\sqrt{2}}.
\] Alice is initially in possession of the first qubit, while Bob has
possession of the second qubit. Note that $\ket{\psi}$ is a fixed state; there
is no need for Alice to have sent Bob any qubits in order to prepare this
state. Instead, some third party may prepare the entangled state ahead of time,
sending one of the qubits to Alice, and the other to Bob.

Alice performs the following operations based on the bitstring she wishes to
send to Bob:
\begin{enumerate}
  \item 00: $\ket{\psi} \xrightarrow{I} \frac{\ket{00} + \ket{11}}{\sqrt{2}}$;
  \item 01: $\ket{\psi} \xrightarrow{Z} \frac{\ket{00} - \ket{11}}{\sqrt{2}}$;
  \item 10: $\ket{\psi} \xrightarrow{X} \frac{\ket{10} + \ket{01}}{\sqrt{2}}$;
  \item 11: $\ket{\psi} \xrightarrow{iY} \frac{\ket{01} - \ket{10}}{\sqrt{2}}$.
\end{enumerate}
These are the Bell states, which form an orthonormal basis, and can therefore
be distinguished by an appropriate quantum measurment. If Alice sends her qubit
to Bob, giving Bob possession of both qubits, then by doing a measurement in
the Bell basis Bob can determine which of the four possible bitstrings Alice
sent.

\subsection{The density operator}

\subsubsection{Ensembles of quantum states}

Suppose a quantum system is in one of a number of states $\ket{\psi_i}$, where
$i$ is an index, with respective probabilities $p_i$. We shall call $\{p_i,
\ket{\psi_i}\}$ an \emph{ensemble of pure states}.

\paragraph{Density operator.} The \emph{density operator} for the system is
defined by the equation \[
  \rho \equiv \sum_i p_i\op{\psi_i}.
\] The density operator is often known as the \emph{density matrix} too. All
the postulates of quantum mechanics can be reformulated in terms of the density
operator language.

\paragraph{Pure and mixed states.} A quantum system whose state $\ket{\psi}$ is
known exactly is said to be in a \emph{pure state}, such that the density
operator is simply $\rho = \op{\psi}$. Otherwise, $p$ is in a \emph{mixed
state} and said to be a mixture of the different pure states in the ensemble
for $\rho$.

A pure state satisfies $\tr(\rho^2) = 1$, while a mixed state satisfies
$\tr(\rho^2) < 1$.

Suppose that $\rho_i$ arises from some ensemble $\{p_{ij}, \ket{\psi_{ij}}\}$
of pure states, so the probability for being in the state $\ket{\psi_{ij}}$ is
$p_ip_{ij}$. The density matrix for the system is thus \[
  \rho = \sum_{ij} p_ip_{ij}\op{\psi_{ij}} = \sum_i p_i\rho_i
\] where we have used the definition $\rho_i = \sum_j p_{ij}\op{\psi_{ij}}$. We
say that $\rho$ is a mixture of the states $\rho_i$ with probabilities $p_i$.

\subsubsection{General properties of the density operator}

\setcounter{theorem}{4}
\begin{theorem}[Characterization of the density operators]
  An operator $\rho$ is the density operator associated to some ensemble
  $\{p_i, \ket{\psi_i}\}$ if and only if it satisfies the conditions:
  \begin{enumerate}[(1)]
    \item \textbf{(Trace condition)} $\tr(\rho) = 1$
    \item \textbf{(Positivity condition)} $\rho$ is a positive operator.
  \end{enumerate}
\end{theorem}
\begin{proof}
  Suppose $\rho = \sum_i p_i\op{\psi_i}$ is a density operator. Then \[
    \tr(\rho) = \sum_i p_i\tr(\op{\psi_i}) = \sum_i p_i = 1,
  \] so that the trace condition $\tr(\rho) = 1$ is satisfied. Suppose
  $\ket{\phi}$ is an arbitrary vector in state space. Then
  \begin{align*}
    \ev{\rho}{\phi}
    &= \sum_i p_i\ip{\phi}{\psi_i}\ip{\psi_i}{\phi} \\
    &= \sum_i p_i|\ip{\phi}{\psi_i}|^2 \\
    &\geq 0,
  \end{align*}
  so the positivity condition is satisfied.

  Conversely, suppose $\rho$ is any operator satisfying the trace and
  positivity conditions. Since $\rho$ is positive, it must have a spectral
  decomposition \[
    \rho = \sum_j \lambda_j\op{j},
  \] where the vectors $\ket{j}$ are orthogonal, and $\lambda_j$ are real,
  non-negative eigenvalues of $\rho$. From the trace condition we see that
  $\sum_j \lambda_j = 1$. Therefore, a system in state $\ket{j}$ with
  probability $\lambda_j$ will have density operator $\rho$. That is, the
  ensemble $\{\lambda_j, \ket{j}\}$ is an ensemble of states giving rise to the
  density operator $\rho$.
\end{proof}

We can now reformulate the postulates of quantum mechanics in the density
operator picture:

\setcounter{postulate}{0}
\begin{postulate}
  Associated to any isolated physical system is a Hilbert space known as the
  \emph{state space} of the system. The system is completely described by its
  \emph{density operator}, acting on the state space of the system. If a
  quantum system is in the state $\rho_i$ with probability $p_i$, then the
  density operator for the system is $\sum_i p_i\rho_i$.
\end{postulate}

\begin{postulate}
  The evolution of a closed quantum system is described by a unitary
  transformation. That is, the state $\rho$ of the system at time $t_1$ is
  related to the state $\rho'$ of the sysetm at time $t_2$ by a unitary
  operator $U$ which depends only on the times $t_1$ and $t_2$, \[
    \rho' = U \rho U^{\dagger}.
  \]
\end{postulate}

\begin{postulate}
  Quantum measurements are described by a collection $\{M_m\}$ of
  \emph{measurement operators}. These are operators acting on the state space
  of the system being measured. The index $m$ refers to the measurement
  outcomes that may occur in the experiment. If the state of the quantum system
  is $\rho$ immediately before the measurement then the probability that result
  $m$ occurs is given by \[
    p(m) = \tr(M_m^{\dagger}M_m\rho),
  \] and the state of the system after the measurement is \[
    \frac{M_m \rho M_m^{\dagger}}{\tr(M_m^{\dagger}M_m\rho)}.
  \] The measurement operators satisfy the completeness equation, \[
    \sum_m M_m^{\dagger}M_m = I.
  \]
\end{postulate}

\begin{postulate}
  The state space of a composite physical system is the tensor product of the
  state spaces of the component physical systems. Moreover, if we have systenms
  numbered 1 through $n$, and system number $i$ is prepared in the state
  $\rho_i$, then the joint state of the total system is $\rho_1 \otimes \rho_2
  \otimes \cdots \otimes \rho_n$.
\end{postulate}

We say the set $\ket{\tilde{\psi_i}}$ (which denotes vectors that may not be
normalized to unit length) \emph{generates} the operator $\rho \equiv \sum_i
\op{\tilde{\psi_i}}$, and thus the connection to the usual ensemble picture
of density operators is expressed by the equation $\ket{\tilde{\psi_i}} =
\sqrt{p_i}\ket{\psi_i}$.

\begin{theorem}[Unitary freedom in the ensemble for density matrices]
  The sets $\ket{\tilde{\psi_i}}$ and $\ket{\tilde{\phi_j}}$ generate
  the same density matrix if and only if \[
    \ket{\tilde{\psi_i}} = \sum_j u_{ij}\ket{\tilde{\psi_j}},
  \] where $u_{ij}$ is a unitary matrix of complex numbers, with indices $i$
  and $j$, and we 'pad' whichever set of vectors $\ket{\tilde{\psi_i}}$ or
  $\ket{\tilde{\phi_j}}$ is smaller with additional vectors $0$ so that the
  two sets have the same number of elements.
\end{theorem}
\begin{proof}
  Suppose $\ket{\tilde{\psi_i}} = \sum_j u_{ij}\ket{\tilde{\phi_j}}$ for some
  unitary $u_{ij}$. Then
  \begin{align*}
    \sum_i \op{\tilde{\psi_i}}
    &= \sum_{ijk} u_{ij}u_{ik}^*\op{\tilde{\phi_j}}{\tilde{\phi_k}} \\
    &= \sum_{jk}\left(\sum_i u_{ki}^{\dagger}u_{ij}\right)
      \op{\tilde{\phi_j}}{\tilde{\phi_k}} \\
    &= \sum_{jk} \delta_{kj}\op{\tilde{\phi_j}}{\tilde{\phi_k}} \\
    &= \sum_j \op{\tilde{\phi_j}}{\tilde{\phi_j}},
  \end{align*}
  which shows that $\ket{\tilde{\psi_i}}$ and $\ket{\tilde{\phi_j}}$ generate
  the same operator.

  Conversely, suppose \[
    A = \sum_i \op{\tilde{\psi_i}} = \sum_j \op{\tilde{\phi_j}}.
  \] Let $A = \sum_k \lambda_k\op{k}$ be a decomposition for $A$ such that the
  states $\ket{k}$ are orthonormal, and the $\lambda_k$ are strictly positive.
  Define $\ket{\tilde{k}} \equiv \sqrt{\lambda_k}\ket{k}$ and let $\ket{\psi}$
  be any vector orthonormal to the space spanned by the $\ket{\tilde{k}}$, so
  $\ip{\psi}{\tilde{k}}\ip{\tilde{k}}{\psi} = 0$ for all $k$, and thus we see
  that \[
    0 = \ev{A}{\psi}
    = \sum_i \ip{\psi}{\tilde{\psi_i}}\ip{\tilde{\psi_i}}{\psi}
    = \sum_i \left|\ip{\psi}{\tilde{\psi_i}}\right|^2.
  \] Thus $\ip{\psi}{\tilde{\psi_i}} = 0$ for all $i$ and all $\ket{\psi}$
  orthonormal to the space spanned by the $\ket{\tilde{k}}$. It follows that
  each $\ket{\tilde{\psi_i}}$ can be expressed as a linear combination of the
  $\ket{\tilde{k}}$, $\ket{\tilde{\psi_i}} = \sum_k c_{ik}\ket{\tilde{k}}$.
  Since $A = \sum_k \ip{\tilde{k}} = \sum_i \ip{\tilde{\psi_i}}$ we see that \[
    \sum_k \ip{\tilde{k}}
    = \sum_{kl} \left(\sum_i c_{ik}c_{il}^*\right)\op{\tilde{k}}{\tilde{l}}.
  \] The operators $\op{\tilde{k}}{\tilde{l}}$ are easily seen to be linearly
  independent, and thus it must be that $\sum_i c_{ik}c_{il}^* = \delta_{kl}$.
  This ensures that we may append extra columns to $c$ to obtain a unitary
  matrix $v$ such that $\ket{\tilde{\psi_i}} = \sum_k v_{ik}\ket{\tilde{k}}$,
  where we have appended zero vectors to the list of $\ket{\tilde{k}}$.
  Similarly, we can find a unitary matrix $w$ such that $\ket{\tilde{\phi_j}} =
  \sum_k w_{jk}\ket{\tilde{k}}$. Thus $\ket{\tilde{\psi_i}} = \sum_j
  u_{ij}\ket{\tilde{\phi_j}}$, where $u = vw^{\dagger}$ is unitary.
\end{proof}

As a consequence of this theorem, note that $\rho = \sum_i p_i\op{\psi_i} =
\sum_j q_j\op{\phi_j}$ for normalized states $\ket{\psi_i}$, $\ket{\phi_j}$ and
probability distributions $p_i$ and $q_j$ if and only if \[
  \sqrt{p_i}\ket{\psi_i} = \sum_j u_{ij}\sqrt{q_j}\ket{\phi_j},
\] for some unitary matrix $u_{ij}$, and we may pad the smaller ensemble with
entries having probability zero in order to make the two ensembles the same
size.

\subsubsection{The reduced density operator}

\paragraph{Reduced density operator.} Suppose we have physical systems $A$ and
$B$, whose state is described by a density operator $\rho^{AB}$. The reduced
density operator for system $A$ is described by \[
  \rho^A \equiv \tr_B(\rho^{AB}),
\] where $\tr_B$ is a map of operators konwn as the \emph{partial trace} over
system $B$.

\paragraph{Patrial trace.} The partial trace is defined by \[
  \tr_B(\op{a_1}{a_2} \otimes \op{b_1}{b_2}) \equiv
  \op{a_1}{a_2}\tr(\op{b_1}{b_2})
\] where $\ket{a_1}$ and $\ket{a_2}$ are any two vectors in the state space of
$A$, and $\ket{b_1}$ and $\ket{b_2}$ are any two vectors in the state space of
$B$.

\subsection{The Schmidt decomposition and purifications}

\begin{theorem}[Schmidt decomposition]
  Suppose $\ket{\psi}$ is a pure state of a composite system, $AB$. Then there
  exist orthonormal states $\ket{i_A}$ for system $A$, and orthonormal states
  $\ket{i_B}$ of system $B$ such that \[
    \ket{\psi} = \sum_i \lambda_i\ket{i_A}\ket{i_B},
  \] where $\lambda_i$ are non-negative real numbers satisfying $\sum_i
  \lambda_i^2 = 1$ known as \emph{Schmidt co-efficients}.
\end{theorem}
\begin{proof}
  We give the proof for the case where systems $A$ and $B$ have state spaces of
  the same dimension.

  Let $\ket{j}$ and $\ket{k}$ be any fixed orthonormal bases for systems $A$ and
  $B$, respectively. Then $\ket{\psi}$ can be written \[
    \ket{\psi} = \sum_{jk} a_{jk}\ket{j}\ket{k},
  \] for some matrix $a$ of complex numbers $a_{jk}$. By the singular value
  decomposition, $a = udv$, where $d$ is a diagonal matrix with non-negative
  elements, and $u$ and $v$ are unitary matrices. Thus \[
    \ket{\psi} = \sum_{ijk} u_{ji}d_{ii}v_{ik}\ket{j}\ket{k}.
  \] Defining $\ket{i_A} \equiv \sum_j u_{ji}\ket{j}$, $\ket{i_B} \equiv \sum_k
  v_{ik}\ket{k}$, and $\lambda_i \equiv d_{ii}$, we see that this gives \[
    \ket{\psi} = \sum_i \lambda_i\ket{i_A}\ket{i_B}.
  \] It is easy to check that $\ket{i_A}$ forms an orthonormal set, from the
  unitarity of $u$ and the orthonormality of $\ket{j}$, and similarly that the
  $\ket{i_B}$ form an orthonormal set.
\end{proof}

The bases $\ket{i_A}$ and $\ket{i_B}$ are called the \emph{Schmidt bases} for
$A$ and $B$, respectively, and the number of non-zero values $\lambda_i$ is
called the \emph{Schmidt number} for the state $\ket{\psi}$.

\begin{theorem}[Schmidt purification]
  Suppose we are given a state $\rho^A$ of a quantum system $A$. It is possible
  to introduce another system, which we denote $R$, and define a pure state
  $\ket{AR}$ for the joint system $AR$ such that $\rho^A = \tr_R(\op{AR})$. That
  is, the pure state $\ket{AR}$ reduces to $\rho^A$ when we look at system $A$
  alone.
\end{theorem}
\begin{proof}
  Suppose $\rho^A$ has orthonormal decomposition $\rho^A = \sum_i p_i\op{i^A}$.
  To purify $\rho^A$ we introduce a system $R$ which has the same state space as
  system $A$, with orthonormal basis states $\ket{i^R}$, and define a pure state
  for the combined system \[
    \ket{AR} \equiv \sum_i \sqrt{p_i}\ket{i^A}\ket{i^R}.
  \] We now calculate the reduced density operator for system $A$ correspondingto the state $\ket{AR}$:
  \begin{align*}
    \tr_R(\op{AR}) &= \sum_{ij} \sqrt{p_ip_j}\op{i^A}{j^A}\tr(\op{i^R}{j^R}) \\
                   &= \sum_{ij} \sqrt{p_ip_j}\op{i^A}{j^A}\delta_{ij} \\
                   &= \sum_{i} p_i\op{i^A} \\
                   &= \rho^A.
  \end{align*}
  Thus $\ket{AR}$ is a purification of $\rho^A$.
\end{proof}

\end{document}
