\documentclass{article}

\usepackage{amsmath,amssymb,amsthm}
\usepackage[shortlabels]{enumitem}

\newtheorem{corollary}{Corollary}
\numberwithin{corollary}{subsection}
\newtheorem*{corollary*}{Corollary}
\newtheorem{definition}{Definition}
\numberwithin{definition}{subsection}
\newtheorem*{definition*}{Definition}
\newtheorem{lemma}{Lemma}
\numberwithin{lemma}{subsection}
\newtheorem*{lemma*}{Lemma}
\newtheorem{theorem}{Theorem}
\numberwithin{theorem}{subsection}
\newtheorem*{theorem*}{Theorem}

\DeclareMathOperator{\area}{area}

\title{Notes to ``Calculus'', 3rd edition (Spivak)}
\author{Ng Wei En}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Basic Properties of Numbers}

Of the first twelve properties in this chapter, the first nine are concerned
with the fundamental operations of addition and multiplication.

\begin{tabular}{l p{1.5in}}
  (P1: Associative law for addition) & $a + (b + c) = (a + b) + c$. \\
  (P2: Existence of an additive identity) & $a + 0 = 0 + a = a$. \\
  (P3: Existence of additive inverses) & $a + (-a) = (-a) + a = 0$. \\
  (P4: Commutative law for addition) & $a + b = b + a$. \\
  (P5: Associative law for multiplication) & $a \cdot (b \cdot c) = (a \cdot
    b) \cdot c$. \\
  (P6: Existence of a multiplicative identity) & $a \cdot 1 = 1 \cdot a = a; 1
    \neq 0$. \\
  (P7: Existence of a multiplicative inverse) & $a \cdot a^{-1} = a^{-1} \cdot
    a = 1, \text{ for } a \neq 0$. \\
  (P8: Commutative law for multiplication) & $a \cdot b = b \cdot a$. \\
  (P9: Distributive law) & $a \cdot (b + c) = a \cdot b + a \cdot c$.
\end{tabular}

The last three are concerned with inequalities. Considering the collection of
all positive numbers, $P$,

\begin{tabular}{l p{2in}}
  (P10: Trichotomy law) & For every number $a$, one and only one of the
    following holds:
    \begin{enumerate}
      \itemsep0em
      \item $a = 0$,
      \item $a$ is in the collection $P$,
      \item $-a$ is in the collection $P$.
    \end{enumerate} \\
  (P11: Closure under addition) & If $a$ and $b$ are in $P$, then $a + b$ is
    in $P$. \\
  (P12: Closure under multiplication) & If $a$ and $b$ are in $P$, then $a
    \cdot b$ is in $P$.
\end{tabular}

\setcounter{section}{2}
\section{Functions}

\begin{definition*}
  A \textbf{function} is a collection of pairs of numbers with the following
  properties: if $(a, b)$ and $(a, c)$ are both in the collection, then $b =
  c$.
\end{definition*}

If $f$ is a function, the \textbf{domain} of $f$ is the set of all $a$ for
which there is some $b$ such that $(a, b)$ is in $f$. If $a$ is in the domain
of $f$, it follows from the definition that there is a \emph{unique} number $b$
such that $(a, b)$ is in $f$. This unique $b$ is denoted by $f(a)$.

\subsection{Ordered Pairs}

\begin{definition*}
  $(a, b) = \{\{a\}, \{a, b\}\}$.
\end{definition*}

\begin{theorem}
  If $(a, b) = (c, d)$, then $a = c$ and $b = d$.
\end{theorem}

\setcounter{section}{4}
\section{Limits}

\begin{definition*}
  The function \textbf{$f$ approaches the limit $l$ near $a$} means: for every
  $\epsilon > 0$ there is some $\delta > 0$ such that, for all $x$, if $0 < |x
  - a| < \delta$, then $|f(x) - l| < \epsilon$.
\end{definition*}

\begin{definition*}
  $\lim_{x \to a^+} f(x) = l$ (i.e. the function \textbf{$f$ approaches the
  limit $l$ as $x$ approaches $a$ from above}) means: for every $\epsilon > 0$
  there is a $\delta > 0$ such that, for all $x$, if $0 < x - a < \delta$, then
  $|f(x) - l| < \epsilon$.
\end{definition*}

A similar definition holds for $\lim_{x \to a^-} f(x) = l$ (i.e. the function
\textbf{$f$ approaches the limit $l$ as $x$ approaches $a$ from below}).

\begin{theorem}
  A function cannot approach two different limits near $a$. In other words, if
  $f$ approaches $l$ near $a$, and $f$ approaches $m$ near $a$, then $l = m$.
\end{theorem}
\begin{proof}
  For any $\epsilon > 0$ there is some number $\delta_1, \delta_2 > 0$ such
  that, for all $x$,
  \begin{align*}
    &\text{if } 0 < |x - a| < \delta_1, &\text{ then } &|f(x) - l| < \epsilon;
    \\
    &\text{if } 0 < |x - a| < \delta_2, &\text{ then } &|f(x) - m| < \epsilon.
  \end{align*}
  We can now pick some $\delta = \min(\delta_1, \delta_2) > 0$ such that, for
  any $\epsilon > 0$ and all $x$,
  \begin{align*}
    \text{if } 0 < |x - a| < \delta,
    \text{ then } &|f(x) - l| < \epsilon \\
    \text{ and } &|f(x) - m| < \epsilon;
  \end{align*}

  If $l \neq m$, so that $|l - m| > 0$, we can choose $\epsilon = |l - m|/2$.
  But this implies that for $0 < |x - a| < \delta$ we have
  \begin{align*}
    |l - m| = |l - f(x) + f(x) - m|
    &\leq |l - f(x)| + |f(x) - m| \\
    &< \frac{|l - m|}{2} + \frac{|l - m|}{2} \\
    &= |l - m|,
  \end{align*}
  a contradiction.
\end{proof}

\begin{theorem}
  If $\lim_{x \to a}f(x) = l$ and $\lim_{x \to a}g(x) = m$, then
  \begin{enumerate}
    \item $\lim_{x \to a}(f + g)(x) = l + m;$
    \item $\lim_{x \to a}(f \cdot g)(x) = l \cdot m.$
  \end{enumerate}
  Moreover, if $m \neq 0$, then
  \begin{enumerate}
    \setcounter{enumi}{2}
    \item $\lim_{x \to a}\left(\frac{1}{g}\right)(x) = \frac{1}{m}.$
  \end{enumerate}
\end{theorem}

\section{Continuous Functions}

\begin{definition*}
  The function $f$ is \textbf{continuous at $a$} if $\lim_{x \to a} f(x) =
  f(a)$. If $f$ is continuous at $x$ for all $x$ in $(a, b)$, then $f$ is
  called \textbf{continuous on $(a, b)$}. A function $f$ is called
  \textbf{continuous on $[a, b]$} if
  \begin{enumerate}
    \item $f$ is continuous on $(a, b)$, and
    \item $\lim_{x \to a^+} f(x) = f(a)$ and $\lim_{x \to b^-} f(x) = f(b)$.
  \end{enumerate}
\end{definition*}

\begin{theorem}
  If $f$ and $g$ are continuous at $a$, then
  \begin{enumerate}
    \item $f + g$ is continuous at $a$,
    \item $f \cdot g$ is continuous at $a$.
  \end{enumerate}
  Moreover, if $g(a) \neq 0$, then
  \begin{enumerate}
    \setcounter{enumi}{2}
    \item $1/g$ is continuous at $a$.
  \end{enumerate}
\end{theorem}

\begin{theorem}
  If $g$ is continuous at $a$, and $f$ is continuous at $g(a)$, then $f \circ
  g$ is continuous at $a$.
\end{theorem}
\begin{proof}
  Let $\epsilon > 0$. Since $f$ is continuous at $g(a)$, there is a $\delta' >
  0$ such that for all $x$, \[
    \text{if } |g(x) - g(a)| < \delta', \text{ then } |f(g(x)) - f(g(a))| <
      \epsilon.
  \] We also conclude, by the continuity of $g$ at $a$, that there is a $\delta
  > 0$ such that for all $x$, \[
    \text{if } |x - a| < \delta, \text{ then } |g(x) - g(a)| < \delta'.
  \] Combining these two statements proves that $f \circ g$ is continuous at
  $a$.
\end{proof}

\begin{theorem}
  Suppose $f$ is continuous at $a$, and $f(a) > 0$. Then there is a number
  $\delta > 0$ such that $f(x) > 0$ for all $x$ satisfying $|x - a| < \delta$.
  Similarly, if $f(a) < 0$, then there is a number $\delta > 0$ such that $f(x)
  < 0$ for all $x$ satisfying $|x - a| < \delta$.
\end{theorem}
\begin{proof}
  Consider the case $f(a) > 0$. Since $f$ is continuous at $a$, for $f(a) > 0$
  there is a $\delta > 0$ such that, for all $x$, \[
    \text{if } |x - a| < \delta, \text{ then } |f(x) - f(a)| < f(a),
  \] and this last inequality implies $f(x) > 0$. A similar proof can be given
  in the case $f(a) < 0$.
\end{proof}

\section{Three Hard Theorems}

\begin{theorem}[Intermediate Value Theorem cf. Theorems 7-1, 7-4, 7-5]
  If $f$ is continuous on $[a, b]$ and $f(a) < c < f(b)$, then there is some
  $x$ in $[a, b]$ such that $f(x) = c$. Similarly, if $f(a) > c > f(b)$, then
  there is some $x$ in $[b, a]$ such that $f(x) = c$.
\end{theorem}

\begin{theorem}[Boundedness Theorem cf. Theorems 7-2, 7-6]
  If $f$ is continuous on $[a, b]$, then $f$ is bounded on $[a, b]$, that
  is, there are some numbers $N_1$ and $N_2$ such that $f(x) \leq N_1$ and
  $f(x) \geq N_2$ for all $x$ in $[a, b]$.
\end{theorem}

\begin{theorem}[Extreme Value Theorem cf. Theorems 7-3, 7-7]
  If $f$ is continuous on $[a, b]$, then there is some $y_1$ in $[a, b]$ such
  that $f(y_1) \leq f(x)$ for all $x$ in $[a, b]$. Similarly, there is some
  $y_2$ in $[a, b]$ such that $f(y_2) \geq f(x)$ for all $x$ in $[a, b]$.
\end{theorem}

Following on from Theorem 1 to 7, we proceed to prove a few interesting
results.

\setcounter{theorem}{7}
\begin{theorem}
  Every positive number has a square root. In other words, if $\alpha > 0$,
  then there is some number $x$ such that $x^2 = \alpha$.
\end{theorem}
\begin{proof}
  Consider the continuous function $f(x) = x^2$. There is obviously a number $b
  > 0$ such that $f(b) > \alpha$; if $\alpha > 1$ we can take $b = \alpha$,
  while if $\alpha < 1$ we can take $b = 1$. Since $f(0) < \alpha < f(b)$, the
  Intermediate Value Theorem applied to $[0, b]$ implies that for some $x$ in
  $[0, b]$, we have $f(x) = \alpha$.
\end{proof}

\begin{theorem}
  If $n$ is odd, then any equation \[
    x^n + a_{n-1}x^{n-1} + \cdots + a_0 = 0
  \] has a root.
\end{theorem}
\begin{proof}
  We consider the function \[
    f(x) = x^n + a_{n-1}x^{n-1} + \cdots + a_0
    = x^n \left(1 + \frac{a_{n-1}}{x} + \cdots + \frac{a_0}{x^n}\right).
  \] Note that \[
    \left|\frac{a_{n-1}}{x} + \frac{a_{n-2}}{x^2} + \cdots +
    \frac{a_0}{x^n}\right|
    \leq \frac{|a_{n-1}|}{|x|} + \cdots + \frac{|a_0|}{|x^n|}.
  \] Consequently, if we choose $x$ satisfying \[
    \label{eq:thm7-9-cond} \tag{*}
    |x| \geq 1, 2n|a_{n-1}|, \ldots, 2n|a_0|,
  \] then \[
    \frac{|a_{n-k}|}{|x^k|}
    \leq \frac{|a_{n-k}|}{|x|}
    \leq \frac{|a_{n-k}|}{2n |a_{n-k}|}
    = \frac{1}{2n},
  \] so \[
    \left|\frac{a_{n-1}}{x} + \frac{a_{n-2}}{x^2} + \cdots +
    \frac{a_0}{x^n}\right|
    \leq \underbrace{\frac{1}{2n} + \cdots + \frac{1}{2n}}_\text{$n$ terms}
    = \frac{1}{2}
  \] which implies that \[
    \frac{1}{2} \leq 1 + \frac{a_{n-1}}{x} + \cdots + \frac{a_0}{x^n}.
  \] Therefore, if we choose an $x_1 > 0$ which satisfies
  \eqref{eq:thm7-9-cond}, then \[
    \frac{(x_1)^n}{2}
    \leq (x_1)^n\left(1 + \frac{a_{n-1}}{x_1} + \cdots +
    \frac{a_0}{(x_1)^n}\right)
    = f(x_1),
  \] so that $f(x_1) > 0$. On the other hand, if $x_2 < 0$ satisfies
  \eqref{eq:thm7-9-cond}, then \[
    \frac{(x_2)^n}{2}
    \geq (x_2)^n\left(1 + \frac{a_{n-1}}{x_2} + \cdots +
    \frac{a_0}{(x_2)^n}\right)
    = f(x_2),
  \] so that $f(x_2) < 0$. Now applying the Intermediate Value Theorem to the
  interval $[x_2, x_1]$, we conclude that there is an $x$ in $[x_2, x_1]$ such
  that $f(x) = 0$.
\end{proof}

\begin{theorem}
  If $n$ is even and $f(x) = x^n + a_{n-1}x^{n-1} + \cdots + a_0$, then there
  is a number $y$ such that $f(y) \leq f(x)$ for all $x$.
\end{theorem}
\begin{proof}
  As in the proof of Theorem 9, if \[
    M = \max(1, 2n|a_{n-1}|, \ldots, 2n|a_0|),
  \] then for all $x$ with $|x| \geq M$, we have \[
    \frac{1}{2} \leq 1 + \frac{a_{n-1}}{x} + \cdots + \frac{a_0}{x^n}.
  \] Since $n$ is even, $x^n \geq 0$ for all $x$, so \[
    \frac{x^n}{2}
    \leq x^n \left(1 + \frac{a_{n-1}}{x} + \cdots + \frac{a_0}{x^n}\right)
    = f(x),
  \] provided that $|x| \geq M$. Now consider the number $f(0)$. Let $b > 0$ be
  a number such that $b^n/2 \geq f(0)$ and also $b \geq M$. Then, if $x \geq
  |b|$, with even $n$ we have \[
    f(x) \geq \frac{x^n}{2} \geq \frac{b^n}{2} \geq f(0).
  \]

  Now apply the Extreme Value Theorem to the function $f$ on the interval $[-b,
  b]$. We conclude that there is a number $y$ such that
  \begin{align*}
    \text{if } &-b \leq x \leq b, &\text{ then } &f(y) \leq f(x); \\
    \text{if } &x \leq -b \text{ or } x \geq b, &\text{ then } &f(x) \geq f(0)
      \geq f(y).
  \end{align*}
  Hence, we see that $f(y) \leq f(x)$ for all $x$.
\end{proof}

\begin{theorem}
  Consider the equation \[
    \label{eq:thm7-11-eq} \tag{*}
    x^n + a_{n-1}x^{n-1} + \cdots + a_0 = c, 
  \] and suppose $n$ is even. Then there is a number $m$ such that
  \eqref{eq:thm7-11-eq} has a solution for $c \geq m$ and has no solution for $c
  < m$.
\end{theorem}
\begin{proof}
  Let $f(x) = x^n + a_{n-1}x^{n-1} + \cdots + a_0$.

  According to Theorem 10 there is a number $y$ such that $f(y) \leq f(x)$ for
  all $x$. Let $m = f(y)$. If $c < m$, then the equation
  \eqref{eq:thm7-11-eq} has no solution, since the left side always has a
  value $\geq m$. If $c \geq m$, let $b$ be a number such that $b > y$ and
  $f(b) > c$. Then $f(y) = m \leq c < f(b)$. Consequently, by the Intermediate
  Value Theorem, there is some number $x$ in $[y, b]$ such that $f(x) = c$, so
  $x$ is a solution of \eqref{eq:thm7-11-eq}.
\end{proof}

On the basis of our present knowledge about the real numbers (namely, P1-P12)
a proof of Theorems 1-7 is impossible and will be left to later
sections.

\section{Least Upper Bounds}

\begin{definition*}
  A set $A$ of real numbers is \textbf{bounded above} if there is a number $x$
  such that $x \geq a$ for every $a$ in $A$. Such a number $x$ is called an
  \textbf{upper bound} for $A$.
\end{definition*}

\begin{definition*}
  A number $x$ is the \textbf{least upper bound} of $A$ if
  \begin{itemize}
    \item $x$ is an upper bound of $A$, and
    \item if $y$ is an upper bound of $A$, then $x \leq y$.
  \end{itemize}

  The term \textbf{supremum} of $A$ is synonymous with ``least upper bound''
  and abbreviates to $\sup A$.
\end{definition*}

Similar definitions hold for `\textbf{bounded above}' and `\textbf{greatest
lower bound}'. The term \textbf{infimum} of $A$ is synonymous with ``least
upper bound'' and abbreviates to $\inf A$. \newline

The last property of the real numbers can now be stated:

\begin{tabular}{l p{1.5in}}
  (P13: The least upper bound property) & If $A$ is a set of real numbers, $A
  \neq \varnothing$, and $A$ is bounded above, then $A$ has a least upper
  bound.
\end{tabular}

We shall apply P13 to the proofs that were omitted in Chapter 7.

\begin{theorem*}[cf. Theorem 7-1]
  If $f$ is continuous on $[a, b]$ and $f(a) < 0 < f(b)$, then there is some
  number $x$ in $[a, b]$ such that $f(x) = 0$.
\end{theorem*}
\begin{proof}
  Define the set $A$ as follows: \[
    A = \{x : a \leq x \leq b, \text{ and } f \text{ is negative on the interval
      } [a, x]\}.
  \] Clearly $A \neq \varnothing$, since $a$ is in $A$; in fact, from Theorem
  6-3, as $f$ is continuous at $a$ and $f(a) < 0$, there is some $\delta > 0$
  such that $A$ contains all points $x$ satisfying $a \leq x < a + \delta$.
  Similarly, as $f$ is continuous at $b$ and $f(b) > 0$, there is some $\delta
  > 0$ such that all points $x$ satisfying $b - \delta < x \leq b$ are upper
  bounds for $A$. It follows that $A$ has a least upper bound $\alpha$ and that
  $a < \alpha < b$.

  Suppose first that $f(\alpha) < 0$. Similarly by Theorem 6-3, there is a
  $\delta > 0$ such that $f(x) < 0$ for $\alpha - \delta < x < \alpha +
  \delta$. Suppose we have some $x_0$ in $A$ which satisfies $\alpha - \delta <
  x_0 < \alpha$ so that $f$ is negative on $[a, x_0]$ (because otherwise
  $\alpha$ would not be the least upper bound of $A$). But for some $x_1$
  satisfying $\alpha < x_1 < \alpha + \delta$, $f$ is also negative on $[x_0,
  x_1]$. Therefore $f$ is negative on the interval $[a, x_1]$ and $x_1$ is in
  $A$, contradicting the fact that $\alpha$ is an upper bound for $A$.

  Suppose, on the other hand, that $f(\alpha) > 0$. Then there is a number
  $\delta > 0$ such that $f(x) > 0$ for $\alpha - \delta < x < \alpha + \delta$.
  Once again we know that there is an $x_0$ in $A$ satisfying $\alpha - \delta <
  x_0 < \alpha$; but this means that $f$ is negative on $[a, x_0]$, which is
  impossible, since $f(x_0) > 0$. Thus the assumption $f(\alpha) > 0$ also leads
  to a contradiction, leaving $f(\alpha) = 0$ as the only possible alternative.
\end{proof}

The proofs of Theorems 7-2 and 7-3 require a simple preliminary
result, which will play much the same role as Theorem 6-3 played in the
previous proof.

\begin{theorem}
  If $f$ is continuous at $a$, then there is a number $\delta > 0$ such that
  $f$ is bounded above on the interval $(a - \delta, a + \delta)$.
\end{theorem}

Observe that if $\lim_{x \to a^+} f(x) = f(a)$, then there is a $\delta > 0$
such that $f$ is bounded on the set $\{x: a \leq x < a + \delta\}$, and a
similar observation holds if $\lim_{x \to b^-} f(x) = f(b)$. Having made these
observations, we tackle our second major theorem.

\begin{theorem*}[cf. Theorem 7-2]
  If $f$ is continuous on $[a, b]$, then $f$ is bounded above on $[a, b]$.
\end{theorem*}
\begin{proof}
  Let \[
    A = \{x: a \leq x \leq b \text{ and } f \text{ is bounded above on } [a,
      x]\}.
  \] Clearly $A \neq \varnothing$ (since $a$ is in $A$), and $A$ is bounded
  above (by $b$), so $A$ has a least upper bound $\alpha$.

  Suppose that $\alpha = a$. By Theorem 1 there is $\delta_0 > 0$ such that $f$
  is bounded on $[\alpha, \alpha + \delta_0)$. But this means that if $x_0$ is
  any number with $\alpha < x_0 < \alpha + \delta_0$, then $f$ is also bounded
  on $[a, x_0]$. So $x_0$ is in $A$, contradicting the fact that $\alpha$ is an
  upper bound for $A$.

  Suppose that $\alpha < b$. By Theorem 1 there is $\delta_1 > 0$ such that $f$
  is bounded on $(\alpha - \delta_1, \alpha + \delta_1)$. Since $\alpha$ is the
  least upper bound of $A$ there is some $x_1$ in $A$ satisfying $\alpha -
  \delta_1 < x_1 < \alpha$. This means that $f$ is bounded on $[a, x_1]$. But
  if $x_2$ is any number with $\alpha < x_2 < \alpha + \delta_2$, then $f$ is
  also bounded on $[x_1, x_2]$. Therefore $f$ is bounded on $[a, x_2]$, so
  $x_2$ is in $A$, contradicting the fact that $\alpha$ is an upper bound for
  $A$.

  These two contradictions show that $\alpha = b$, so $f$ is bounded on $[a,
  x]$ for every $x < b$. By Theorem 1 again there is a $\delta > 0$ such that
  $f$ is bounded on $(b - \delta, b]$, and therefore a $x$ in $A$ such that $b
  - \delta < x < b$. Thus $f$ is bounded on $[a, x]$ and also on $[x, b]$, so
  $f$ is bounded on $[a, b]$.
\end{proof}

To prove the third important theorem we resort to a trick.

\begin{theorem*}[cf. Theorem 7-3]
  If $f$ is continuous on $[a, b]$, then there is a number $y$ in $[a, b]$ such
  that $f(y) \geq f(x)$ for all $x$ in $[a, b]$.
\end{theorem*}
\begin{proof}
  We already know that $f$ is bounded on $[a, b]$, which means that the set \[
    \{f(x): x \in [a, b]\}
  \] is bounded. This set is obviously not $\varnothing$, so it has a least
  upper bound $\alpha$. Since $\alpha \geq f(x)$ for $x$ in $[a, b]$ it
  suffices to show that $\alpha = f(y)$ for some $y$ in $[a, b]$.

  Suppose instead that $\alpha \neq f(y)$ for all $y$ in $[a, b]$. Then the
  function $g$ defined by \[
    g(x) = \frac{1}{\alpha - f(x)}, x \in [a, b]
  \] is continuous on $[a, b]$, since the denominator of the right side is
  never 0. On the other hand, $\alpha$ is the least upper bound of $\{f(x): x
  \in [a, b]\}$; this means that \[
    \text{for every } \epsilon > 0 \text{ there is } x \in [a, b] \text{ with }
    \alpha - f(x) < \epsilon.
  \] This, in turn, means that \[
    \text{for every } \epsilon > 0 \text{ there is } x \in [a, b] \text{ with }
    g(x) > \frac{1}{\epsilon}.
  \] But this means that $g$ is not bounded on $[a, b]$, contradicting the
  previous theorem.
\end{proof}

\begin{theorem}
  $\mathbb{N}$ is not bounded above.
\end{theorem}

\begin{theorem}
  For any $\epsilon > 0$ there is a natural number $n$ with $1/n < \epsilon$.
\end{theorem}

\subsection{Uniform Continuity}

\begin{definition*}
  The function $f$ is \emph{uniformly continuous on an interval $A$} if for
  every $\epsilon > 0$ there is some $\delta > 0$ such that, for all $x$ and
  $y$ in $A$, \[
    \text{if } |x - y| < \delta, \text{ then } |f(x) - f(y)| < \epsilon.
  \]
\end{definition*}

\begin{lemma*}
  Let $a < b < c$ and let $f$ be continuous on the interval $[a, c]$. Let
  $\epsilon > 0$, and suppose that statements (i) and (ii) hold:
  \begin{enumerate}[(i)]
    \item if $x$ and $y$ are in $[a, b]$ and $|x - y| < \delta_1$, then
      $|f(x) - f(y)| < \epsilon$,
    \item if $x$ and $y$ are in $[b, c]$ and $|x - y| < \delta_2$, then
      $|f(x) - f(y)| < \epsilon$.
  \end{enumerate}
  Then there is a $\delta > 0$ such that, \[
    \text{if } x \text{ and } y \text{ are in } [a, c] \text{ and } |x - y| <
    \delta, \text{ then } |f(x) - f(y)| < \epsilon.
  \]
\end{lemma*}

\begin{theorem}
  If $f$ is continuous on $[a, b]$, then $f$ is uniformly continuous on $[a,
  b]$.
\end{theorem}
\begin{proof}
  For $\epsilon > 0$ we call $f$ \emph{$\epsilon$-good} on $[a, b]$ if there is
  some $\delta > 0$ such that, for all $y$ and $z$ in $[a, b]$, \[
    \text{if } |y - z| < \delta, \text{ then } |f(y) - f(z)| < \epsilon.
  \] We aim to prove that $f$ is $\epsilon$-good on $[a, b]$ for all $\epsilon
  > 0$.

  Consider any particular $\epsilon > 0$. Let \[
    A = \{x: a \leq x \leq b \text{ and } f \text{ is }
    \epsilon\text{-good on } [a, x]\}.
  \] Then $A \neq \emptyset$ (since $a$ is in $A$), and $A$ is bounded above
  (by $b$), so $A$ has a least upper bound $\alpha$.

  Suppose that we had $\alpha < b$. Since $f$ is continuous at $\alpha$, there
  is some $\delta_0 > 0$ such that, if $|y - \alpha| < \delta_0$, then $|f(y) -
  f(\alpha)| < \epsilon/2$. Consequently, if $|y - \alpha| < \delta_0$ and $|z
  - a| < \delta_0$, then $|f(y) - f(z)| < \epsilon$. So $f$ is surely
  $\epsilon$-good on the interval $[\alpha - \delta_0, \alpha + \delta_0]$. On
  the other hand, since $\alpha$ is the least upper bound of $A$, it is also
  clear that $f$ is $\epsilon$-good on $[a, \alpha - \delta_0]$. Then the Lemma
  implies that $f$ is $\epsilon$-good on $[a, \alpha + \delta_0]$, so $\alpha +
  \delta_0$ is in $A$, contradicting the fact that $\alpha$ is an upper bound.

  For $\alpha = b$, since $f$ is continuous at $b$, there is some $\delta_0 >
  0$ such that, if $|y - b| < \delta_0$, then $|f(y) - f(b)| < \epsilon/2$. So
  $f$ is $\epsilon$-good on $[b - \delta_0, b]$. But $b - \delta_0$ is also in
  $A$, so $f$ must be $\epsilon$-good on $[a, b - \delta_0]$. The Lemma implies
  that $f$ is $\epsilon$-good on $[a, b]$.
\end{proof}

\section{Derivatives}

\begin{definition*}
  The function $f$ is \textbf{differentiable at $A$} if \[
    \lim_{h \to 0}\frac{f(a + h) - f(a)}{h}
  \] exists. In this case the limit is denoted by $\mathbf{f'(a)}$ and is
  called the \textbf{derivative of $f$ at $a$}.
\end{definition*}

\begin{theorem}
  If $f$ is differentiable at $a$, then $f$ is continuous at $a$.
\end{theorem}

\section{Differentiation}

\begin{theorem}
  If $f$ is a constant function, $f(x) = c$, then $f'(a) = 0$ for all numbers
  $a$.
\end{theorem}

\begin{theorem}
  If $f$ is the identity function, $f(x) = x$, then $f'(a) = 1$ for all numbers
  $a$.
\end{theorem}

\begin{theorem}[The Sum Rule]
  If $f$ and $g$ are differentiable at $a$, then $f + g$ is also differentiable
  at $a$, and $(f + g)'(a) = f'(a) + g'(a)$.
\end{theorem}

\begin{theorem}[The Product Rule]
  If $f$ and $g$ are differentiable at $a$, then $(f \cdot g)'(a) = f'(a) \cdot
  g(a) + f(a) \cdot g'(a)$.
\end{theorem}

\begin{theorem}[Multiplication by Constant]
  If $g(x) = cf(x)$ and $f$ is differentiable at $a$, then $g$ is
  differentiable at $a$, and $g'(a) = c \cdot f'(a)$.
\end{theorem}

\begin{theorem}[The Power Rule]
  If $f(x) = x^n$ for some natural number $n$, then $f'(a) = na^{n-1}$ for all
  $a$.
\end{theorem}

Now, we move on to more complex theorems.

\begin{theorem}
  If $g$ is differentiable at $a$, and $g(a) \neq 0$, then $1/g$ is
  differentiable at $a$, and \[
    \left(\frac{1}{g}\right)'(a) = \frac{-g'(a)}{[g(a)]^2}.
  \]
\end{theorem}
\begin{proof}
  Before we consider \[
    \lim_{h \to 0} \frac{\left(\frac{1}{g}\right)(a + h) -
    \left(\frac{1}{g}\right)(a)}{h},
  \] it is necessary to check that $(1/g)(a + h)$ is defined for sufficiently
  small $h$. Since $g$ is, by hypothesis, differentiable at $a$, it follows
  from Theorem 9-1 that $g$ is continuous at $a$. Since $g(a) \neq 0$, it
  follows from Theorem 6-3 that there is some $\delta > 0$ such that $g(a + h)
  \neq 0$ for $|h| < \delta$. Therefore $(1/g)(a + h)$ does make sense for
  small enough $h$, and we can write
  \begin{align*}
    \lim_{h \to 0} \frac{\left( \frac{1}{g} \right)(a + h) - \left( \frac
    {1}{g} \right)(a)}{h}
    &= \lim_{h \to 0} \frac{\frac{1}{g(a + h)} - \frac{1}{g(a)}}{h} \\
    &= \lim_{h \to 0} \frac{g(a) - g(a + h)}{h[g(a) \cdot g(a + h)]} \\
    &= \lim_{h \to 0} \left[-\frac{g(a + h) - g(a)}{h} \cdot \frac{1}
    {g(a)g(a + h)}\right] \\
    &= -\lim_{h \to 0} \frac{g(a + h) - g(a)}{h} \cdot \lim_{h \to 0}
    \frac{1}{g(a) \cdot g(a + h)} \\
    &= -g'(a) \cdot \frac{1}{[g(a)]^2}
  \end{align*}
  (Notice that we have used continuity of $g$ at $a$ once again.)
\end{proof}

\begin{theorem}[The Quotient Rule]
  If $f$ and $g$ are differentiable at $a$ and $g(a) \neq 0$, then $f/g$ is
  differentiable at $a$, and \[
    \left( \frac{f}{g} \right)'(a)
    = \frac{g(a) \cdot f'(a) - f(a) \cdot g'(a)}{[g(a)]^2}.
  \]
\end{theorem}

\begin{theorem}[The Chain rule]
  If $g$ is differentiable at $a$, and $f$ is differentiable at $g(a)$, then $f
  \circ g$ is differentiable at $a$, and \[
    (f \circ g)'(a) = f'(g(a)) \cdot g'(a).
  \]
\end{theorem}
\begin{proof}
  Define a function $\phi$ as follows: \[
    \phi(h) =
    \begin{cases}
      \frac{f(g(a + h)) - f(g(a))}{g(a + h) - g(a)}, &\text{ if } g(a + h) -
      g(a) \neq 0 \\
      f'(g(a)), &\text{ if } g(a + h) - g(a) = 0.
    \end{cases}
  \] We know that $f$ is differentiable at $g(a)$, so \[
    \lim_{k \to 0} \frac{f(g(a) + k) - f(g(a))}{k} = f'(g(a)).
  \] Thus, if $\epsilon > 0$ there is some number $\delta' > 0$ such that, for
  all $k$, \[ \tag{1}
    \text{if } 0 < |k| < \delta',
    \text{ then } \left| \frac{f(g(a) + k) - f(g(a))}{k} - f'(g(a)) \right|
    < \epsilon.
  \]
  Now $g$ is differentiable at $a$ and hence continuous at $a$, so there is a
  $\delta > 0$ such that, for all $h$, \[ \tag{2}
    \text{if } |h| < \delta, \text{ then } |g(a + h) - g(a)| < \delta'.
  \] Consider now any $h$ with $|h| < \delta$. If $k = g(a + h) - g(a) \neq 0$,
  then \[
    \phi(h)
    = \frac{f(g(a + h)) - f(g(a))}{g(a + h) - g(a)}
    = \frac{f(g(a) + k) - f(g(a))}{k};
  \] it follows from (2) that $|k| < \delta'$, and hence from (1) that \[
    |\phi(h) - f'(g(a))| < \epsilon.
  \] On the other hand, if $g(a + h) - g(a) = 0$, then $\phi(h) = f'(g(a))$, so
  it is surely true that \[
    |\phi(h) - f'(g(a))| < \epsilon.
  \] We have therefore proven that $\lim_{h \to 0} \phi(h) = f'(g(a))$, so
  $\phi$ is continuous at 0.

  If $h \neq 0$, then we have \[
    \frac{f(g(a + h)) - f(g(a))}{h} = \phi(h) \cdot \frac{g(a + h) - g(a)}{h}
  \] even if $g(a + h) - g(a) = 0$ (because in that case both sides are 0).
  Therefore \begin{align*}
    (f \circ g)'(a)
    &= \lim_{h \to 0} \frac{f(g(a + h)) - f(g(a))}{h}
    = \lim_{h \to 0} \phi(h) \cdot \lim_{h \to 0} \frac{g(a + h) - g(a)}{h} \\
    &= f'(g(a)) \cdot g'(a).
  \end{align*}
\end{proof}

\section{Significance of the Derivative}

\begin{definition}
  Let $f$ be a function and $A$ a set of numbers contained in the domain of
  $f$. A point $x$ in $A$ is a \textbf{maximum point} for $f$ on $A$ if \[
    f(x) \geq f(y) \text{ for every } y \text{ in } A.
  \] The number $f(x)$ itself is called the \textbf{maximum value} of $f$ on
  $A$.
\end{definition}

\begin{theorem}
  Let $f$ be any function defined on $(a, b)$. If $x$ is a maximum (or a
  minimum) point for $f$ on $(a, b)$, and $f$ is differentiable at $x$, then
  $f'(x) = 0$.
\end{theorem}

\begin{definition}
  Let $f$ be a function, and $A$ a set of numbers contained in the domain of
  $f$. A point $x$ in $A$ is a \emph{local maximum [minimum] point} for $f$ on
  $A$ if there is some $\delta > 0$ such that $x$ is a \emph{maximum [minimum]
  point} for $f$ on $A \cap (x - \delta, x + \delta)$.
\end{definition}

\begin{theorem}
  If $f$ is defined on $(a, b)$ and has a local maximum (or minimum) at $x$,
  and $f$ is differentiable at $x$, then $f'(x) = 0$.
\end{theorem}

\begin{definition}
  A \emph{critical point} of a function $f$ is a number $x$ such that $f'(x) =
  0$. The number $f(x)$ itself is called a \emph{critical value} of $f$.
\end{definition}

\begin{theorem}[Rolle's Theorem]
  If $f$ is continuous on $[a, b]$ and differentiable on $(a, b)$, and $f(a) =
  f(b)$, then there is a number $x$ in $(a, b)$ such that $f'(x) = 0$.
\end{theorem}
\begin{proof}
  It follows from the continuity of $f$ on $[a, b]$ that $f$ has a maximum and
  a minimum value on $[a, b]$.

  Suppose first that the maximum value occurs at a point $x$ in $(a, b)$. Then
  $f'(x) = 0$ by Theorem 1, and we are done.

  Suppose next that the minimum value occurs at some point $x$ in $(a, b)$.
  Then, again, $f'(x) = 0$ by Theorem 1.

  Finally, suppose the maximum and minimum values both occur at the end points.
  Since $f(a) = f(b)$, the maximum and minimum values of $f$ are equal, so $f$
  is a constant function, and for a constant function we can choose any $x$ in
  $(a, b)$.
\end{proof}

\begin{theorem}[The Mean Value Theorem]
  If $f$ is continuous on $[a, b]$ an differentiable on $(a, b)$, then there is
  a number $x$ in $(a, b)$ such that \[
    f'(x) = \frac{f(b) - f(a)}{b - a}.
  \]
\end{theorem}
\begin{proof}
  Let \[
    h(x) = f(x) - \left[\frac{f(b) - f(a)}{b - a}\right](x - a).
  \] Clearly, $h$ is continuous on $[a, b]$ and differentiable on $(a, b)$, and
  \begin{align*}
    h(a) &= f(a), \\
    h(b) &= f(b) - \left[\frac{f(b) - f(a)}{b - a}\right](b - a) \\
         &= f(a).
  \end{align*}
  Consequently, we may apply Rolle's Theorem to $h$ and conclude that there is
  some $x$ in $(a, b)$ such that \[
    0 = h'(x) = f'(x) - \frac{f(b) - f(a)}{b - a},
  \] so that \[
    f'(x) = \frac{f(b) - f(a)}{b - a}.
  \]
\end{proof}

\begin{corollary}
  If $f$ is defined on an interval and $f'(x) = 0$ for all $x$ in the interval,
  then $f$ is constant on the interval.
\end{corollary}

\begin{corollary}
  If $f$ and $g$ are defined on the same interval, and $f'(x) = g'(x)$ for all
  $x$ in the interval, then there is some number $c$ such that $f = g + c$.
\end{corollary}

\begin{definition}
  A function is \textbf{increasing} on an interval if $f(a) < f(b)$ whenever $a$
  and $b$ are two numbers in the interval with $a < b$.
\end{definition}

\begin{corollary}
  If $f'(x) > 0$ for all $x$ in an interval, then $f$ is increasing on the
  interval; if $f'(x) < 0$ for all $x$ in the interval, then $f$ is decreasing
  on the interval.
\end{corollary}

\begin{theorem}
  Suppose $f'(a) = 0$. If $f''(a) > 0$, then $f$ has a local minimum at $a$; if
  $f''(a) < 0$, then $f$ has a local maximum at $a$.
\end{theorem}

Theorem 5 automatically proves a partial converse of itself:

\begin{theorem}
  Suppose $f''(a)$ exists. If $f$ has a local minimum at $a$, then $f''(a) \geq
  0$; if $f$ has a local maximum at $a$, then $f''(a) \leq 0$.
\end{theorem}

The remainder of the theorems deal with three consequences of the Mean Value
Theorem.

\begin{theorem}
  Suppose that $f$ is continuous at $a$, and that $f'(x)$ exists for all $x$ in
  some interval containing $a$, except perhaps for $x = a$. Suppose, moreover,
  that $\lim_{x \to a} f'(x)$ exists. Then $f'(a)$ also exists, and $f'(a) =
  \lim_{x \to a} f'(x)$.
\end{theorem}

\begin{proof}
  For sufficiently small $h > 0$ the function $f$ will be continuous on
  $[a, a + h]$ and differentiable on $(a, a + h)$. By the Mean Value Theorem
  there is a number $\alpha_h$ in $(a, a + h)$ such that \[
    \frac{f(a + h) - f(a)}{h} = f'(\alpha_h).
  \] Let $L = \lim_{x \to a} f'(x)$; for any $\epsilon > 0$ there exists a
  $\delta > 0$ such that if $0 < |x - a| < \delta$ then $|f'(x) - L| <
  \epsilon$. Similarly, if $0 < |\alpha_h - a| < \delta$ then $|f'(\alpha_h) -
  L| < \epsilon$. By setting $h = x - a$, as $a < \alpha_h < a + h$ this means
  that
  \begin{align*}
    \text{if } &0 < |x - a| = |h| < \delta, \\
    \text{then } &|f'(\alpha_h) - L|
    = \left|\frac{f(x) - f(a)}{x - a} - L\right|
    = \left|\frac{f(a + h) - f(a)}{h} - L\right| < \epsilon,
  \end{align*}
  and therefore $f'(a) = \lim_{x \rightarrow a} f'(x)$.
\end{proof}

The next theorem, a generalization of the Mean Value Theorem, is of interest
mainly because of its applications.

\begin{theorem}[The Cauchy Mean Value Theorem]
  If $f$ and $g$ are continuous on $[a, b]$ and differentiable on $(a, b)$,
  then there is a number $x$ in $(a, b)$ such that \[
    [f(b) - f(a)]g'(x) = [g(b) - g(a)]f'(x).
  \]
\end{theorem}
\begin{proof}
  Let $h(x) = f(x)[g(b) - g(a)] - g(x)[f(b) - f(a)]$. Then $h$ is continuous on
  $[a, b]$, differentiable on $(a, b)$, and $h(a) = f(a)g(b) - g(a)f(b) =
  h(b)$. It follows from Rolle's Theorem that $h'(x) = 0$ for some $x$ in $(a,
  b)$, which means that $0 = f'(x)[g(b) - g(a)] - g'(x)[f(b) - f(a)]$.
\end{proof}

\begin{theorem}[L'H{\^o}pital's Rule]
  Suppose that \[
    \lim_{x \to a} f(x) = 0 \text{ and } \lim_{x \to a} g(x) = 0,
  \] and suppose also that $\lim_{x \to a} f'(x)/g'(x)$ exists. Then
  $\lim_{x \to a} f(x)/g(x)$ exists, and \[
    \lim_{x \to a} \frac{f(x)}{g(x)} = \lim_{x \to a} \frac{f'(x)}{g'(x)}.
  \]
\end{theorem}

\begin{proof}
  The hypothesis that $\lim_{x \to a} f'(x)/g'(x)$ exists contains two implicit
  assumptions:
  \begin{enumerate}
    \item there is an interval $(a - \delta, a + \delta)$ such that $f'(x)$ and
      $g'(x)$ exist for all $x$ in $(a - \delta, a + \delta)$ except, perhaps,
      for $x = a$,
    \item in this interval $g'(x) \neq 0$ with, once again, the possible
      exception of $x = a$.
  \end{enumerate}
  On the other hand, $f$ and $g$ are not even assumed to be defined at $a$. If
  we define $f(a) = g(a) = 0$ (changing the previous value of $f(a)$ and
  $g(a)$, if necessary), then $f$ and $g$ are continuous at $a$. If $a < x < a
  + \delta$, applying the Mean Value Theorem to $g$ on $[a, x]$ we see that
  $g(x) \neq 0$, for if $g(x) = 0$ there would be some $x_1$ in $(a, x)$ with
  $g'(x_1) = 0$, contradicting (2). Now applying the Cauchy Mean Value Theorem
  to $f$ and $g$, we see that there is a number $\alpha_x$ in $(a, x)$ such
  that \[
    [f(x) - 0]g'(\alpha_x) = [g(x) - 0]f'(\alpha_x)
    \iff \frac{f(x)}{g(x)} = \frac{f'(\alpha_x)}{g'(\alpha_x)}.
  \]

  Let $L = \lim_{x \rightarrow a} \frac{f'(x)}{g'(x)}$; for any $\epsilon > 0$
  there exists a $\delta > 0$ such that \[
    \text{if } 0 < |x - a| < \delta,
    \text{ then } \left|\frac{f'(x)}{g'(x)} - L\right| < \epsilon.
  \] Similarly, \[
    \text{if } 0 < |\alpha_x - a| < \delta,
    \text{ then } \left|\frac{f'(\alpha_x)}{g'(\alpha_x)} - L\right|
    < \epsilon.
  \] As $a < \alpha_x < x$ this means that
  \begin{align*}
    \text{if } &0 < |x - a| < \delta, \\
    \text{then } &\left|\frac{f'(\alpha_x)}{g'(\alpha_x)} - L\right|
    = \left|\frac{f(x)}{g(x)} - L\right| < \epsilon,
  \end{align*}
  and therefore \[
    \lim_{x \to a} \frac{f(x)}{g(x)} = \lim_{x \to a} \frac{f'(x)}{g'(x)}.
  \]
\end{proof}

\subsection{Convexity and Concavity}

\begin{definition}
  A function $f$ is \textbf{convex} on an interval, if for all $a$ and $b$ in
  the interval, the line segment joining $(a, f(a))$ and $(b, f(b))$ lies above
  the graph of $f$.
\end{definition}

\begin{definition}
  A function $f$ is \textbf{convex} on an interval, if for $a$, $x$, and $b$ in
  the interval with $a < x < b$ we have \[
    \frac{f(x) - f(a)}{x - a} < \frac{f(b) - f(a)}{b - a}.
  \]
\end{definition}

\begin{theorem}
  Let $f$ be convex. If $f$ is differentiable at $a$, then the graph of $f$
  lies above the tangent line through $(a, f(a))$, except at $(a, f(a))$
  itself. If $a < b$ and $f$ is differentiable at $a$ and $b$, then $f'(a) <
  f'(b)$.
\end{theorem}
\begin{proof}
  If $0 < h_1 < h_2$, then by definition of a convex function \[
    \frac{f(a + h_1) - f(a)}{h_1} < \frac{f(a + h_2) - f(a)}{h_2}.
  \] This shows that the values of \[
    \frac{f(a + h) - f(a)}{h}
  \] decreases as $h \to 0^+$. Consequently, \[
    f'(a) < \frac{f(a + h) - f(a)}{h} \text{ for } h > 0.
  \] But this means that for $h > 0$ the secant line through $(a, f(a))$ and
  $(a + h, f(a + h))$ has larger slope than the tangent line through $(a,
  f(a))$, which implies that $(a + h, f(a + h))$ lies above the tangent line.
  A similar argument can be made for negative $h$.

  Now suppose that $a < b$. Then it is clear that
  \begin{align*}
    f'(a) &< \frac{f(a + (b - a)) - f(a)}{b - a} \text{ since } b - a > 0 \\
          &= \frac{f(b) - f(a)}{b - a}
  \end{align*}
  and similarly it can be shown that $f'(b) > \frac{f(b) - f(a)}{b - a}$.
  Combining these inequalities, we obtain $f'(a) < f'(b)$.
\end{proof}

Theorem 1 has two converses. We begin with a lemma stating that if $f'$ is
increasing, then the graph of $f$ lies below any secant line which happens to
be horizontal.

\begin{lemma*}
  Suppose $f$ is differentiable and $f'$ is increasing. If $a < b$ and $f(a) =
  f(b)$, then $f(x) < f(a) = f(b)$ for $a < x < b$.
\end{lemma*}
\begin{proof}
  Suppose first that $f(x) > f(a) = f(b)$ for some $x$ in $(a, b)$. Then the
  maximum of $f$ on $[a, b]$ occurs at some point $x_0$ in $(a, b)$ with
  $f(x_0) > f(a)$ and $f'(x_0) = 0$. On the other hand, applying the Mean Value
  Theorem to the interval $[a, x_0]$, we find that there is $x_1$ with $a < x_1
  < x_0$ and \[
    f'(x_1) = \frac{f(x_0) - f(a)}{x_0 - a} > 0,
  \] contradicting the fact that $f'$ is increasing. This proves that $f(x)
  \leq f(a) = f(b)$ for $a < x < b$.

  Suppose now that $f(x) = f(a)$ for some $x$ in $(a, b)$. We know that $f$ is
  not constant on $[a, x]$, so there is some $x_1$ with $a < x_1 < x$ and
  $f(x_1) < f(a)$. Applying the Mean Value Theorem to $[x_1, x]$ we conclude
  that there is $x_2$ with $x_1 < x_2 < x$ and \[
    f'(x_2) = \frac{f(x) - f(x_1)}{x - x_1} > 0.
  \] On the other hand, $f'(x) = 0$, since a local maximum occurs at $x$. Again
  this contradicts the hypothesis that $f'$ is increasing.
\end{proof}

\begin{theorem}
  If $f$ is differentiable and $f'$ is increasing, then $f$ is convex.
\end{theorem}
\begin{proof}
  Let $a < b$. Define $g$ by \[
    g(x) = f(x) - \frac{f(b) - f(a)}{b - a}(x - a).
  \] It is easy to see that $g'$ is also increasing; moreover, $g(a) = g(b) =
  f(a)$. Applying the lemma to $g$ we conclude that \[
    g(x) < f(a) \text{ if } a < x < b.
  \] In other words, if $a < x < b$, then \[
    f(x) - \frac{f(b) - f(a)}{b - a}(x - a) < f(a)
  \] or \[
    \frac{f(x) - f(a)}{x - a} < \frac{f(b) - f(a)}{b - a}.
  \] Hence $f$ is convex.
\end{proof}

\begin{theorem}
  If $f$ is differentiable and the graph of $f$ lies above each tangent line
  except at the point of contact, then $f$ is convex.
\end{theorem}
\begin{proof}
  Let $a < b$. Since the tangent line at $(a, f(a))$ is the graph of the
  function $g(x) = f'(a)(x - a) + f(a)$, and since $(b, f(b))$ lies above the
  tangent line, we have \[
    \label{thm11-3-ine1} \tag{1}
    f(b) > f'(a)(b - a) + f(a).
  \] Similarly, since the tangent line at $(b, f(b))$ is the graph of the
  function $h(x) = f'(b)(x - b) + f(b)$, and $(a, f(a))$ lies above the tangent
  line at $(b, f(b))$, we have \[
    \label{thm11-3-ine2} \tag{2}
    f(a) > f'(b)(a - b) + f(b).
  \] It follows from \eqref{thm11-3-ine1} and \eqref{thm11-3-ine2} that $f'(a)
  < f'(b)$. Following from Theorem 2, it is proven that $f$ is convex.
\end{proof}

\section{Inverse Functions}

\begin{definition}
  A function $f$ is \emph{one-one} if $f(a) \neq f(b)$ whenever $a \neq b$.
\end{definition}

\begin{definition}
  For any function $f$, the \emph{inverse} of $f$, denoted by $f^{-1}$, is the
  set of all pairs $(a, b)$ for which the pair $(b, a)$ is in $f$.
\end{definition}

\begin{theorem}
  $f^{-1}$ is a function if and only if $f$ is one-one.
\end{theorem}

\begin{theorem}
  If $f$ is continuous and one-one of an interval, then $f$ is either
  increasing or decreasing on that interval.
\end{theorem}
\begin{proof}
  Let $a_0 < b_0$ be two numbers in the interval. Since $f$ is one-one, we know
  that
  \begin{align*}
    \text{either} &&\text{ (i) } &f(b_0) - f(a_0) > 0 \\
    \text{or} &&\text{ (ii) } &f(b_0) - f(a_0) < 0.
  \end{align*}
  We will assume that (i) is true, and show that the same inequality holds for
  any $a_1 < b_1$ in the interval, so that $f$ is increasing. (A similar
  argument shows that if (ii) is true, then $f$ is decreasing.)

  Let
  \begin{align*}
    x_t &= (1 - t)a_0 + ta_1 \\
    y_t &= (1 - t)b_0 + tb_1
  \end{align*}
  for $0 \leq t \leq 1$. Then $x_0 = a_0$ and $x_1 = a_1$ and the points $x_t$
  all lie between $a_0$ and $a_1$. An analogous statement holds for $y_t$. So
  $x_t$ and $y_t$ are all in the domain of $f$. Moreover, since $a_0 < b_0$ and
  $a_1 < b_1$, we also have \[
    x_t < y_t \text{ for } 0 \leq t \leq 1.
  \] Now consider the function \[
    g(t) = f(y_t) - f(x_t) \text{ for } 0 \leq t \leq 1.
  \] Using Theorem 6-2, it is easy to see that $g$ is continuous on $[0, 1]$.
  Moreover, $g(t)$ is never 0, since $x_t < y_t$ and $f$ is one-one.
  Consequently, $g(t)$ is either positive for all $t$ in $[0, 1]$ or negative
  for all $t$ in $[0, 1]$ (otherwise, by the Intermediate Value Theorem it
  would also be 0 somewhere in $[0, 1]$). But $g(0) > 0$ by (i). So also $g(1)
  > 0$, which means that (i) also holds for $a_1, b_1$.
\end{proof}

Moving on, we now consider which important properties of a one-one function are
inherited by its inverse.

\begin{theorem}
  If $f$ is continuous and one-one on an interval, then $f^{-1}$ is also
  continuous.
\end{theorem}
\begin{proof}
  We know by Theorem 2 that $f$ is either increasing or decreasing. We might as
  well assume that $f$ is increasing, since we can then take care of the other
  case by applying the usual trick of considering $-f$.

  We must show that \[
    \lim_{x \to b} f^{-1}(x) = f^{-1}(b)
  \] for each $b$ in the domain of $f^{-1}$. Such a number $b$ is of the form
  $f(a)$ for some $a$ in the domain of $f$. For any $\epsilon > 0$, we want to
  find a $\delta > 0$ such that, for all $x$, \[
    \text{if } f(a) - \delta < x < f(a) + \delta,
    \text{ then } a - \epsilon < f^{-1}(x) < a + \epsilon.
  \] Since $a - \epsilon < a < a + \epsilon$, it follows that $f(a - \epsilon)
  < f(a) < f(a + \epsilon)$; we let $\delta$ be the smaller of $f(a + \epsilon)
  - f(a)$ and $f(a) - f(a - \epsilon)$.

  Our choice of $\delta$ ensures that \[
    f(a - \epsilon)
    \leq f(a) - \delta \text{ and } f(a) + \delta \leq f(a + \epsilon).
  \] Consequently, if \[
    f(a) - \delta < x < f(a) + \delta.
  \] then \[
    f(a - \epsilon) < x < f(a + \epsilon).
    \] Since $f$ is increasing, $f^{-1}$ is also increasing, and we obtain \[
    f^{-1}(f(a - \epsilon)) < f^{-1}(x) < f^{-1}(f(a + \epsilon)),
  \] i.e., \[
    a - \epsilon < f^{-1}(x) < a + \epsilon,
  \] which is precisely what we want.
\end{proof}

The proof of differentiability is more involved.

\begin{theorem}
  If $f$ is a continuous one-one function defined on an interval and
  $f'(f^{-1}(a)) = 0$, then $f^{-1}$ is \emph{not} differentiable at $a$.
\end{theorem}

\begin{theorem}
  Let $f$ be a continuous one-one function defined on an interval, and suppose
  that $f$ is differentiable at $f^{-1}(b)$, with derivative $f'(f^{-1}(b))
  \neq 0$. Then $f^{-1}$ is differentiable at $b$, and \[
    (f^{-1})'(b) = \frac{1}{f'(f^{-1}(b))}.
  \]
\end{theorem}
\begin{proof}
  Let $b = f(a)$. Then \[
    \lim_{h \to 0} \frac{f^{-1}(b + h) - f^{-1}(b)}{h}
    = \lim_{h \to 0} \frac{f^{-1}(b + h) - a}{h}.
  \] Now every number $b + h$ in the domain of $f^{-1}$ can be written in the
  form \[
    b + h = f(a + k)
  \] for an unique $k$. Then \[
    \lim_{h \to 0} \frac{f^{-1}(b + h) - a}{h}
    = \lim_{h \to 0} \frac{f^{-1}(f(a + k)) - a}{f(a + k) - b}
    = \lim_{h \to 0} \frac{k}{f(a + k) - f(a)}.
  \] It is not hard to get an explicit expression for k; since \[
    b + h = f(a + k)
  \] we have \[
    f^{-1}(b + h) = a + k
  \] or \[
    k = f^{-1}(b + h) - f^{-1}(b).
  \] Now by Theorem 3, the function $f^{-1}$ is continuous at $b$. This means
  that $k$ approaches 0 as $h$ approaches 0. Since \[
    \lim_{k \to 0} \frac{f(a + k) - f(a)}{k} = f'(a) = f'(f^{-1}(b)) \neq 0,
  \] this implies that \[
    (f^{-1})'(b) = \frac{1}{f'(f^{-1}(b))}.
  \]
\end{proof}

\section{Integrals}

\begin{definition}
  A function $f$ is \emph{one-one} if $f(a) \neq f(b)$ whenever $a \neq b$.
\end{definition}

\begin{definition}
  For any function $f$, the \emph{inverse} of $f$, denoted by $f^{-1}$, is the
  set of all pairs $(a, b)$ for which the pair $(b, a)$ is in $f$.
\end{definition}

\begin{theorem}
  $f^{-1}$ is a function if and only if $f$ is one-one.
\end{theorem}

\begin{theorem}
  If $f$ is continuous and one-one of an interval, then $f$ is either
  increasing or decreasing on that interval.
\end{theorem}
\begin{proof}
  Let $a_0 < b_0$ be two numbers in the interval. Since $f$ is one-one, we know
  that
  \begin{align*}
    \text{either} &&\text{ (i) } &f(b_0) - f(a_0) > 0 \\
    \text{or} &&\text{ (ii) } &f(b_0) - f(a_0) < 0.
  \end{align*}
  We will assume that (i) is true, and show that the same inequality holds for
  any $a_1 < b_1$ in the interval, so that $f$ is increasing. (A similar
  argument shows that if (ii) is true, then $f$ is decreasing.)

  Let
  \begin{align*}
    x_t &= (1 - t)a_0 + ta_1 \\
    y_t &= (1 - t)b_0 + tb_1
  \end{align*}
  for $0 \leq t \leq 1$. Then $x_0 = a_0$ and $x_1 = a_1$ and the points $x_t$
  all lie between $a_0$ and $a_1$. An analogous statement holds for $y_t$. So
  $x_t$ and $y_t$ are all in the domain of $f$. Moreover, since $a_0 < b_0$ and
  $a_1 < b_1$, we also have \[
    x_t < y_t \text{ for } 0 \leq t \leq 1.
  \] Now consider the function \[
    g(t) = f(y_t) - f(x_t) \text{ for } 0 \leq t \leq 1.
  \] Using Theorem 6-2, it is easy to see that $g$ is continuous on $[0, 1]$.
  Moreover, $g(t)$ is never 0, since $x_t < y_t$ and $f$ is one-one.
  Consequently, $g(t)$ is either positive for all $t$ in $[0, 1]$ or negative
  for all $t$ in $[0, 1]$ (otherwise, by the Intermediate Value Theorem it
  would also be 0 somewhere in $[0, 1]$). But $g(0) > 0$ by (i). So also $g(1)
  > 0$, which means that (i) also holds for $a_1, b_1$.
\end{proof}

Moving on, we now consider which important properties of a one-one function are
inherited by its inverse.

\begin{theorem}
  If $f$ is continuous and one-one on an interval, then $f^{-1}$ is also
  continuous.
\end{theorem}
\begin{proof}
  We know by Theorem 2 that $f$ is either increasing or decreasing. We might as
  well assume that $f$ is increasing, since we can then take care of the other
  case by applying the usual trick of considering $-f$.

  We must show that \[
    \lim_{x \to b} f^{-1}(x) = f^{-1}(b)
  \] for each $b$ in the domain of $f^{-1}$. Such a number $b$ is of the form
  $f(a)$ for some $a$ in the domain of $f$. For any $\epsilon > 0$, we want to
  find a $\delta > 0$ such that, for all $x$, \[
    \text{if } f(a) - \delta < x < f(a) + \delta,
    \text{ then } a - \epsilon < f^{-1}(x) < a + \epsilon.
  \] Since $a - \epsilon < a < a + \epsilon$, it follows that $f(a - \epsilon)
  < f(a) < f(a + \epsilon)$; we let $\delta$ be the smaller of $f(a + \epsilon)
  - f(a)$ and $f(a) - f(a - \epsilon)$.

  Our choice of $\delta$ ensures that \[
    f(a - \epsilon)
    \leq f(a) - \delta \text{ and } f(a) + \delta \leq f(a + \epsilon).
  \] Consequently, if \[
    f(a) - \delta < x < f(a) + \delta.
  \] then \[
    f(a - \epsilon) < x < f(a + \epsilon).
    \] Since $f$ is increasing, $f^{-1}$ is also increasing, and we obtain \[
    f^{-1}(f(a - \epsilon)) < f^{-1}(x) < f^{-1}(f(a + \epsilon)),
  \] i.e., \[
    a - \epsilon < f^{-1}(x) < a + \epsilon,
  \] which is precisely what we want.
\end{proof}

The proof of differentiability is more involved.

\begin{theorem}
  If $f$ is a continuous one-one function defined on an interval and
  $f'(f^{-1}(a)) = 0$, then $f^{-1}$ is \emph{not} differentiable at $a$.
\end{theorem}

\begin{theorem}
  Let $f$ be a continuous one-one function defined on an interval, and suppose
  that $f$ is differentiable at $f^{-1}(b)$, with derivative $f'(f^{-1}(b))
  \neq 0$. Then $f^{-1}$ is differentiable at $b$, and \[
    (f^{-1})'(b) = \frac{1}{f'(f^{-1}(b))}.
  \]
\end{theorem}
\begin{proof}
  Let $b = f(a)$. Then \[
    \lim_{h \to 0} \frac{f^{-1}(b + h) - f^{-1}(b)}{h}
    = \lim_{h \to 0} \frac{f^{-1}(b + h) - a}{h}.
  \] Now every number $b + h$ in the domain of $f^{-1}$ can be written in the
  form \[
    b + h = f(a + k)
  \] for an unique $k$. Then \[
    \lim_{h \to 0} \frac{f^{-1}(b + h) - a}{h}
    = \lim_{h \to 0} \frac{f^{-1}(f(a + k)) - a}{f(a + k) - b}
    = \lim_{h \to 0} \frac{k}{f(a + k) - f(a)}.
  \] It is not hard to get an explicit expression for k; since \[
    b + h = f(a + k)
  \] we have \[
    f^{-1}(b + h) = a + k
  \] or \[
    k = f^{-1}(b + h) - f^{-1}(b).
  \] Now by Theorem 3, the function $f^{-1}$ is continuous at $b$. This means
  that $k$ approaches 0 as $h$ approaches 0. Since \[
    \lim_{k \to 0} \frac{f(a + k) - f(a)}{k} = f'(a) = f'(f^{-1}(b)) \neq 0,
  \] this implies that \[
    (f^{-1})'(b) = \frac{1}{f'(f^{-1}(b))}.
  \]
\end{proof}

\section{The Fundamental Theorem of Calculus}

\begin{theorem}[The First Fundamental Theorem of Calculus]
  Let $f$ be integrable on $[a, b]$, and define $F$ on $[a, b]$ by \[
    F(x) = \int_a^x f.
  \] If $f$ is continuous at $c$ in $[a, b]$, then $F$ is differentiable at
  $c$, and \[
    F'(c) = f(c).
  \] (If $c = a$ or $b$, then $F'(c)$ is understood to mean the right- or
  left-hand derivative of $F$.)
\end{theorem}
\begin{proof}
  We will assume that $c$ is in $(a, b)$; the easy modifications for $c = a$ or
  $b$ may be supplied by the reader. By definition, \[
    F'(c) = \lim_{h \to 0}\frac{F(c + h) - F(c)}{h}.
  \] Suppose first that $h > 0$. Then \[
    F(c + h) - F(c) = \int_c^{c + h} f.
  \] Define $m_h$ and $M_h$ as follows:
  \begin{align*}
    m_h &= \inf\{f(x): c \leq x \leq c + h\}, \\
    M_h &= \sup\{f(x): c \leq x \leq c + h\}.
  \end{align*}
  It follows from Theorem 13-7 that \[
    m_h \cdot h \leq \int_c^{c + h} f \leq M_h \cdot h.
  \] Therefore \[
    m_h \leq \frac{F(c + h) - F(c)}{h} \leq M_h.
  \] If $h < 0$, only a few details of the argument have to be changed. Let
  \begin{align*}
    m_h &= \inf\{f(x): c + h \leq x \leq c\}, \\
    M_h &= \sup\{f(x): c + h \leq x \leq c\}.
  \end{align*}
  Then \[
    m_h \cdot (-h) \leq \int_{c + h}^c f \leq M_h \cdot (-h).
  \] Since \[
    F(c + h) - F(c) = \int_c^{c + h} f = -\int_{c + h}^c f
  \] this yields \[
    m_h \cdot h \geq \int_c^{c + h} f \geq M_h \cdot h.
  \] Since $h < 0$, dividing by $h$ reverses the inequality again, yielding the
  same result as before: \[
    m_h \leq \frac{F(c + h) - F(c)}{h} \leq M_h.
  \] This inequality is true for any integrable function, continuous or not.
  Since $f$ is continuous at $c$, however, \[
    \lim_{h \to 0} m_h = \lim_{h \to 0} M_h = f(c),
  \] and this proves that \[
    F'(c) = \lim_{h \to 0} \frac{F(c + h) - F(c)}{h} = f(c).
  \]
\end{proof}

Theoerm 1 has a simple corollary which frequently reduces computation of
integrals to a trivality.

\begin{corollary}
  If $f$ is continuous on $[a, b]$ and $f = g'$ for some function $g$, then \[
    \int_a^b f = g(b) - g(a).
  \]
\end{corollary}

Theorem 2 is a somewhat strong result than the corollary to Theorem 1, but
proven in an entirely different manner.

\begin{theorem}[The Second Fundamental Theorem of Calculus]
  If $f$ is integrable on $[a, b]$ and $f = g'$ for some function $g$, then \[
    \int_a^b f = g(b) - g(a).
  \]
\end{theorem}
\begin{proof}
  Let $P = \{t_0, \ldots, t_n\}$ be any partition of $[a, b]$. By the Mean
  Value Theorem there is a point $x_i$ in $[t_{i-1}, t_i]$ such that
  \begin{align*}
    g(t_i) - g(t_{i-1})
    &= g'(x_i)(t_i - t_{i-1}) \\
    &= f(x_i)(t_i - t_{i-1}).
  \end{align*}
  If
  \begin{align*}
    m_i &= \inf\{f(x): t_{i-1} \leq x \leq t_i\}, \\
    M_i &= \sup\{f(x): t_{i-1} \leq x \leq t_i\},
  \end{align*}
  then clearly \[
    m_i(t_i - t_{i-1}) \leq f(x_i)(t_i - t_{i-1}) \leq M_i(t_i - t_{i-1}),
  \] that is, \[
    m_i(t_i - t_{i-1}) \leq g(t_i) - g(t_{i-1}) \leq M_i(t_i - t_{i-1}).
  \] Adding these equations for $i = 1, \ldots, n$ we obtain \[
    \sum_{i=1}^n m_i(t_i - t_{i-1})
    \leq g(b) - g(a)
    \leq \sum_{i=1}^n M_i(t_i - t_{i-1})
  \] so that \[
    L(f, P) \leq g(b) - g(a) \leq U(f, P)
  \] for every partition $P$. But this means that \[
    g(b) - g(a) = \int_a^b f.
  \]
\end{proof}

If $f$ is any bounded function on $[a, b]$, then \[
  \sup\{L(f, P)\} \text{ and } \inf\{U(f, P)\}
\] will both exist, even if $f$ is not integrable. These numbers
are called the \emph{lowest integral} and the \emph{upper integral} of $f$ on
$[a, b]$ respectively, and will be denoted by \[
  \textbf{L}\int_a^b f \text{ and } \textbf{U}\int_a^b f.
\]

\begin{theorem*}[cf. Theorem 13-3]
  If $f$ is continuous on $[a, b]$, then $f$ is integrable on $[a, b]$.
\end{theorem*}

\begin{proof}
  Define functions $L$ and $U$ on $[a, b]$ by \[
    L(x) = \textbf{L}\int_a^x f \text{ and } U(x) = \textbf{U}\int_a^x f.
  \] Let $x$ be in $(a, b)$. If $h > 0$ and
  \begin{align*}
    m_h &= \inf\{f(t): x \leq t \leq x + h\}, \\
    M_h &= \sup\{f(t): x \leq t \leq x + h\},
  \end{align*}
  then \[
    m_h \cdot h \leq \textbf{L}\int_x^{x + h} f \leq \textbf{U}\int_x^{x + h} f
    \leq M_h \cdot h,
  \] so \[
    m_h \cdot h \leq L(x + h) - L(x) \leq U(x + h) - U(x) \leq M_h \cdot h
  \] so \[
    m_h \leq \frac{L(x + h) - L(x)}{h} \leq \frac{U(x + h) - U(x)}{h} \leq M_h.
  \] If $h < 0$ and
  \begin{align*}
    m_h &= \inf\{f(t): x + h \leq t \leq x\}, \\
    M_h &= \sup\{f(t): x + h \leq t \leq x\},
  \end{align*}
  one obtains the same inequality, precisely as in the proof of
  Theorem 1.

  Since $f$ is continuous at $x$, we have \[
    \lim_{h \to 0} m_h = \lim_{h \to 0} M_h = f(x),
  \] and this proves that \[
    L'(x) = U'(x) = f(x) \text{ for } x \text{ in } (a, b).
  \] This means that there is a number $c$ such that \[
    U(x) = L(x) + c \text{ for all } x \text{ in } [a, b].
  \] Since \[
    U(a) = L(a) = 0,
  \] the number $c$ must equal 0, so \[
    U(x) = L(x) \text{ for all } x \text{ in } [a, b].
  \] In particular, \[
    \textbf{U}\int_a^b f = U(b) = L(b) = \textbf{L}\int_a^b f,
  \] and this means that $f$ is integrable on $[a, b]$.
\end{proof}

\section{The Trigonometric Functions}

\begin{theorem}[The First Fundamental Theorem of Calculus]
  Let $f$ be integrable on $[a, b]$, and define $F$ on $[a, b]$ by \[
    F(x) = \int_a^x f.
  \] If $f$ is continuous at $c$ in $[a, b]$, then $F$ is differentiable at
  $c$, and \[
    F'(c) = f(c).
  \] (If $c = a$ or $b$, then $F'(c)$ is understood to mean the right- or
  left-hand derivative of $F$.)
\end{theorem}
\begin{proof}
  We will assume that $c$ is in $(a, b)$; the easy modifications for $c = a$ or
  $b$ may be supplied by the reader. By definition, \[
    F'(c) = \lim_{h \to 0}\frac{F(c + h) - F(c)}{h}.
  \] Suppose first that $h > 0$. Then \[
    F(c + h) - F(c) = \int_c^{c + h} f.
  \] Define $m_h$ and $M_h$ as follows:
  \begin{align*}
    m_h &= \inf\{f(x): c \leq x \leq c + h\}, \\
    M_h &= \sup\{f(x): c \leq x \leq c + h\}.
  \end{align*}
  It follows from Theorem 13-7 that \[
    m_h \cdot h \leq \int_c^{c + h} f \leq M_h \cdot h.
  \] Therefore \[
    m_h \leq \frac{F(c + h) - F(c)}{h} \leq M_h.
  \] If $h < 0$, only a few details of the argument have to be changed. Let
  \begin{align*}
    m_h &= \inf\{f(x): c + h \leq x \leq c\}, \\
    M_h &= \sup\{f(x): c + h \leq x \leq c\}.
  \end{align*}
  Then \[
    m_h \cdot (-h) \leq \int_{c + h}^c f \leq M_h \cdot (-h).
  \] Since \[
    F(c + h) - F(c) = \int_c^{c + h} f = -\int_{c + h}^c f
  \] this yields \[
    m_h \cdot h \geq \int_c^{c + h} f \geq M_h \cdot h.
  \] Since $h < 0$, dividing by $h$ reverses the inequality again, yielding the
  same result as before: \[
    m_h \leq \frac{F(c + h) - F(c)}{h} \leq M_h.
  \] This inequality is true for any integrable function, continuous or not.
  Since $f$ is continuous at $c$, however, \[
    \lim_{h \to 0} m_h = \lim_{h \to 0} M_h = f(c),
  \] and this proves that \[
    F'(c) = \lim_{h \to 0} \frac{F(c + h) - F(c)}{h} = f(c).
  \]
\end{proof}

Theoerm 1 has a simple corollary which frequently reduces computation of
integrals to a trivality.

\begin{corollary}
  If $f$ is continuous on $[a, b]$ and $f = g'$ for some function $g$, then \[
    \int_a^b f = g(b) - g(a).
  \]
\end{corollary}

Theorem 2 is a somewhat strong result than the corollary to Theorem 1, but
proven in an entirely different manner.

\begin{theorem}[The Second Fundamental Theorem of Calculus]
  If $f$ is integrable on $[a, b]$ and $f = g'$ for some function $g$, then \[
    \int_a^b f = g(b) - g(a).
  \]
\end{theorem}
\begin{proof}
  Let $P = \{t_0, \ldots, t_n\}$ be any partition of $[a, b]$. By the Mean
  Value Theorem there is a point $x_i$ in $[t_{i-1}, t_i]$ such that
  \begin{align*}
    g(t_i) - g(t_{i-1})
    &= g'(x_i)(t_i - t_{i-1}) \\
    &= f(x_i)(t_i - t_{i-1}).
  \end{align*}
  If
  \begin{align*}
    m_i &= \inf\{f(x): t_{i-1} \leq x \leq t_i\}, \\
    M_i &= \sup\{f(x): t_{i-1} \leq x \leq t_i\},
  \end{align*}
  then clearly \[
    m_i(t_i - t_{i-1}) \leq f(x_i)(t_i - t_{i-1}) \leq M_i(t_i - t_{i-1}),
  \] that is, \[
    m_i(t_i - t_{i-1}) \leq g(t_i) - g(t_{i-1}) \leq M_i(t_i - t_{i-1}).
  \] Adding these equations for $i = 1, \ldots, n$ we obtain \[
    \sum_{i=1}^n m_i(t_i - t_{i-1})
    \leq g(b) - g(a)
    \leq \sum_{i=1}^n M_i(t_i - t_{i-1})
  \] so that \[
    L(f, P) \leq g(b) - g(a) \leq U(f, P)
  \] for every partition $P$. But this means that \[
    g(b) - g(a) = \int_a^b f.
  \]
\end{proof}

If $f$ is any bounded function on $[a, b]$, then \[
  \sup\{L(f, P)\} \text{ and } \inf\{U(f, P)\}
\] will both exist, even if $f$ is not integrable. These numbers
are called the \emph{lowest integral} and the \emph{upper integral} of $f$ on
$[a, b]$ respectively, and will be denoted by \[
  \textbf{L}\int_a^b f \text{ and } \textbf{U}\int_a^b f.
\]

\begin{theorem*}[cf. Theorem 13-3]
  If $f$ is continuous on $[a, b]$, then $f$ is integrable on $[a, b]$.
\end{theorem*}

\begin{proof}
  Define functions $L$ and $U$ on $[a, b]$ by \[
    L(x) = \textbf{L}\int_a^x f \text{ and } U(x) = \textbf{U}\int_a^x f.
  \] Let $x$ be in $(a, b)$. If $h > 0$ and
  \begin{align*}
    m_h &= \inf\{f(t): x \leq t \leq x + h\}, \\
    M_h &= \sup\{f(t): x \leq t \leq x + h\},
  \end{align*}
  then \[
    m_h \cdot h \leq \textbf{L}\int_x^{x + h} f \leq \textbf{U}\int_x^{x + h} f
    \leq M_h \cdot h,
  \] so \[
    m_h \cdot h \leq L(x + h) - L(x) \leq U(x + h) - U(x) \leq M_h \cdot h
  \] so \[
    m_h \leq \frac{L(x + h) - L(x)}{h} \leq \frac{U(x + h) - U(x)}{h} \leq M_h.
  \] If $h < 0$ and
  \begin{align*}
    m_h &= \inf\{f(t): x + h \leq t \leq x\}, \\
    M_h &= \sup\{f(t): x + h \leq t \leq x\},
  \end{align*}
  one obtains the same inequality, precisely as in the proof of
  Theorem 1.

  Since $f$ is continuous at $x$, we have \[
    \lim_{h \to 0} m_h = \lim_{h \to 0} M_h = f(x),
  \] and this proves that \[
    L'(x) = U'(x) = f(x) \text{ for } x \text{ in } (a, b).
  \] This means that there is a number $c$ such that \[
    U(x) = L(x) + c \text{ for all } x \text{ in } [a, b].
  \] Since \[
    U(a) = L(a) = 0,
  \] the number $c$ must equal 0, so \[
    U(x) = L(x) \text{ for all } x \text{ in } [a, b].
  \] In particular, \[
    \textbf{U}\int_a^b f = U(b) = L(b) = \textbf{L}\int_a^b f,
  \] and this means that $f$ is integrable on $[a, b]$.
\end{proof}

\section{$\pi$ is Irrational}

\begin{definition}
  $\pi$ is the area of the unit circle, or more precisely, twice the area of a
  semicircle: \[
    \pi = 2 \cdot \int_{-1}^1 \sqrt{1 - x^2} \,dx.
  \]
\end{definition}

\begin{definition}
  For $-1 \leq x \leq 1$, the area $A(x)$ of the sector bounded by the unit
  circle, the horizontal axis, and the half-line through $(x, \sqrt{1 - x^2})$
  is \[
    A(x) = \frac{x\sqrt{1 - x^2}}{2} + \int_x^1 \sqrt{1 - t^2} \,dt.
  \]
\end{definition}

Notice that if $-1 < x < 1$, then $A$ is differentiable at $x$ and (using the
Fundamental Theorem of Calculus),
\begin{align*}
  A'(x)
  &= \frac{1}{2}\left[
    x \cdot \frac{-2x}{2\sqrt{1 - x^2}}
    + \sqrt{1 - x^2}
  \right] - \sqrt{1 - x^2} \\
  &= \frac{1}{2}\left[ \frac{-x^2 + (1 - x^2)}{\sqrt{1 - x^2}} \right] -
  \sqrt{1 - x^2} \\
  &= \frac{1 - 2x^2}{2\sqrt{1 - x^2}} - \sqrt{1 - x^2} \\
  &= \frac{1 - 2x^2 - 2(1 - x^2)}{2\sqrt{1 - x^2}} \\
  &= \frac{-1}{2\sqrt{1 - x^2}}.
\end{align*} Notice also that on the interval $[-1, 1]$ the function $A$
decreases from \[
  A(-1) = 0 + \int_{-1}^1 \sqrt{1 - t^2} \,dt = \frac{\pi}{2}
\] to $A(1) = 0$.

\begin{definition}
  For $0 \leq x \leq \pi$ we wish to define $\cos x$ and $\sin x$ as the
  coordinates of a point $P = (\cos x, \sin x)$ on the unit circle which
  determines a sector whose area is $x/2$.

  In other words, $\cos x$ is the unique number in $[-1, 1]$ such that
  \[
    A(\cos x) = \frac{x}{2};
  \] and \[
    \sin x = \sqrt{1 - \cos^2 x}.
  \]
\end{definition}

\begin{theorem}
  If $0 < x < \pi$, then
  \begin{align*}
    \cos'(x) &= -\sin x, \\
    \sin'(x) &= \cos x.
  \end{align*}
\end{theorem}

The values of $\sin x$ and $\cos x$ for $x$ not in $[0, \pi]$ are defined by a
two-step piecing together process:
\begin{enumerate}
  \item If $\pi \leq x \leq 2\pi$, then
    \begin{align*}
      \sin x &= -\sin(2\pi - x), \\
      \cos x &= \cos(2\pi - x).
    \end{align*}
  \item If $x = 2\pi k + x'$ for some integer $k$, and some $x'$ in $[0,
    2\pi]$, then
    \begin{align*}
      \sin x &= \sin x', \\
      \cos x &= \cos x'.
    \end{align*}
\end{enumerate}

The other standard trigonometric functions are defined by
\begin{align*}
  &\left.
    \begin{array}{c}
      \sec x = \frac{1}{\cos x} \\
      \tan x = \frac{\sin x}{\cos x} \\
    \end{array}
  \right\} x \neq k\pi + \pi/2, \\
  &\left.
    \begin{array}{c}
      \csc x = \frac{1}{\sin x} \\
      \cot x = \frac{\cos x}{\sin x}
    \end{array}
  \right\} x \neq k\pi.
\end{align*}

\begin{theorem}
  If $x \neq k\pi + \pi/2$, then
  \begin{align*}
    \sec'(x) &= \sec x \tan x, \\
    \tan'(x) &= \sec^2 x.
  \end{align*}
  If $x \neq k\pi$, then
  \begin{align*}
    \csc'(x) &= -\csc x \cot x, \\
    \cot'(x) &= -\csc^2 x. \\
  \end{align*}
\end{theorem}

The trigonometric functions are not one-one, so it is first necessary to
restrict them to suitable intervals in order to work with their inverses. The
intervals usually chosen are
\begin{align*}
  [-\pi/2, \pi/2] &\text{ for } \sin, \\
  [0, \pi]        &\text{ for } \cos, \\
  (-\pi/2, \pi/2) &\text{ for } \tan.
\end{align*}

\begin{theorem}
  If $-1 < x < 1$, then
  \begin{align*}
    \arcsin'(x) &= \frac{1}{\sqrt{1 - x^2}}, \\
    \arccos'(x) &= \frac{-1}{\sqrt{1 - x^2}}.
  \end{align*}
  Moreover, for all $x$ we have \[
    \arctan'(x) = \frac{1}{1 + x^2}.
  \]
\end{theorem}

To derive the addition formulas, we require a lemma.

\begin{lemma*}
  Suppose $f$ has a second derivative everywhere and that
  \begin{align*}
    f'' + f &= 0, \\
    f(0) &= 0, \\
    f'(0) &= 0.
  \end{align*}
  Then $f = 0$.
\end{lemma*}

\begin{theorem}
  If $x$ and $y$ are any two numbers, then \begin{align*}
    \sin(x + y) &= \sin x \cos y + \cos x \sin y, \\
    \cos(x + y) &= \cos x \cos y - \sin x \sin y.
  \end{align*}
\end{theorem}

\begin{proof}
  For any particular number $y$ we can define a function $f$ by \[
    f(x) = \sin(x + y).
  \] Then
  \begin{align*}
    f'(x) &= \cos(x + y) \\
    f''(x) &= -\sin(x + y).
  \end{align*}
  Consequently,
  \begin{align*}
    f'' + f &= 0, \\
    f(0)    &= \sin y, \\
    f'(0)   &= \cos y.
  \end{align*}
  It follows from Theorem 4 that \[
    f = (\cos y ) \cdot \sin{} + (\sin y) \cdot \cos{};
  \] that is, \[
    \sin(x + y) = \cos y \sin x + \sin y \cos x, \text{ for all } x.
  \] Since any number $y$ could have been chosen to begin with, this proves the
  first formula for all $x$ and $y$.

  The second formula is proven similarly.
\end{proof}

\section{Planetary Motion}

We describe the motion of a planet by the parametrized curve \[
  c(t) = r(t)(\cos \theta(t), \sin \theta(t)),
\] so that $r$ always gives the length of the line from the sun to
the planet, while $\theta$ gives the angle. It will be convenient to write this
as
\begin{equation} \label{eq:planet-disp}
  c(t) = r(t) \cdot \textbf{e}(\theta(t)),
\end{equation}
where \[
  \textbf{e}(t) = (\cos t, \sin t)
\] is just the parametrized curve that runs along the unit circle. Note that \[
  \textbf{e}'(t) = (-\sin t, \cos t)
\] is also a vector of unit length, but perpendicular to $\textbf{e}(t)$, and
that we also have
\begin{equation} \label{eq:e-det}
  \det(\textbf{e}(t), \textbf{e}'(t)) = 1.
\end{equation}
Differentiating \eqref{eq:planet-disp}, we obtain
\begin{equation} \label{eq:planet-vel}
  c'(t) = r'(t) \cdot \textbf{e}(\theta(t))
  + r(t)\theta'(t) \cdot \textbf{e}'(\theta(t))
\end{equation}
and combining with \eqref{eq:planet-disp}, together with the
formulas in Problem 6 of Appendix I to Chapter 4, we get
\begin{align*}
  \det(c(t), c'(t))
  &= r(t)r'(t)\det(\textbf{e}(\theta(t)), \textbf{e}(\theta(t)))
  + r(t)^2\theta'(t)\det(\textbf{e}(\theta(t)), \textbf{e}'(\theta(t))) \\
  &= r(t)^2\theta'(t)\det(\textbf{e}(\theta(t)), \textbf{e}'(\theta(t)))
\end{align*}
since $\det(v, v)$ is always 0. Using \eqref{eq:e-det} we then get
\begin{equation} \label{eq:planet-disp-det}
  \det(c, c') = r^2\theta'.
\end{equation}

Suppose that $A(t)$ is the area swept out from time 0 to $t$. The area of the
triangle $\Delta(h)$ with vertices $O$, $c(t)$, and $c(t + h)$, according to
Problems 4 and 5 of Appendix 1 to Chapter 4, is \[
  \area(\Delta(h)) = \frac{1}{2}\det(c(t), c(t + h) - c(t)).
\] Since the triangle $\Delta(h)$ has practically the same area as the region
$A(t + h) - A(t)$, this shows that
\begin{align*}
  A'(t) &= \lim_{h \to 0}\frac{A(t + h) - A(t)}{h} \\
        &= \lim_{h \to 0}\frac{\area(\Delta(h))}{h} \\
        &= \frac{1}{2}\det\left(
          c(t),
          \lim_{h \to 0}\frac{c(t + h) - c(t)}{h}
        \right) \\
        &= \frac{1}{2}\det(c(t), c'(t)).
\end{align*}

A rigorous derivation can be established with Problem 13-24, which gives a
formula for the area of a region determined by the graph of a function in polar
coordinates. We can write
\begin{equation} \label{eq:area-pol} \tag{*}
  A(t) = \frac{1}{2} \int_0^{\theta(t)} \rho(\phi)^2 \,d\phi
\end{equation}
if our parametrized curve $c(t) = r(t) \cdot \mathbf{e}(\theta(t))$ is the
graph of the function $\rho$ in polar coordinates. Now the function $\rho$ is
\[
  \rho = r \circ \theta^{-1},
\] so applying the First Fundamental Theorem of Caclulus and the Chain Rule on
\eqref{eq:area-pol} we immediately get
\begin{align*}
  A'(t) &= \frac{1}{2}\rho(\theta(t))^2 \cdot \theta'(t) \\
        &= \frac{1}{2} r(t)^2 \theta'(t).
\end{align*} Therefore, we have
\begin{equation} \label{eq:A'}
  \boxed{
    A' = \frac{1}{2}\det(c, c') = \frac{1}{2}r^2\theta'.
  }
\end{equation}

\begin{theorem}
  Kepler's second law is true if and only if the force is central, and in this
  case each planetary path $c(t) = r(t) \cdot \mathbf{e}(\theta(t))$ satisfies
  the equation
  \begin{equation} \label{eq:kep2} \tag{$K_2$}
    A' = \frac{1}{2}\det(c, c') = \text{constant},
  \end{equation} or that $A'' = 0$.
\end{theorem}
\begin{proof}
  First note that \[
    A'' = \frac{1}{2}[\det(c, c')]' = \frac{1}{2}[\det(c', c') + \det(c, c'')]
    = \frac{1}{2}\det(c, c''),
  \] so the statement is equivalent to $\det(c, c'') = 0$. A central force
  points along $c(t)$, but $c''(t)$ points in the direction of the force. This
  is equivalent to saying that $c''(t)$ always points along $c(t)$, so
  $\det(c, c'') = 0$.
\end{proof}

Newton next showed that if the gravitational force of the sun is a central
force and also satisfies an inverse square law, then the path of any object in
it will be a conic section having the sun at one focus.

\begin{theorem}
  If the gravitational force of the sun is a central force that satisfies an
  inverse square law, then the path of any body in it will be a conic section
  having the sun at one focus.
\end{theorem}
\begin{proof}
  Notice that our conclusion specifies the shape of the path, not a particular
  parametrization. But this parametrization is essentially determined by
  Theorem 1: the hypothesis of a central force implies that the area $A(t)$ is
  proportional to $t$, so determining $c(t)$ is essentially equivalent to
  determining $A$ for arbitrary points on the ellipse.

  By Theorem 1, the hypothesis of a central force implies that
  \begin{equation} \tag{$K_2$}
    A' = \frac{1}{2}r^2\theta' = \frac{1}{2}\det(c, c') = \frac{1}{2}M,
  \end{equation}
  for some constant $M$. The hypothesis of an inverse square law can be written
  \begin{equation} \label{eq:planet-acc-1} \tag{*}
    c''(t) = -\frac{H}{r(t)^2}\mathbf{e}(\theta(t)),
  \end{equation}
  for some constant $H$. Using \eqref{eq:kep2}, this can be written \[
    \frac{c''(t)}{\theta'(t)} = -\frac{H}{M}\mathbf{e}(\theta(t)).
  \] Notice that the left-hand side of this equation is \[
    [c' \circ \theta^{-1}]'(\theta(t)).
  \] So if we let \[
    D = c' \circ \theta^{-1},
  \] then this equation can be written as \[
    D'(\theta(t)) = -\frac{H}{M}\mathbf{e}(\theta(t))
    = -\frac{H}{M}(\cos \theta(t), \sin \theta(t)),
  \] and we can write this simply as \[
    D'(u) = -\frac{H}{M}(\cos u, \sin u)
    = \left(-\frac{H}{M} \cos u, -\frac{H}{M} \sin u\right),
  \] completely eliminating $\theta$. This offers a pair of equations for the
  components of $D$, which we find to be \[
    D(u) = \left(
      \frac{H \cdot \sin u}{-M} + A,
      \frac{H \cdot \cos u}{M} + B
    \right)
  \] for two constants $A$ and $B$. Letting $u = \theta(t)$ again we thus have
  an explicit formula for $c'$: \[
    c' = \left(
      \frac{H \cdot \sin \theta}{-M} + A,
      \frac{H \cdot \cos \theta}{M} + B
    \right)
  \] Substituting this with $c = r(\cos \theta, \sin \theta)$ into the equation
  \begin{equation} \tag{$K_2$}
    \det(c, c') = M,
  \end{equation} we get \[
    r\left[
      \frac{H}{M} \cos^2 \theta + B \cos \theta
      + \frac{H}{M} \sin^2 \theta - A \sin \theta
    \right] = M,
  \] which simplifies to \[
    r\left[
      \frac{H}{M^2} + \frac{B}{M} \cos \theta - \frac{A}{M} \sin \theta
    \right] = 1,
  \] Problem 15-8 shows that this can be written in the form \[
    r(t)\left[\frac{H}{M^2} + C\cos(\theta(t) + D)\right] = 1,
  \] for some constants $C$ and $D$. We can let $D = 0$, since this simply
  amounts to rotating our polar coordinate system (choosing which ray
  corresponds to $\theta = 0$), so we can write, finally, \[
    r[1 + \epsilon \cos \theta] = \frac{M^2}{H} = \Lambda.
  \] But this is the formula for a conic section derived in Appendix 3 of
  Chapter 4.
\end{proof}

In terms of the constant $M$ in the equation \[
  r^2\theta' = M
\] and the constant $\Lambda$ in the equation of the orbit \[
  r[1 + \epsilon \cos \theta] = \Lambda
\] the last equation in our proof shows that we can rewrite
\eqref{eq:planet-acc-1} as
\begin{equation} \label{eq:planet-acc-2} \tag{**}
  c''(t) = -\frac{M^2}{\Lambda} \cdot \frac{1}{r^2}\mathbf{e}(\theta(t)).
\end{equation}
Recall that the major axis $a$ of the ellipse is given by
\begin{equation} \label{eq:ell-a} \tag{a}
  a = \frac{\Lambda}{1 - \epsilon^2},
\end{equation}
while the minor axis $b$ is given by
\begin{equation} \label{eq:ell-b} \tag{b}
  b = \frac{\Lambda}{\sqrt{1 - \epsilon^2}}.
\end{equation}
Consequently,
\begin{equation} \label{eq:ell-c} \tag{c}
  \frac{b^2}{\Lambda} = a.
\end{equation}
Remember that \eqref{eq:A'} gives \[
  A'(t) = \frac{1}{2}r^2\theta' = \frac{1}{2}M.
\] and thus \[
  A(t) = \frac{1}{2}Mt.
\] We can therefore interpret $M$ in terms of the period $T$ of the orbit. This
period $T$ is, by definition, the value of $t$ for which we have $\theta(t) =
2\pi$, so that we obtain the complete ellipse. Hence \[
  \text{area of the ellipse} = A(T) = \frac{1}{2}MT
\] or \[
  M = \frac{2(\text{area of the ellipse})}{T} = \frac{2 \pi ab}{T} \quad
  \text{by Problem 13-17}.
\] Hence the constant $M^2/\Lambda$ in \eqref{eq:planet-acc-2} is
\begin{align*}
  \frac{M^2}{\Lambda}
  &= \frac{4 \pi^2 a^2 b^2}{T^2 \Lambda} \\
  &= \frac{4 \pi^2 a^3}{T^2}, \quad \text{using (c)}.
\end{align*} This completes the final step of Newton's analysis.

\begin{theorem}
  Kepler's third law is true if and only if the acceleration $c''(t)$ of any
  planet, moving on an ellipse, satisfies \[
    c''(t) = -G \cdot \frac{1}{r^2} \mathbf{e}(\theta(t))
  \] for a constant $G$ that does not depend on the planet.
\end{theorem}

It should be mentioned that the converse of Theorem 2 is also true. To prove
this, we first want to establish one further consequence of Kepler's second
law. Recall that for \[
  \mathbf{e}(t) = (\cos t, \sin t)
\] we have \[
  \mathbf{e}'(t) = (-\sin t, \cos t).
\] Consequently, \[
  \mathbf{e}''(t) = (-\cos t, -\sin t) = -\mathbf{e}(t).
\] Now differentiating \eqref{eq:planet-vel} gives
\begin{multline*}
  c''(t)
  = r''(t) \cdot \mathbf{e}(\theta(t))
  + r'(t)\theta'(t) \cdot \mathbf{e}'(\theta(t)) \\
  + r'(t)\theta'(t) \cdot \mathbf{e}'(\theta(t))
  + r(t)\theta''(t) \cdot \mathbf{e}'(\theta(t))
  + r(t)\theta'(t)\theta'(t) \cdot \mathbf{e}''(\theta(t)).
\end{multline*}
Using $\mathbf{e}''(t) = -\mathbf{e}(t)$ we get \[
  c''(t)
  = [r''(t) - r(t)\theta'(t)^2] \cdot \mathbf{e}(\theta(t))
  + [2r'(t)\theta'(t) + r(t)\theta''(t)] \cdot \mathbf{e}'(\theta(t)).
\] Since Kepler's second law implies central forces, hence that $c''(t)$ is
always a multiple of $c(t)$, and thus always a multiple of
$\mathbf{e}(\theta(t))$, the coefficient of $\mathbf{e}'(\theta(t))$ must be 0.
Thus Kepler's second law implies that
\begin{equation} \label{eq:planet-acc}
  c''(t) = [r''(t) - r(t)\theta'(t)^2] \cdot \mathbf{e}(\theta(t)).
\end{equation}

\begin{theorem}
  If the path of a planet moving under a central gravitational force is an
  ellipse with the sun as focus, then the force must satisfy an inverse square
  law.
\end{theorem}
\begin{proof}
  Once again, the hypothesis of a central force implies that
  \begin{equation} \label{eq:k3-1} \tag{$K_2$}
    r^2\theta' = M,
  \end{equation} for some constant $M$, and the hypothesis that the path is an
  ellipse with the sun as focus implies that it satisfies the equation
  \begin{equation} \label{eq:k3-2} \tag{A}
    r[1 + \epsilon \cos \theta] = \Lambda,
  \end{equation} for some $\epsilon$ and $\Lambda$. For our proof, we will keep
  differentiating and substituting from these two equations.

  First, we differentiate \eqref{eq:k3-2} to obtain \[
    r'[1 + \epsilon \cos \theta] - \epsilon r \theta' \sin \theta = 0.
  \] Multiplying by $r$ this becomes \[
    rr'[1 + \epsilon \cos \theta] - \epsilon r^2 \theta' \sin \theta = 0.
  \] Using both \eqref{eq:k3-1} and \eqref{eq:k3-2}, this becomes \[
    \Lambda r' - \epsilon M \sin \theta = 0.
  \] Differentiating again, we get \[
    \Lambda r'' - \epsilon M \theta' \cos \theta = 0.
  \] Using \eqref{eq:k3-1} we get \[
    \Lambda r'' - \frac{\epsilon M^2}{r^2} \cos \theta = 0,
  \] and then using \eqref{eq:k3-2} we get \[
    \Lambda r'' - \frac{M^2}{r^2}\left[\frac{\Lambda}{r} - 1\right] = 0.
  \] Substituting from \eqref{eq:k3-1} yet again, we get \[
    \Lambda[r'' - r(\theta')^2] + \frac{M^2}{r^2} = 0,
  \] or \[
    r'' - r(\theta')^2 = -\frac{M^2}{\Lambda r^2}.
  \] Comparing with \eqref{eq:planet-acc}, we obtain \[
    c''(t) = -\frac{M^2}{\Lambda r^2} \mathbf{e}(\theta(t)),
  \] which is precisely what we wanted to show: the force is inversely
  proportional to the square of the distance from the sun to the planet.
\end{proof}

\section{The Logarithm and Exponential Functions}

\begin{definition*}
  If $x > 0$, then \[ \pmb{\log x} = \int_1^x \frac{1}{t} \,dt. \]
\end{definition*}

\begin{theorem}
  If $x, y > 0$, then \[ \log(xy) = \log x + \log y. \]
\end{theorem}
\begin{proof}
  Notice first that $\log'(x) = 1/x$, by the Fundamental Theorem of Calculus.
  Now choose a number $y > 0$ and let \[ f(x) = \log(xy). \] Then \[ f'(x) =
  \log'(xy) \cdot y = \frac{1}{xy} \cdot y = \frac{1}{x}. \] Thus $f' = \log'$.
  This means that there is a number $c$ such that \[
    f(x) = \log x + c
    \text{ for all } x > 0,
  \] that is, \[
    \log(xy) = \log x + c
    \text{ for all } x > 0.
  \] The number $c$ can be evaluated by noting that when $x = 1$ we obtain \[
    \log(1 \cdot y) = \log 1 + c = c.
  \] Thus \[
    \log(xy) = \log x + \log y
    \text{ for all } x.
  \] Since this is true for all $y > 0$, the theorem is proved.
\end{proof}

\begin{corollary}
  If $n$ is a natural number and $x > 0$, then \[ \log(x^n) = n\log x. \]
\end{corollary}

\begin{corollary}
  If $x, y > 0$, then \[ \log\left(\frac{x}{y}\right) = \log x - \log y. \]
\end{corollary}

\begin{definition}
  The "exponential function", $\pmb{\exp}$, is defined as $\log^{-1}$.
\end{definition}

\begin{theorem}
  For all numbers $x$, $\exp'(x) = \exp(x)$.
\end{theorem}

\begin{theorem}
  If $x$ and $y$ are any two numbers, then \[ \exp(x + y) = \exp(x) \cdot
  \exp(y). \]
\end{theorem}

\begin{definition}
  $\pmb{e} = \exp(1)$, or \[ 1 = \log e = \int_1^e \frac{1}{t} \,dt. \] For any
  number $x$, \[ \pmb{e^x} = \exp(x). \]
\end{definition}

The $\exp$ function allows us to define $e^x$ for an arbitrary (even
irrational) exponent $x$, and by extension $a^x$ for $a > 0$.

\begin{definition}
  If $a > 0$, then, for any real number $x$, \[ \pmb{a^x} = e^{x\log a}. \]
\end{definition}

\begin{theorem}
  If $a > 0$, then
  \begin{enumerate}
    \item $(a^b)^c = a^{bc}$ for all $b, c$.

      (Notice that $a^b$ will automatically be positive, so $(a^b)^c$ will be
      defined);
    \item $a^1 = a$ and $a^{x + y} = a^x \cdot a^y$ for all $x, y$.

      (Notice that (2) implies that this definition of $a^x$ agrees with the
      old one for all rational $x$.)
  \end{enumerate}
\end{theorem}

\begin{theorem}
  If $f$ is differentiable and \[ f'(x) = f(x) \text{ for all } x, \] then
  there is a number $c$ such that \[ f(x) = ce^x \text{ for all } x. \]
\end{theorem}
\begin{proof}
  Let \[ g(x) = \frac{f(x)}{e^x}. \] (This is permissible, since $e^x \neq 0$
  for all $x$.) Then \[
    g'(x) = \frac{e^xf'(x) - f(x)e^x}{(e^x)^2} = 0.
  \] Therefore there is a number $c$ such that \[
    g(x) = \frac{f(x)}{e^x} = c \text{ for all } x.
  \]
\end{proof}

Contrary to the basic property stated in Theorem 2, there are infinitely many
other functions which satisfy the property $f(x + y) = f(x) + f(y)$ stated in
Theorem 3. However, any \emph{continuous} function $f$ satisfying this property
must be of the form $f(x) = a^x$ or $f(x) = 0$.

In addition to the two basic properties stated in Theorems 2 and 3, the
function $\exp$ "grows faster than any other polynomial". In other words,

\begin{theorem}
  For any natural number $n$, \[ \lim_{x \to \infty} \frac{e^x}{x^n} = \infty.
  \]
\end{theorem}
\begin{proof}
  The proof consists of several steps.

  \paragraph{Step 1.} $e^x > x$ for all $x$, and consequently
  $\lim_{x \to \infty} e^x = \infty$ (this may be considered to be the case $n
  = 0$).

  To prove this statement (which is clear for $x \leq 0$) it suffices to show
  that \[ x > \log x \text{ for all } x > 0. \] If $x < 1$ this is clearly
  true, since $\log x < 0$. If $x > 1$, then $x - 1$ is an upper sum for $f(t)
  = 1/t$ on $[1, x]$, so $\log x < x - 1 < x$.

  \paragraph{Step 2.} $\lim_{x \to \infty} \frac{e^x}{x} = \infty$.

  To prove this, note that \[
    \frac{e^x}{x}
    = \frac{e^{x/2} \cdot e^{x/2}}{\frac{x}{2} \cdot 2}
    = \frac{1}{2}\left(\frac{e^{x/2}}{\frac{x}{2}}\right) \cdot e^{x/2}
  \] By Step 1, the expression in parentheses is greater than 1, and
  $\lim_{x \to \infty} e^{x/2} = \infty$; this shows that $\lim_{x \to \infty}
  d^x/x = \infty$.

  \paragraph{Step 3.} $\lim_{x \to \infty} \frac{e^x}{x^n} = \infty$.

  Note that \[
    \frac{e^x}{x^n}
    = \frac{(e^{x/n})^n}{\left(\frac{x}{n}\right)^n \cdot n^n}
    = \frac{1}{n^n} \cdot \left(\frac{e^{x/n}}{\frac{x}{n}}\right)^n.
  \] The expression in parentheses becomes arbitrarily large, by Step 2, so the
  $n$th power certainly becomes arbitrarily large.
\end{proof}



\end{document}

