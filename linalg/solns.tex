\documentclass{article}

\usepackage{amsmath,amssymb,amsthm}
\usepackage[shortlabels]{enumitem}
\usepackage{systeme}

\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\nullity}{nullity}

\title{Solutions to ``Linear Algebra'', 2nd edition (Hoffman, Kunze)}
\author{Ng Wei En}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Linear Equations}

\setcounter{subsection}{1}
\subsection{Systems of Linear Equations}

\paragraph{6.} Prove that if two homogeneous systems of linear equations in two
unknowns have the same solutions, then they are equivalent.

\begin{proof}
  We express these two systems of linear equations as:
  \begin{align*}
    a_{11}x + a_{12}y &= 0 & b_{11}x + b_{12}y &= 0 \\
    a_{21}x + a_{22}y &= 0 & b_{21}x + b_{22}y &= 0 \\
    &\vdots & &\vdots \\
    a_{m1}x + a_{m2}y &= 0 & b_{m1}x + b_{m2}y &= 0 \\
  \end{align*}
  Each system consists of a set of lines through $(0, 0)$ in the $x$-$y$ plane.
  Thus the two systems have the same solution iff they both have either $(0, 0)$
  as their only solution (Case 1) or single line $ux + vy = 0$ as their common
  solution (Case 2).

  In Case 1, all equations are simply multiples of the same line, so clearly the
  two systems are equivalent.

  In Case 2, assume WLOG that the first two equations in the first system are
  different lines. Then \[
    \frac{a_{11}}{a_{12}} \neq \frac{a_{21}}{a_{22}}.
  \] We need to show that there exists some $(u, v)$ that solves
  \begin{align*}
    a_{11}u + a_{12}v &= b_{i1} \\
    a_{21}u + a_{22}v &= b_{i2}
  \end{align*}
  for some $i$. Solving for $u$ and $v$, \[
    u = \frac{a_{22}b_{i1} - a_{12}b_{i2}}{a_{11}a_{22} - a_{12}a_{21}}, \\
    v = \frac{a_{21}b_{i1} - a_{11}b_{i2}}{a_{12}a_{21} - a_{11}a_{22}}.
  \] Since $a_{11}a_{22} - a_{12}a_{21} \neq 0$, both $u$ and $v$ are
  well-defined, thus the two systems are equivalent.
\end{proof}

\paragraph{7.} Prove that each subfield of the field of complex numbers contains
every rational number.

\begin{proof}
  For a subfield $F$ of $C$, $1 \in F$ and $n \cdot 1 = 0 \in F \subseteq C
  \iff n = 0$ as $C$ has characteristic zero, so $n = 1, 2, 3, \ldots$ are all
  distinct elements of $F$. Each of these elements have additive inverses $-1,
  -2, -3, \ldots$ which are also distinct elements of $F$, and therefore $Z
  \subseteq F$.

  For all $n \in Z$, $n \neq 0$, their multiplicative inverses $\frac{1}{n}$ are
  also distinct elements of $F$. Consider a rational number $\frac{m}{n} \in Q$.
  Then, $m, n \in Z \implies m, \frac{1}{n} \in F$ and so $m \cdot \frac{1}{n} =
  \frac{m}{n} \in F$. Hence, we can conclude that $Q \subseteq F$.
\end{proof}

\paragraph{8.} Prove that each field of characteristic zero contains a copy of
the rational number field.

\begin{proof}
  For a field $F$ of characteristic zero, let its additive and multiplicative
  identity be $0_F$ and $1_F$ respectively, and define \[
    n_F = \underbrace{1_F + 1_F + \cdots + 1_F}_{n\text{ terms}}.
  \] Define the additive and multiplicative inverse of $n_F$ as $-n_F$ and
  $n_F^{-1}$ respectively. As $F$ has characteristic zero, if $n \neq m$ then
  $n_F \neq m_F$. For $m, n \in Z$, let $\left(\frac{m}{n}\right)_F = m_F \cdot
  n_F^{-1}$; if $\frac{m}{n} \neq \frac{m'}{n'}$ then
  $\left(\frac{m}{n}\right)_F \neq \left(\frac{m'}{n'}\right)_F$. So the map
  $h: \frac{m}{n} \to \left(\frac{m}{n}\right)_F$ is a bijection from $Q$ to a
  subset of $F$, with $h(0) = 0_F$, $h(1) = 1_F$, $h(x + y) = h(x) + h(y)$ and
  $h(xy) = h(x)h(y)$.
\end{proof}

\setcounter{subsection}{5}
\subsection{Invertible Matrices}

\paragraph{9.} An $n \times n$ matrix $A$ is called \textbf{upper-triangular} if
$A_{ij} = 0$ for $i > j$, that is, if every entry below the main diagonal is 0.
Prove that an upper-triangular (square) matrix is invertible if and only if
every entry on its main diagonal is different from 0.

\begin{proof}
  Suppose $A_{ii} \neq 0$ for all $i$. Construct the matrix $A'$ by dividing
  each row $i$ of $A$ by $A_{ii}$ such that $A'_{ii} = 1$ for all $i$. Then,
  $I_n$ can be easily obtained by applying elementary row operations on $A'$ to
  eliminate all non-zero entries above the main diagonal. Hence, $A$ is
  row-equivalent to $I_n$ and invertible by \textbf{Theorem 12}.

  Now, suppose $A_{ii} = 0$ for some $i$. If $A_{ii} = 0$ for all $i$, then the
  last row of $A$ is all zeroes and $A$ cannot be row-equivalent to $I_n$. Now,
  taking $i'$ as the largest index such that $A_{i'i'} = 0$ and $A_{ii} \neq 0$
  for all $i > i'$, construct the matrix $A'$ by dividing all rows $i$ below row
  $i'$ of $A$ by $A_{ii}$ such that $A'_{ii} = 1$ for all $i > i'$. Then add
  multiples of these rows below row $i'$ such that row $i'$ is all zeroes. Then
  $A$ cannot be row-equivalent to $I_n$.
\end{proof}

\section{Vector Spaces}

\setcounter{subsection}{1}
\subsection{Subspaces}

\paragraph{6.}
\begin{enumerate}[(a)]
  \item Prove that the only subspaces of $R^1$ are $R^1$ and the zero subspace.
    \begin{proof}
      Suppose $V = \{0\}$, the zero subspace; clearly, $V$ is a subspace of
      $R^1$. Now suppose that for a subspace $V$ of $R^1$ there exists $v \in
      V \subseteq R^1$ with $v \neq 0$. Consider some $\alpha \in R$; then
      $\alpha = \frac{\alpha}{v} \cdot v \in V$ where $\frac{\alpha}{v}$ is a
      scalar in $R$, and so $V = R^1$.
    \end{proof}
  \item Prove that a subspace of $R^2$ is $R^2$, or the zero subspace, or
    consists of all scalar multiples of some fixed vector in $R^2$.
    \begin{proof}
      Suppose that for a subspace $V$ of $R^2$ there exists $v_1 \in V$ with
      $v_1 \neq 0$. Then for all $\alpha \in R$, $\alpha v_1 \in V$. So if $V
      \neq \{0\}$ then $\{\alpha v_1 \mid \alpha \in R\} \subseteq V$.

      If for all $v \in V$ we have $v = \alpha v_1$ for some $\alpha \in R$,
      then it is clear that $V = \{\alpha v_1 \mid \alpha \in R\}$ and consists
      of all scalar multiples of $v_1 \in R^2$.

      Now suppose that there exists a $v_2 \in V, v_2 \neq 0$ such that $v_2
      \neq \alpha v_1$ for all $\alpha \in R$. Defining $v_1 = \begin{bmatrix}
        a \\
        b
      \end{bmatrix}$ and $v_2 = \begin{bmatrix}
        c \\
        d
      \end{bmatrix}$, we may construct a matrix $A = \begin{bmatrix}
        v_1 & v_2
      \end{bmatrix}
      = \begin{bmatrix}
        a & c \\
        b & d
      \end{bmatrix}$. Suppose $a \neq 0$ and let $r = c/a$; then, $ra = c$ and
      it follows that $rb \neq d$ since $v_2$ is not a scalar multiple of $v_1$.
      A similar argument holds for $a = 0$ and $b \neq 0$. Thus, $ad - bc \neq
      0$ and it follows from \textbf{Exercise 1.6.8} that $A$ is invertible.

      From \textbf{Theorem 1.13}, any vector $v \in R^2$ can be rewritten
      into $v = A\begin{bmatrix}
        e \\
        f
      \end{bmatrix} = ev_1 + fv_2$ for $e, f \in R$. Hence, $V = R^2$ if there
      exist two non-zero vectors $v_1, v_2 \in R^2$ which are not scalar
      multiples of each other.
    \end{proof}
  \item Can you describe the subspaces of $R^3$?

    \begin{proof}
      The subspaces of $R^3$ are either the zero subspace, all scalar multiples
      of some fixed vector in $R^3$, all linear combinations of two non-parallel
      vectors in $R^3$, or $R^3$.
    \end{proof}
\end{enumerate}

\paragraph{7.} Let $W_1$ and $W_2$ be subspaces of a vector space $V$ such that
the set-theoretical union of $W_1$ and $W_2$ is also a subspace. Prove that one
of the spaces $W_i$ is contained in the other.

\begin{proof}
  Suppose $W_1 \not\subseteq W_2$ and $W_2 \not\subseteq W_1$. Consider $v_1 \in
  W_1 \setminus W_2$ and $v_2 \in W_2 \setminus W_1$; since $v_1 + v_2 \in W_1
  \cup W_2$, there must exist some $w'_1 \in W_1$ such that $w'_1 = v_1 + v_2$
  or some $w'_2 \in W_2$ such that $w'_2 = v_1 + v_2 \in W_2$.

  If the former is true, then with $w'_1 \in W_1$ and $v_1 \in W_1 \setminus W_2
  \subset W_1$, we must have $w'_1 - v_1 = v_2 \in W_1$. This contradicts the
  assumption that $v_2 \not\in W_1$. Likewise, the latter contradicts the
  assumption that $v_1 \not\in W_2$. Hence, it is proven that either $W_1
  \subseteq W_2$ or $W_2 \subseteq W_1$.
\end{proof}

\paragraph{8.} Let $V$ be the vector space of all functions from $R$ into $R$;
let $V_e$ be the subset of even functions, $f(-x) = f(x)$; let $V_o$ be the
subset of odd functions, $f(-x) = -f(x)$.

\begin{enumerate}[(a)]
  \item Prove that $V_e$ and $V_o$ are subspaces of $V$.

    \begin{proof}
      By \textbf{Theorem 2.1}, $V_e$ is a subspace of $V$ if and only if for
      each pair of vectors $\alpha, \beta \in V_e$ and each scalar $c \in R$
      the vector $c\alpha + \beta$ is again in $V_e$. Let $\gamma = c\alpha + 
      \beta$. Then \[
        \gamma(-x) = c\alpha(-x) + \beta(-x) = c\alpha(x) + \beta(x) =
        \gamma(x),
      \] so $\gamma$ is even and in $V_e$. Hence, $V_e$ is a subspace of $V$.

      Similarly, for each pair of vectors $\alpha, \beta \in V_o$ and each
      scalar $c \in R$ the vector $\gamma = c\alpha + \beta$ has the property
      that \[
        \gamma(-x) = c\alpha(-x) + \beta(-x) = c[-\alpha(x)] + [-\beta(x)] =
        -[c\alpha(x) + \beta(x)] = -\gamma(x),
      \] so it is odd and in $V_o$. Hence, $V_o$ is also a subspace of $V$.
    \end{proof}
  \item Prove that $V_e + V_o = V$.

    \begin{proof}
      Let $f \in V$, and define $f_e(x) = [f(x) + f(-x)]/2$ and $f_o(x) =
      [f(x) - f(-x)]/2$ so that $f = f_e + f_o$. It is clear that \[
        f_e(-x) = \frac{f(-x) + f(x)}{2} = \frac{f(x) + f(-x)}{2} = f_e(x)
      \] and \[
        f_o(-x) = \frac{f(-x) - f(x)}{2} = -\frac{f(x) - f(-x)}{2} = -f_o(x),
      \] so $f_e \in V_e$ and $f_o \in V_o$. Therefore any function $f = f_e +
      f_o \in V$ for some $f_e \in V_e$ and $f_o \in V_o$, proving that $V_e +
      V_o = V$.
    \end{proof}
  \item Prove that $V_e \cap V_o = \{0\}$.

    \begin{proof}
      For a function $f \in V$ to be in both $V_e$ and $V_o$, it must be even
      and odd (i.e. $f(x) = f(-x)$ and $f(x) = -f(-x)$ respectively). Clearly,
      only the zero function fulfils both properties.
    \end{proof}
\end{enumerate}

\paragraph{9.} Let $W_1$ and $W_2$ be subspaces of a vector space $V$ such that
$W_1 + W_2 = V$ and $W_1 \cap W_2 = \{0\}$. Prove that for each vector $\alpha$
in $V$ there are unique vectors $\alpha_1$ in $W_1$ and $\alpha_2$ in $W_2$ such
that $\alpha = \alpha_1 + \alpha_2$.

\begin{proof}
  By definition of $V = W_1 + W_2$, for any vector $\alpha \in V$ there must
  exist a pair of vectors $\alpha_1 \in W_1$ and $\alpha_2 \in W_2$ such that
  $\alpha = \alpha_1 + \alpha_2$.

  Now suppose $\alpha = \alpha_1 + \alpha_2$ for $\alpha_i \in W_i$ and $\alpha
  = \beta_1 + \beta_2$ for $\beta_i \in W_i$. Then $\alpha_1 + \alpha_2 =
  \beta_1 + \beta_2$ which implies that $\alpha_1 - \beta_1 = \beta_2 -
  \alpha_2$. Suppose $\gamma = \alpha_1 - \beta_1 = \beta_2 - \alpha_2$; it then
  follows that $\gamma = \alpha_1 - \beta_1 \in W_1$ and $\gamma = \alpha_2 -
  \beta_2 \in W_2$. From $W_1 \cap W_2 = \{0\}$ it is clear that $\gamma = 0$,
  thus $\alpha_1 - \beta_1 = 0 \iff \alpha_1 = \beta_1$. Similarly, it can be
  shown that $\alpha_2 = \beta_2$. This proves that the vectors $\alpha_1$ and
  $\alpha_2$ are unique.
\end{proof}

\subsection{Bases and Dimensions}

\paragraph{9.} Let $V$ be a vector space over a subfield $F$ of the complex
numbers. Suppose $\alpha$, $\beta$ and $\gamma$ are linearly independent vectors
in $V$. Prove that $(\alpha + \beta), (\beta + \gamma)$, and $(\gamma + \alpha)$
are linearly independent.

\begin{proof}
  Suppose $c_1(\alpha + \beta) + c_2(\beta + \gamma) + c_3(\gamma + \alpha) =
  0$. Rearranging this equation yields $(c_1 + c_3)\alpha + (c_1 + c_2) \beta +
  (c_2 + c_3)\gamma$. Since $\alpha, \beta, \gamma$ are linearly independent, it
  follows that $c_1 + c_3 = c_1 + c_2 = c_2 + c_3 = 0$. This gives a system of
  equations in $c_1, c_2, c_3$ with matrix \[
    \begin{bmatrix}
      1 & 0 & 1 \\
      1 & 1 & 0 \\
      0 & 1 & 1
    \end{bmatrix}
  \] which can be easily shown to be row-reducible to the identity matrix. By
  \textbf{Theorem 1.7}, the only solution is $c_1 = c_2 = c_3 = 0$. Thus
  $(\alpha + \beta), (\beta + \gamma), (\gamma + \alpha)$ are linearly
  independent.
\end{proof}

\paragraph{14.} Let $V$ be the set of real numbers. Regard $V$ as a vector space
over the field of rational numbers, with the usual operations. Prove that this
vector space is not finite-dimensional.

\begin{proof}
  Suppose $\{r_1, r_2, \ldots, r_n\}$ is a basis for $V$ over the field
  $Q$, for some $n \in Z^+$. Then any element of $V$ can be written as \[
    c_1r_1 + c_2r_2 + \ldots + c_nr_n,
  \] so any element of $V$ can be mapped to a $n$-tuple of rational numbers in
  $Q^n$. This suggests that $\vert V \vert = \vert Q^n \vert$. However, $V = R$
  is uncountable, while $Q$ and by extension $Q^n$ is countable, leading to a
  contradiction. Therefore no finite basis exists for the vector space $V$.
\end{proof}

\subsection{Coordinates}

\paragraph{5.} Let $\alpha = (x_1, x_2)$ and $\beta = (y_1, y_2)$ be vectors in
$R^2$ such that \[
  x_1y_1 + x_2y_2 = 0,\quad
  x_1^2 + x_2^2 = y_1^2 + y_2^2 = 1.
\] Prove that $B = \{\alpha, \beta\}$ is a basis for $R^2$. Find the coordinates
of the vector $(a, b)$ in the ordered basis $B = \{\alpha, \beta\}$. (The
conditions on $\alpha$ and $\beta$ say, geometrically, that $\alpha$ and $\beta$
are perpendicular and each has length 1.)

\begin{proof}
  By \textbf{Theorem 5}, any linearly independent subset of $R^2$ is finite and
  is part of a basis for $R^2$. It is sufficient to prove that $\alpha$ and
  $\beta$ are linearly independent, such that $B$ spans a subspace of $R^2$ with
  dimension two (which is $R^2$ itself.)

  Suppose that $\beta = c\alpha$ for some $c \in R$, such that $y_1 = cx_1$,
  $y_2 = cx_2$. Then \[
    0 = x_1y_1 + x_2y_2 = x_1(cx_1) + x_2(cx_2) = c(x_1^2 + x_2^2) = c \cdot 1 =
    c
  \] which implies that $c = 0$, contradicting $y_1^2 + y_2^2 = 1$. Thus,
  $\alpha$ and $\beta$ are linearly independent and $B = \{\alpha, \beta\}$
  forms a basis for $R^2$.

  Let $P = \begin{bmatrix}
    \alpha & \beta
  \end{bmatrix} = \begin{bmatrix}
    x_1 & y_1 \\
    x_2 & y_2
  \end{bmatrix}$. By \textbf{Corollary 3 to Theorem 5}, $P$ is invertible and so
  \[
    P^{-1} = \frac{1}{x_1y_2 - x_2y_1}\begin{bmatrix}
      y_2 & -y_1 \\
      -x_2 & x_1
    \end{bmatrix}.
  \] Therefore \[
    \begin{bmatrix}
      a \\
      b
    \end{bmatrix}_B = P^{-1}\begin{bmatrix}
      a \\
      b
    \end{bmatrix} = \frac{1}{x_1y_2 - x_2y_1}\begin{bmatrix}
      ay_2 - by_1 \\
      -ax_2 + bx_1
    \end{bmatrix}.
  \]
\end{proof}

\paragraph{6.} Let $V$ be the vector space over the complex numbers of all
functions from $R$ into $C$, i.e., the space of all complex-valued functions on
the real line. Let $f_1(x) = 1$, $f_2(x) = e^{ix}$, $f_3(x) = e^{-ix}$.

\begin{enumerate}[(a)]
  \item Prove that $f_1$, $f_2$ and $f_3$ are linearly independent.
    \begin{proof}
      Consider the equation \[
        c_1f_1(x) + c_2f_2(x) + c_3f_3(x) = 0 \iff
        c_1 + c_2e^{ix} + c_3e^{-ix} = 0
      \] for some $c_1, c_2, c_3 \in R$. Let $y = e^{ix} \in C$. Then the
      equation can be rewritten into \[
        c_1 + c_2y + \frac{c_3}{y} = 0,
      \] or the quadratic equation \[
        c_2y^2 + c_1y + c_3 = 0.
      \] Assume not all $c_1, c_2, c_3$ are zero. Then $y$ takes on at most two
      distinct values, by the fundamental theorem of algebra. But $e^{ix}$ takes
      on infinitely many distinct values as $x$ varies in $R$, so there is a
      contradiction. Hence, the only solution to the equation is $c_1 = c_2 =
      c_3 = 0$ and $f_1$, $f_2$ and $f_3$ are linearly independent.
    \end{proof}

  \item Let $g_1(x) = 1$, $g_2(x) = \cos x$, $g_3(x) = \sin x$. Find an
    invertible $3 \times 3$ matrix $P$ such that \[
      g_i = \sum_{i=1}^3 P_{ij}f_i.
    \]
    \begin{proof}
      Note that $f_2(x) = e^{ix} = \cos x + i\sin x$ and $f_3(x) = e^{-ix} =
      \cos x - i\sin x$. Hence, we may express \[
        g_1 = f_1,\quad
        g_2 = \frac{1}{2}(f_2 + f_3),\quad
        g_3 = \frac{-i}{2}(f_2 - f_3),
      \] and so \[
        P = \begin{bmatrix}
          1 & 0 & 0 \\
          0 & 1/2 & i/2 \\
          0 & 1/2 & -i/2
        \end{bmatrix}.
      \]
    \end{proof}
\end{enumerate}

\paragraph{7.} Let $V$ be the (real) vector space of all polynomial functions
from $R$ into $R$ of degree 2 or less, i.e., the space of all functions $f$ of
the form \[
  f(x) = c_0 + c_1x + c_2x^2.
\] Let $t$ be a fixed real number and define \[
  g_1(x) = 1,\quad
  g_2(x) = x + t,\quad
  g_3(x) = (x + t)^2.
\] Prove that $B = \{g_1, g_2, g_3\}$ is a basis for $V$. If \[
  f(x) = c_0 + c_1x + c_2x^2
\] what are the coordinates of $f$ in this ordered basis $B$?

\begin{proof}
  From \textbf{Example 16} it is clear that $V$ has dimension 3. Thus by
  \textbf{Corollary 2(b) to Theorem 4}, as $B$ has 3 vectors, it is sufficient
  to prove that $B$ is linearly independent for $B$ to span $V$.

  Consider the equation \[
    b_1g_1(x) + b_2g_2(x) + b_3g_3(x) = 0
  \] for some $b_1, b_2, b_3 \in R$. Suppose not all $b_1, b_2, b_3$ are zero.
  Then the left hand side can be rewritten as \[
    b_1 + b_2(x + t) + b_3(x + t)^2 = (b_1 + b_2t + b_3t^2) + (b_2 + 2b_3t)x +
    b_3x^2,
  \] and it follows that $x$ takes on at most two distinct values by the
  fundamental theorem of algebra. But this contradicts the fact that $x$ can
  take on any real value. Therefore we have $b_1 = b_2 = b_3 = 0$ as the only
  solution to the equation, and $B = \{g_1, g_2, g_3\}$ is linearly independent.

  Suppose $f(x) = b_1g_1(x) + b_2g_2(x) + b_3g_3(x)$ for some $b_1, b_2, b_3 \in
  R$; it follows that
  \begin{align*}
    b_3 &= c_2, \\
    b_2 &= c_1 - 2b_3t = c_1 - 2c_2t, \\
    b_1 &= c_0 - b_2t - b_3t^2 = c_0 - c_1t + c_2t^2.
  \end{align*}
  Therefore $f(x)$ can be written as the coordinate vector \[
    (c_0 - c_1t + c_2t^2, c_1 - 2c_2t, c_2).
  \]
\end{proof}

\section{Linear Transformations}

\subsection{Linear Transformations}

\paragraph{12.} Let $V$ be an $n$-dimensional vector space over the field $F$
and let $T$ be a linear transformation from $V$ into $V$ such that the range and
null space of $T$ are identical. Prove that $n$ is even. (Can you give an
example of such a linear transformation $T$?)

\begin{proof}
  From \textbf{Theorem 2}, $\rank(T) + \nullity(T) = \dim V = n$. Since the
  range and null space of $T$ is identical, it follows that $\rank(T) =
  \nullity(T)$. Let $m = \rank(T) = \nullity(T)$. Then $n = m + m = 2m$ which is
  even.

  $T$ satisfies $T^2(\alpha) = 0$ for all $\alpha \in V$. An example of such a
  $T$ for $V
  = \mathbb{R}^2$ is \[
    T(\alpha) = \begin{bmatrix}
      0 & 0 \\
      1 & 0
    \end{bmatrix}\alpha,
  \] with \[
    T^2(\alpha) = \begin{bmatrix}
      0 & 0 \\
      0 & 0
    \end{bmatrix}\alpha = 0.
  \]
\end{proof}

\paragraph{13.} Let $V$ be a vector space and $T$ a linear transformation from
$V$ into $V$. Prove that the following two statements about $T$ are equivalent.
\begin{enumerate}[(a)]
  \item The intersection of the range of $T$ and the null space of $T$ is the
    zero subspace of $V$.
  \item If $T(T\alpha) = 0$, then $T\alpha = 0$.
\end{enumerate}

\begin{proof}
  Suppose (a) is true and $T(T\alpha) = 0$ for some $\alpha \in V$, i.e.
  $T\alpha$ is in the nullspace of $T$. Since $T\alpha$ is also in the range of
  $T$, $T\alpha$ must be in the intersection of the range and null space of $T$.
  So $T\alpha = 0$ by (a) and (b) is true.

  Now suppose that (b) is true. Consider some vector $\beta$ in both the range
  and null space of $T$. Since $\beta$ is in the range of $T$, $\beta = T\alpha$
  for some $\alpha \in V$. But $\beta$ is in the null space of $T$, so $T(\beta)
  = T(T\alpha) = 0$. By (b) it follows that $T\alpha = \beta = 0$, so (a) is
  true.
\end{proof}

\subsection{The Algebra of Linear Transformations}

\paragraph{6.} Let $T$ be a linear transformation from $R^3$ into $R^2$, and let
$U$ be a linear transformation from $R^2$ into $R^3$. Prove that the
transformation $UT$ is not invertible. Generalize the theorem.

\begin{proof}
  Let $\{\alpha_1, \alpha_2, \alpha_3\}$ be a basis for $R^3$. Then
  $T(\alpha_1), T(\alpha_2), T(\alpha_3)$ must be linearly dependent by
  \textbf{Theorem 2.4}, since $\dim R^2 = 2 < 3$.

  Suppose $b_1T(\alpha_1) + b_2T(\alpha_2) + b_3T(\alpha_3) = 0$ and not all
  $b_1, b_2, b_3$ are zero. Then $b_1\alpha_1 + b_2\alpha_2 + b_3\alpha_3 \neq
  0$ by the linear independence of $\alpha_1, \alpha_2, \alpha_3$, and \[
    UT(b_1\alpha_1 + b_2\alpha_2 + b_3\alpha_3)
    = U(b_1T(\alpha_1) + b_2T(\alpha_2) + b_3T(\alpha_3))
    = U(0)
    = 0.
  \] Hence, $UT$ is not non-singular, and it follows from \textbf{Theorem 9}
  that $UT$ is not invertible. This generalises to any linear transformations
  $T$ from $R^m$ into $R^n$ and $U$ from $R^n$ into $R^m$, where $m < n$.
\end{proof}

\paragraph{9.} Let $T$ be a linear operator on the finite-dimensional space $V$.
Suppose there is a linear operator $U$ on $V$ such that $TU = I$. Prove that $T$
is invertible and $U = T^{-1}$. Give an example which shows that this is false
when $V$ is not finite dimensional. (Hint: Let $T = D$, the differentiation
operator on the space of polynomial functions.)

\begin{proof}
  Since $TU$ is onto, it follows that $T$ is onto. By \textbf{Theorem 9}, $T$ is
  invertible with inverse $T^{-1} = U$.

  For $V$ not finite dimensional, consider $T = D$, the differentiation operator
  on the space of polynomial functions, and define $U$ by $(Uf)(x) = \int_0^x
  f(t) \,dt$ for all polynomials $f(x)$. Then $UT = I$, but $TU(x + 1) = T(1) =
  x \neq x + 1$ so $TU \neq I$. Hence $T$ is not invertible.
\end{proof}

\paragraph{11.} Let $V$ be a finite-dimensional vector space and let $T$ be a
linear operator on $V$. Suppose that $\rank(T^2) = \rank(T)$. Prove that the
range and null space of $T$ are disjoint, i.e., have only the zero vector in
common.

\begin{proof}
  Let $\{\alpha_1, \alpha_2, \ldots, \alpha_n\}$ be a basis for $V$. Suppose
  $k = \rank(T)$ and suppose WLOG that for $S = \{\alpha_1, \alpha_2, \ldots,
  \alpha_k\}$, $T(S)$ is linearly independent. Then $S$ is a basis for the range
  of $T$ and it follows that $T^2(S)$ spans the range of $T^2$. Since the
  dimension of the range of $T^2$ is also equal to $k$, by \textbf{Theorem 2.4},
  $T^2(S)$ is a basis for the range of $T^2$.

  Now suppose $v$ is a vector in both the range and null space of $T$ such that
  $v = c_1T\alpha_1 + \cdots + c_kT\alpha_k$ and $0 = Tv = T(c_1T\alpha_1 +
  \cdots + c_kT\alpha_k) = c_1T^2\alpha_1 + \cdots + c_kT^2\alpha_k$. But
  $T^2(S)$ is linearly independent, so $c_1 = \ldots = c_k = 0$ which implies
  $v = 0$. Therefore we have shown that the range and null space of $T$ have
  only the zero vector in common.
\end{proof}

\end{document}
