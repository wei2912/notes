\documentclass{article}

\usepackage{amsmath,amssymb,amsthm}
\usepackage[shortlabels]{enumitem}

\newtheorem{theorem}{Theorem}
\newtheorem*{corollary*}{Corollary}
\newtheorem*{lemma*}{Lemma}

\title{Notes to ``Linear Algebra'', 2nd edition (Hoffman, Kunze)}
\author{Ng Wei En}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Linear Equations}

\setcounter{subsection}{1}
\subsection{Systems of Linear Equations}

\paragraph{System of linear equations.} Suppose $F$ is a field. We consider the
problem of finding $n$ scalars $x_1, \ldots, x_n \in F$ which satisfy the
conditions
\begin{align*}\label{eq:1-1}\tag{1-1}
  A_{11}x_1 + A_{12}x_2 + \cdots + A_{1n}x_n &= y_1 \\
  A_{21}x_1 + A_{22}x_2 + \cdots + A_{2n}x_n &= y_2 \\
  &\vdots \\
  A_{m1}x_1 + A_{m2}x_2 + \cdots + A_{mn}x_n &= y_m
\end{align*}
while $y_1, \ldots, y_m$ and $A_{ij}$, $1 \leq i \leq m, 1 \leq j \leq n$, are
given elements of $F$. We call \eqref{eq:1-1} a \textbf{system of $m$ linear
equations in $n$ unknowns}. Any $n$-tuple $(x_1, \ldots, x_n)$ of elements of
$F$ which satisfies each of the equations in \eqref{eq:1-1} is called a
\textbf{solution} of the system. If $y_1 = y_2 = \cdots = y_m = 0$, we say that
the system is \textbf{homogeneous}.

\paragraph{Equivalence.} Two systems of linear equations are \textbf{equivalent}
if each equation in each system is a linear combination of the equations in the
other system.

\begin{theorem}
  Equivalent systems of linear equations have exactly the same solutions.
\end{theorem}

\subsection{Matrices and Elementary Row Operations}

\paragraph{Matrix.} An $m \times n$ \textbf{matrix over the field $F$} is a
function $A$ from the set of pairs of integers $(i, j)$, $1 \leq i \leq m, 1
\leq j \leq n$, into the field $F$. The \textbf{entries} of the matrix $A$ are
the scalars $A(i, j) = A_{ij}$.

We can abbreviate the system \eqref{eq:1-1} by $AX = Y$ where
\begin{gather*}
  A = \begin{bmatrix}
    A_{11} & \cdots & A_{1n} \\
    \vdots & & \vdots \\
    A_{m1} & \cdots & A_{mn}
  \end{bmatrix}, \\
  X = \begin{bmatrix}
    x_1 \\
    \vdots \\
    x_n
  \end{bmatrix}
  \quad\text{ and }\quad
  Y = \begin{bmatrix}
    y_1 \\
    \vdots \\
    y_n
  \end{bmatrix}.
\end{gather*}
We call $A$ the \textbf{matrix of coefficients} of the system.

\paragraph{Elementary row operations (EROs).} Three types of EROs can be carried
out on a $m \times n$ matrix $A$ over the field $F$:
\begin{enumerate}
  \item multiplication of one row of $A$ by a non-zero scalar $c$;
  \item replacement of the $r$th row of $A$ by row $r + cs$, $c$ any scalar and
    $r \neq s$;
  \item interchange of two rows of $A$.
\end{enumerate}

\begin{theorem}
  To each ERO $e$ there corresponds an ERO $e_1$, of the same type as $e$, such
  that $e_1(e(A)) = e(e_1(A)) = A$ for each $A$.
\end{theorem}

If $A$ and $B$ are $m \times n$ matrices over the field $F$, we say that $B$
\textbf{is row-equivalent to} $A$ if $B$ can be obtained from $A$ by a finite
sequence of EROs.

\begin{theorem}
  If $A$ and $B$ are row-equivalent $m \times n$ matrices, the homogeneous
  systems of linear equations $AX = 0$ and $BX = 0$ have exactly the same
  solutions.
\end{theorem}

\paragraph{Row-reduced matrix.} An $m \times n$ matrix $R$ is called
\textbf{row-reduced} if:
\begin{enumerate}[(a)]
  \item the first non-zero entry in each non-zero row of $R$ is equal to 1;
  \item each column of $R$ which contains the leading non-zero entry of some
    row has all its other entries 0.
\end{enumerate}

\begin{theorem} \label{thm:4}
  Every $m \times n$ matrix over the field $F$ is row-equivalent to a
  row-reduced matrix.
\end{theorem}

\subsection{Row-Reduced Echelon Matrices}

\paragraph{Row-reduced echelon matrix.} An $m \times n$ matrix $R$ is called a
\textbf{row-reduced echelon matrix} if:
\begin{enumerate}[(a)]
  \item $R$ is row-reduced;
  \item every row of $R$ which has all its entries 0 occurs below every row
    which has a non-zero entry;
  \item if rows $1, \ldots, r$ are the non-zero rows of $R$, and if the leading
    non-zero entry of row $i$ occurs in column $k_i$, $i = 1, \ldots, r$, then
    $k_1 < k_2 < \cdots < k_r$.
\end{enumerate}

\begin{theorem}
  Every $m \times n$ matrix over the field $F$ is row-equivalent to a
  row-reduced echelon matrix (cf. \textbf{Theorem~\ref{thm:4}}).
\end{theorem}

\begin{theorem}
  If $A$ is an $m \times n$ matrix and $m < n$, then the homogeneous system of
  linear equations $AX = 0$ has a non-trivial solution.
\end{theorem}

\begin{theorem}
  If $A$ is an $n \times n$ matrix, then $A$ is row-equivalent to the $n \times
  n$ identity matrix iff the system of equations $AX = 0$ has only
  the trivial solution.
\end{theorem}

\paragraph{Augmented matrix.} The \textbf{augmented matrix} $A'$ of the system
$AX = Y$ is the $m \times (n + 1)$ matrix whose first $n$ columns are the
columns of $A$ and whose last column is $Y$.

\subsection{Matrix Multiplication}

\paragraph{Product.} Let $A$ be an $m \times n$ matrix over the field $F$ and
let $B$ be an $n \times p$ matrix over $F$. The \textbf{product} $AB$ is the $m
\times p$ matrix $C$ whose $i, j$ entry is \[
  C_{ij} = \sum_{r=1}^n A_{ir}B_{rj}.
\]

\begin{theorem}
  If $A, B, C$ are matrices over the field $F$ such that the products $BC$ and
  $A(BC)$ are defined, then so are the products $AB$, $(AB)C$ and \[
    A(BC) = (AB)C.
  \]
\end{theorem}

An $m \times n$ matrix is said to be an \textbf{elementary matrix} if it can be
obtained from the $m \times n$ identity matrix by means of a single ERO.

\begin{theorem}
  Let $e$ be an ERO and let $E$ be the $n \times m$ elementary matrix $E =
  e(I)$. Then, for every $m \times n$ matrix $A$, \[
    e(A) = EA.
  \]
\end{theorem}

\subsection{Invertible Matrices}

\paragraph{Inverses.} Let $A$ be an $n \times n$ matrix over the field $F$. An
$n \times n$ matrix $B$ such that $BA = I$ is called a \textbf{left inverse} of
$A$; an $n \times n$ matrix $B$ such that $AB = I$ is called a \textbf{right
inverse} of $A$. If $AB = BA = I$, then $B$ is called a \textbf{two-sided
inverse} (or simply \textbf{the inverse}) of $A$, denoted as $A^{-1}$, and $A$
is said to be \textbf{invertible}.

\begin{lemma*}
  If $A$ has a left inverse $B$ and a right inverse $C$, then $B = C$.
\end{lemma*}

\begin{theorem}
  Let $A$ and $B$ be $n \times n$ matrices over $F$.
  \begin{enumerate}[(i)]
    \item If $A$ is invertible, so is $A^{-1}$ and $(A^{-1})^{-1} = A$.
    \item If both $A$ and $B$ are invertible, so is $AB$, and $(AB)^{-1} =
      B^{-1}A^{-1}$.
  \end{enumerate}
\end{theorem}

\begin{theorem}
  An elementary matrix is invertible.
\end{theorem}

\begin{theorem}
  If $A$ is an $n \times n$ matrix, the following are equivalent.
  \begin{enumerate}[(i)]
    \item $A$ is invertible.
    \item $A$ is row-equivalent to the $n \times n$ identity matrix.
    \item $A$ is a product of elementary matrices.
  \end{enumerate}
\end{theorem}

\begin{corollary*}
  If $A$ is an invertible $n \times n$ matrix and if a sequence of EROs reduces
  $A$ to the identity, then that same sequence of operations when applied to $I$
  yields $A^{-1}$.
\end{corollary*}

\begin{theorem}
  For an $n \times n$ matrix $A$, the following are equivalent.
  \begin{enumerate}[(i)]
    \item $A$ is invertible.
    \item The homogeneous system $AX = 0$ has only the trivial solution $X = 0$.
    \item The system of equations $AX = Y$ has a solution $X$ for each $n \times
      1$ matrix $Y$.
  \end{enumerate}
\end{theorem}

\section{Vector Spaces}

\subsection{Vector Spaces}

\paragraph{Vector space.} A \textbf{vector space} consists of the following:
\begin{enumerate}
  \item a field $F$ of scalars;
  \item a set $V$ of vectors;
  \item vector addition which associates with each pair of vectors $\alpha,
    \beta \in V$ a vector $\alpha + \beta \in V$, in such a way that
    \begin{enumerate}[(a)]
      \item $\alpha + \beta = \beta + \alpha$;
      \item $\alpha + (\beta + \gamma) = (\alpha + \beta) + \gamma$;
      \item there is a unique vector $0 \in V$ such that $\alpha + 0 = \alpha$
        for all $\alpha \in V$,
      \item for each vector $\alpha \in V$ there is a unique vector $-a \in V$
        such that $\alpha + (-\alpha) = 0$;
    \end{enumerate}
  \item scalar multiplication which associates with each scalar $c \in F$ and
    vector $\alpha \in V$ a vector $ca \in V$, in such a way that
    \begin{enumerate}[(a)]
      \item $1\alpha = \alpha$ for every $\alpha \in V$;
      \item $(c_1c_2)\alpha = c_1(c_2\alpha)$;
      \item $c(\alpha + \beta) = c\alpha + c\beta$;
      \item $(c_1 + c_2)\alpha = c_1\alpha + c_2\alpha$.
    \end{enumerate}
\end{enumerate}

\paragraph{Linear combination.} A vector $\beta \in V$ is said to be a
\textbf{linear combination} of the vectors $\alpha_1, \ldots, \alpha_n \in V$
provided there exist scalars $c_1, \ldots, c_n \in F$ such that \[
  \beta = c_1\alpha_1 + \cdots + c_n\alpha_n = \sum_{i=1}^n c_i\alpha_i.
\]

\subsection{Subspaces}

\paragraph{Subspace.} Let $V$ be a vector space over the field $F$. A
\textbf{subspace} of $V$ is a subset $W$ of $V$ which is itself a vector space
over $F$ with the operations of vector addition and scalar multiplication on
$V$.

\setcounter{theorem}{0}
\begin{theorem}
  A non-empty subset $W$ of $V$ is a subspace of $V$ if and only if for each
  pair of vectors $\alpha, \beta \in W$ and each scalar $c \in F$ the vector
  $c\alpha + \beta$ is again in $W$.
\end{theorem}

\begin{theorem}
  Let $V$ be a vector space over the field $F$. The intersection of any
  collection of subspaces of $V$ is a subspace of $V$.
\end{theorem}

\paragraph{Subspace spanned by a set of vectors.} Let $S$ be a set of vectors
in a vector space $V$. The \textbf{subspace spanned} by $S$ is defined to be
the intersection $W$ of all subspaces of $V$ which contain $S$.

\begin{theorem}
  The subspace spanned by a non-empty subset $S$ of a vector space $V$ is the
  set of all linear combinations of vectors in $S$.
\end{theorem}

\paragraph{Sum of sets of vectors.} If $S_1, S_2, \ldots, S_k$ are subsets of a
vector space $V$, the set of all sums \[
  \alpha_1 + \alpha_2 + \cdots + \alpha_k
\] of vectors $\alpha_i \in S_i$ is called the \textbf{sum} of the subsets $S_1,
S_2, \ldots, S_k$ and is denoted by \[
  S_1 + S_2 + \cdots + S_k
\] or by \[
  \sum_{i=1}^k S_i.
\]

\end{document}
