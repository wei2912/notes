\documentclass{article}

\usepackage{amsmath,amssymb,amsthm}
\usepackage[shortlabels]{enumitem}

\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\nullity}{nullity}
\newtheorem{theorem}{Theorem}[section]
\newtheorem*{lemma*}{Lemma}

\title{Notes to ``Linear Algebra'', 2nd edition (Hoffman, Kunze)}
\author{Ng Wei En}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Linear Equations}

\setcounter{subsection}{1}
\subsection{Systems of Linear Equations}

\paragraph{System of linear equations.} Suppose $F$ is a field. We consider the
problem of finding $n$ scalars $x_1, \ldots, x_n \in F$ which satisfy the
conditions
\begin{align*}\label{eq:1-1}\tag{1-1}
  A_{11}x_1 + A_{12}x_2 + \cdots + A_{1n}x_n &= y_1 \\
  A_{21}x_1 + A_{22}x_2 + \cdots + A_{2n}x_n &= y_2 \\
  &\vdots \\
  A_{m1}x_1 + A_{m2}x_2 + \cdots + A_{mn}x_n &= y_m
\end{align*}
while $y_1, \ldots, y_m$ and $A_{ij}$, $1 \leq i \leq m, 1 \leq j \leq n$, are
given elements of $F$. We call \eqref{eq:1-1} a \textbf{system of $m$ linear
equations in $n$ unknowns}. Any $n$-tuple $(x_1, \ldots, x_n)$ of elements of
$F$ which satisfies each of the equations in \eqref{eq:1-1} is called a
\textbf{solution} of the system. If $y_1 = y_2 = \cdots = y_m = 0$, we say that
the system is \textbf{homogeneous}.

\paragraph{Equivalence.} Two systems of linear equations are \textbf{equivalent}
if each equation in each system is a linear combination of the equations in the
other system.

\begin{theorem}
  Equivalent systems of linear equations have exactly the same solutions.
\end{theorem}

\subsection{Matrices and Elementary Row Operations}

\paragraph{Matrix.} An $m \times n$ \textbf{matrix over the field $F$} is a
function $A$ from the set of pairs of integers $(i, j)$, $1 \leq i \leq m, 1
\leq j \leq n$, into the field $F$. The \textbf{entries} of the matrix $A$ are
the scalars $A(i, j) = A_{ij}$.

We can abbreviate the system \eqref{eq:1-1} by $AX = Y$ where
\begin{gather*}
  A = \begin{bmatrix}
    A_{11} & \cdots & A_{1n} \\
    \vdots & & \vdots \\
    A_{m1} & \cdots & A_{mn}
  \end{bmatrix}, \\
  X = \begin{bmatrix}
    x_1 \\
    \vdots \\
    x_n
  \end{bmatrix}
  \quad\text{ and }\quad
  Y = \begin{bmatrix}
    y_1 \\
    \vdots \\
    y_n
  \end{bmatrix}.
\end{gather*}
We call $A$ the \textbf{matrix of coefficients} of the system.

\paragraph{Elementary row operations (EROs).} Three types of EROs can be carried
out on a $m \times n$ matrix $A$ over the field $F$:
\begin{enumerate}
  \item multiplication of one row of $A$ by a non-zero scalar $c$;
  \item replacement of the $r$th row of $A$ by row $r + cs$, $c$ any scalar and
    $r \neq s$;
  \item interchange of two rows of $A$.
\end{enumerate}

\begin{theorem}
  To each ERO $e$ there corresponds an ERO $e_1$, of the same type as $e$, such
  that $e_1(e(A)) = e(e_1(A)) = A$ for each $A$.
\end{theorem}

If $A$ and $B$ are $m \times n$ matrices over the field $F$, we say that $B$
\textbf{is row-equivalent to} $A$ if $B$ can be obtained from $A$ by a finite
sequence of EROs.

\begin{theorem}
  If $A$ and $B$ are row-equivalent $m \times n$ matrices, the homogeneous
  systems of linear equations $AX = 0$ and $BX = 0$ have exactly the same
  solutions.
\end{theorem}

\paragraph{Row-reduced matrix.} An $m \times n$ matrix $R$ is called
\textbf{row-reduced} if:
\begin{enumerate}[(a)]
  \item the first non-zero entry in each non-zero row of $R$ is equal to 1;
  \item each column of $R$ which contains the leading non-zero entry of some
    row has all its other entries 0.
\end{enumerate}

\begin{theorem} \label{thm:4}
  Every $m \times n$ matrix over the field $F$ is row-equivalent to a
  row-reduced matrix.
\end{theorem}

\subsection{Row-Reduced Echelon Matrices}

\paragraph{Row-reduced echelon matrix.} An $m \times n$ matrix $R$ is called a
\textbf{row-reduced echelon matrix} if:
\begin{enumerate}[(a)]
  \item $R$ is row-reduced;
  \item every row of $R$ which has all its entries 0 occurs below every row
    which has a non-zero entry;
  \item if rows $1, \ldots, r$ are the non-zero rows of $R$, and if the leading
    non-zero entry of row $i$ occurs in column $k_i$, $i = 1, \ldots, r$, then
    $k_1 < k_2 < \cdots < k_r$.
\end{enumerate}

\begin{theorem}
  Every $m \times n$ matrix over the field $F$ is row-equivalent to a
  row-reduced echelon matrix (cf. \textbf{Theorem~\ref{thm:4}}).
\end{theorem}

\begin{theorem}
  If $A$ is an $m \times n$ matrix and $m < n$, then the homogeneous system of
  linear equations $AX = 0$ has a non-trivial solution.
\end{theorem}

\begin{theorem}
  If $A$ is an $n \times n$ matrix, then $A$ is row-equivalent to the $n \times
  n$ identity matrix iff the system of equations $AX = 0$ has only
  the trivial solution.
\end{theorem}

\paragraph{Augmented matrix.} The \textbf{augmented matrix} $A'$ of the system
$AX = Y$ is the $m \times (n + 1)$ matrix whose first $n$ columns are the
columns of $A$ and whose last column is $Y$.

\subsection{Matrix Multiplication}

\paragraph{Product.} Let $A$ be an $m \times n$ matrix over the field $F$ and
let $B$ be an $n \times p$ matrix over $F$. The \textbf{product} $AB$ is the $m
\times p$ matrix $C$ whose $i, j$ entry is \[
  C_{ij} = \sum_{r=1}^n A_{ir}B_{rj}.
\]

\begin{theorem}
  If $A, B, C$ are matrices over the field $F$ such that the products $BC$ and
  $A(BC)$ are defined, then so are the products $AB$, $(AB)C$ and \[
    A(BC) = (AB)C.
  \]
\end{theorem}

An $m \times n$ matrix is said to be an \textbf{elementary matrix} if it can be
obtained from the $m \times n$ identity matrix by means of a single ERO.

\begin{theorem}
  Let $e$ be an ERO and let $E$ be the $n \times m$ elementary matrix $E =
  e(I)$. Then, for every $m \times n$ matrix $A$, \[
    e(A) = EA.
  \]
\end{theorem}

\subsection{Invertible Matrices}

\paragraph{Inverses.} Let $A$ be an $n \times n$ matrix over the field $F$. An
$n \times n$ matrix $B$ such that $BA = I$ is called a \textbf{left inverse} of
$A$; an $n \times n$ matrix $B$ such that $AB = I$ is called a \textbf{right
inverse} of $A$. If $AB = BA = I$, then $B$ is called a \textbf{two-sided
inverse} (or simply \textbf{the inverse}) of $A$, denoted as $A^{-1}$, and $A$
is said to be \textbf{invertible}.

\begin{lemma*}
  If $A$ has a left inverse $B$ and a right inverse $C$, then $B = C$.
\end{lemma*}

\begin{theorem}
  Let $A$ and $B$ be $n \times n$ matrices over $F$.
  \begin{enumerate}[(i)]
    \item If $A$ is invertible, so is $A^{-1}$ and $(A^{-1})^{-1} = A$.
    \item If both $A$ and $B$ are invertible, so is $AB$, and $(AB)^{-1} =
      B^{-1}A^{-1}$.
  \end{enumerate}
\end{theorem}

\begin{theorem}
  An elementary matrix is invertible.
\end{theorem}

\begin{theorem}
  If $A$ is an $n \times n$ matrix, the following are equivalent:
  \begin{enumerate}[(i)]
    \item $A$ is invertible.
    \item $A$ is row-equivalent to the $n \times n$ identity matrix.
    \item $A$ is a product of elementary matrices.
  \end{enumerate}
\end{theorem}

If $A$ is an invertible $n \times n$ matrix and a sequence of EROs reduces $A$
to the identity, then that same sequence of operations when applied to $I$
yields $A^{-1}$.

\begin{theorem}
  If $A$ is an $n \times n$ matrix, the following are equivalent:
  \begin{enumerate}[(i)]
    \item $A$ is invertible.
    \item The homogeneous system $AX = 0$ has only the trivial solution $X = 0$.
    \item The system of equations $AX = Y$ has a solution $X$ for each $n \times
      1$ matrix $Y$.
  \end{enumerate}
\end{theorem}

\section{Vector Spaces}

\subsection{Vector Spaces}

\paragraph{Vector space.} A \textbf{vector space} consists of the following:
\begin{enumerate}
  \item a field $F$ of scalars;
  \item a set $V$ of vectors;
  \item vector addition which associates with each pair of vectors $\alpha,
    \beta \in V$ a vector $\alpha + \beta \in V$, in such a way that
    \begin{enumerate}[(a)]
      \item $\alpha + \beta = \beta + \alpha$;
      \item $\alpha + (\beta + \gamma) = (\alpha + \beta) + \gamma$;
      \item there is a unique vector $0 \in V$ such that $\alpha + 0 = \alpha$
        for all $\alpha \in V$,
      \item for each vector $\alpha \in V$ there is a unique vector $-a \in V$
        such that $\alpha + (-\alpha) = 0$;
    \end{enumerate}
  \item scalar multiplication which associates with each scalar $c \in F$ and
    vector $\alpha \in V$ a vector $ca \in V$, in such a way that
    \begin{enumerate}[(a)]
      \item $1\alpha = \alpha$ for every $\alpha \in V$;
      \item $(c_1c_2)\alpha = c_1(c_2\alpha)$;
      \item $c(\alpha + \beta) = c\alpha + c\beta$;
      \item $(c_1 + c_2)\alpha = c_1\alpha + c_2\alpha$.
    \end{enumerate}
\end{enumerate}

\paragraph{Linear combination.} A vector $\beta \in V$ is said to be a
\textbf{linear combination} of the vectors $\alpha_1, \ldots, \alpha_n \in V$
provided there exist scalars $c_1, \ldots, c_n \in F$ such that \[
  \beta = c_1\alpha_1 + \cdots + c_n\alpha_n = \sum_{i=1}^n c_i\alpha_i.
\]

\subsection{Subspaces}

\paragraph{Subspace.} Let $V$ be a vector space over the field $F$. A
\textbf{subspace} of $V$ is a subset $W$ of $V$ which is itself a vector space
over $F$ with the operations of vector addition and scalar multiplication on
$V$.

\begin{theorem}
  A non-empty subset $W$ of $V$ is a subspace of $V$ if and only if for each
  pair of vectors $\alpha, \beta \in W$ and each scalar $c \in F$ the vector
  $c\alpha + \beta$ is again in $W$.
\end{theorem}

\begin{theorem}
  Let $V$ be a vector space over the field $F$. The intersection of any
  collection of subspaces of $V$ is a subspace of $V$.
\end{theorem}

\paragraph{Subspace spanned by a set of vectors.} Let $S$ be a set of vectors
in a vector space $V$. The \textbf{subspace spanned} by $S$ is defined to be
the intersection $W$ of all subspaces of $V$ which contain $S$.

\begin{theorem}
  The subspace spanned by a non-empty subset $S$ of a vector space $V$ is the
  set of all linear combinations of vectors in $S$.
\end{theorem}

\subsection{Bases and Dimension}

\paragraph{Linear dependence.} Let $V$ be a vector space over $F$. A subset $S$
of $V$ is said to be \textbf{linearly dependent} if there exist distinct vectors
$\alpha_1, \alpha_2, \ldots, \alpha_n \in S$ and scalars $c_1, c_2, \ldots, c_n
\in F$, not all of which are 0, such that \[
  c_1\alpha_1 + c_2\alpha_2 + \cdots + c_n\alpha_n = 0.
\] A set which is not linearly dependent is called \textbf{linearly dependent}.

\paragraph{Basis.} Let $V$ be a vector space. A \textbf{basis} for $V$ is a
linearly independent set of vectors in $V$ which spans the space $V$. The space
$V$ is \textbf{finite-dimensional} if it has a finite basis.

\begin{theorem}
  Let $V$ be a vector space which is spanned by a finite set of vectors
  $\beta_1, \beta_2, \ldots, \beta_m$. Then any independent set of vectors in
  $V$ is finite and contains no more than $m$ elements.
\end{theorem}

For a finite-dimensional vector space $V$,
\begin{enumerate}
  \item Any two bases of $V$ have the same (finite) number of elements.
  \item Let $n = \dim V$. Then
    \begin{enumerate}[(a)]
      \item any subset of $V$ which contains more than $n$ vectors is linearly
        dependent;
      \item no subset of $V$ which contains less than $n$ vectors can span $V$.
    \end{enumerate}
\end{enumerate}

\begin{lemma*}
  Let $S$ be a linearly independent subset of a vector space $V$. Suppose
  $\beta$ is a vector in $V$ which is not in the subspace spanned by $S$. Then
  the set obtained by adjoining $\beta$ to $S$ is linearly independent.
\end{lemma*}

\begin{theorem}
  If $W$ is a subspace of a finite-dimensional vector space $V$, every linearly
  independent subset of $W$ is finite and is part of a (finite) basis for $W$.
\end{theorem}

For a finite-dimensional vector space $V$,
\begin{enumerate}
  \item If $W$ is a proper subspace of $V$, then $W$ is finite-dimensional and
    $\dim W < \dim V$.
  \item In $V$ every non-empty linearly independent set of vectors is part of a
    basis.
\end{enumerate}

It follows that an $n \times n$ matrix over a field $F$ is invertible if its row
vectors form a linearly independent set of vectors in $F^n$.

\begin{theorem}
  If $W_1$ and $W_2$ are finite-dimensional subspaces of a vector space $V$,
  then $W_1 + W_2$ is finite-dimensional and \[
    \dim W_1 + \dim W_2 = \dim (W_1 \cap W_2) + \dim (W_1 + W_2).
  \]
\end{theorem}

\subsection{Coordinates}

\paragraph{Ordered basis.} If $V$ is a finite-dimensional vector space, an
\textbf{ordered basis} for $V$ is a finite sequence of vectors which is linearly
independent and spans $V$.

Given $\alpha \in V$ and an ordered basis $B = \{\alpha_1, \ldots, \alpha_n\}$
for $V$, there is a unique $n$-tuple $(x_1, \ldots, x_n)$ of scalars such that
$\alpha = \sum_{i=1}^n x_ia_i$. $x_i$ is the \textbf{$i$th coordinate of
$\alpha$ relative to the ordered basis $B$}. The corresponding
\textbf{coordinate matrix} is \[
  X = \begin{bmatrix}
    x_1 \\
    \vdots \\
    x_n
  \end{bmatrix}.
\] which is often denoted as $[\alpha]_B$.

\begin{theorem}
  Let $V$ be an $n$-dimensional vector space over the field $F$, and let $B = \{
  \alpha_1, \ldots, \alpha_n\}$ and $B' = \{\alpha'_1, \ldots, \alpha'_n\}$ be
  two ordered bases of $V$. Then there is a unique, necessarily invertible, $n
  \times n$ matrix $P$ with entries in $F$ such that
  \begin{enumerate}[(i)]
    \item $[\alpha]_B = P[\alpha]_{B'}$
    \item $[\alpha]_{B'} = P^{-1}[\alpha]_B$
  \end{enumerate}
  for every vector $\alpha \in V$. The columns of $P$ are given by \[
    P_j = [\alpha'_j]_B,\quad j = 1, \ldots, n.
  \]
\end{theorem}

\begin{theorem}
  Suppose $P$ is an $n \times n$ invertible matrix over $F$. Let $V$ be an
  $n$-dimensional vector space over $F$, and let $B$ be an ordered basis of $V$.
  Then there is a unique ordered basis $B'$ of $V$ such that
  \begin{enumerate}[(i)]
    \item $[\alpha]_B = P[\alpha]_{B'}$
    \item $[\alpha]_{B'} = P^{-1}[\alpha]_B$
  \end{enumerate}
  for every vector $\alpha \in V$.
\end{theorem}

\subsection{Summary of Row-Equivalence}

\paragraph{Row space.} The \textbf{row space} of a $m \times n$ matrix $A$ over
the field $F$ is the subspace of $F^n$ spanned by its row vectors. The
\textbf{row rank} of $A$ is the dimension of its row space.

\begin{theorem}
  Row-equivalent matrices have the same row space.
\end{theorem}

\begin{theorem}
  Let $R$ be a non-zero row-reduced echelon matrix. Then the non-zero-row
  vectors of $R$ form a basis for the row space of $R$.
\end{theorem}

\begin{theorem}
  Let $m$ and $n$ be positive integers and let $F$ be a field. Suppose $W$ is a
  subspace of $F^n$ and $\dim W \leq m$. Then there is precisely one $m \times
  n$ row-reduced echelon matrix over $F$ which has $W$ as its row space.
\end{theorem}

If $A$ and $B$ are $m \times n$ matrices over the field $F$, the following
statements are equivalent:
\begin{enumerate}
  \item $A$ and $B$ are row-equivalent.
  \item $A$ and $B$ have the same row space.
  \item $B = PA$, where $P$ is an invertible $m \times m$ matrix.
\end{enumerate}

\section{Linear Transformations}

\subsection{Linear Transformation}

\paragraph{Linear transformation.} Let $V$ and $W$ be vector spaces over the
field $F$. A \textbf{linear transformation from $V$ into $W$} is a function $T$
from $V$ into $W$ such that \[
  T(c\alpha + \beta) = c(T\alpha) + T\beta
\] for all $\alpha, \beta \in V$ and all scalars $c \in F$.

\begin{theorem}
  Let $V$ be a finite-dimensional vector space over the field $F$ and let
  $\{\alpha_1, \ldots, \alpha_n\}$ be an ordered basis for $V$. Let $W$ be a
  vector space over the same field $F$ and let $\beta_1, \ldots, \beta_n$ be any
  vectors in $W$. Then there is precisely one linear transformation $T$ from $V$
  into $W$ such that \[
    T\alpha_j = \beta_j, \quad j = 1, \ldots, n.
  \]
\end{theorem}

\paragraph{Null space.} Let $V$ and $W$ be vector spaces over the field $F$ and
let $T$ be a linear transformation from $V$ into $W$. The \textbf{null space} of
$T$ is the set of all vectors $\alpha \in V$ such that $T\alpha = 0$.

If $V$ is finite-dimensional, the \textbf{rank} of $T$ is the dimension of the
range of $T$ and the \textbf{nullity} of $T$ is the dimension of the null space
of $T$.

\begin{theorem}
  Let $V$ and $W$ be vector spaces over the field $F$ and let $T$ be a linear
  transformation from $V$ into $W$. Suppose that $V$ is finite-dimensional. Then
  \[
    \rank(T) + \nullity(T) = \dim V.
  \]
\end{theorem}

\begin{theorem}
  If $A$ is an $m \times n$ matrix with entries in the field $F$, then \[
    \text{row rank }(A) = \text{column rank }(A).
  \]
\end{theorem}

\subsection{The Algebra of Linear Transformations}

\begin{theorem}
  Let $V$ and $W$ be vector spaces over the field $F$. Let $T$ and $U$ be linear
  transformations from $V$ into $W$, and $c$ be any element of $F$. The function
  $(T + cU)$ defined by \[
    (T + cU)(\alpha) = T\alpha + cU\alpha
  \] is a linear transformation from $V$ into $W$.

  The set of all linear transformations from $V$ into $W$, together with the
  addition and scalar multiplication defined above, is a vector space over the
  field $F$.
\end{theorem}

We denote the space of linear transformations from $V$ into $W$ by $L(V, W)$,
where $V$ and $W$ are vector spaces over the same field.

\begin{theorem}
  Let $V$ be an $n$-dimensional vector space over the field $F$, and let $W$ be
  an $m$-dimensional vector space over $F$. Then the space $L(V, W)$ is
  finite-dimensional and has dimension $mn$.
\end{theorem}

\begin{theorem}
  Let $V$, $W$, and $Z$ be vector spaces over the field $F$. Let $T$ be a linear
  transformation from $V$ into $W$ and $U$ a linear transformation from $W$ into
  $Z$. Then the composed function $UT$ defined by $(UT)(\alpha) = U(T(\alpha))$
  is a linear transformation from $V$ into $Z$.
\end{theorem}

\paragraph{Linear operator.} If $V$ is a vector space over the field $F$, a
\textbf{linear operator on $V$} is a linear transformation from $V$ into $V$.

\begin{lemma*}
  Let $V$ be a vector space over the field $F$; let $U$, $T_1$ and $T_2$ be
  linear operators on $V$; let $c$ be an element of $F$.
  \begin{enumerate}[(a)]
    \item $IU = UI = U$;
    \item $U(T_1 + T_2) = UT_1 + UT_2$; $(T_1 + T_2)U = T_1U + T_2U$;
    \item $c(UT_1) = (cU)T_1 = U(cT_1)$.
  \end{enumerate}
\end{lemma*}

\paragraph{Invertible.} The function $T$ from $V$ into $W$ is called
\textbf{invertible} if there exists a function $U$ from $W$ into $V$ such that
$UT$ is the identity function on $V$ and $TU$ is the identity function on $W$.

\begin{theorem}
  Let $V$ and $W$ be vector spaces over the field $F$ and let $T$ be a linear
  transformation from $V$ into $W$. If $T$ is invertible, then the inverse
  function $T^{-1}$ is a linear transformation from $W$ onto $V$.
\end{theorem}

\paragraph{Non-singular.} A linear transformation is defined as
\textbf{non-singular} if $T\gamma = 0 \implies \gamma = 0$, i.e. if the null
space of $T$ is $\{0\}$.

\begin{theorem}
  Let $T$ be a linear transformation from $V$ into $W$. Then $T$ is non-singular
  if and only if $T$ carries each linearly independent subset of $V$ onto a
  linearly independent subset of $W$.
\end{theorem}

\begin{theorem}
  Let $V$ and $W$ be finite-dimensional vector spaces over the field $F$ such
  that $\dim V = \dim W$. If $T$ is a linear transformation from $V$ into $W$,
  the following are equivalent:
  \begin{enumerate}[(i)]
    \item $T$ is invertible.
    \item $T$ is non-singular.
    \item $T$ is onto, that is, the range of $T$ is $W$.
  \end{enumerate}
\end{theorem}

\subsection{Isomorphism}

\paragraph{Isomorphism.} If $V$ and $W$ are vector spaces over the field $F$,
any one-one linear transformation $T$ of $V$ onto $W$ is called an
\textbf{isomorphism of $V$ onto $W$}.

\begin{theorem}
  Every $n$-dimensional vector space over the field $F$ is isomorphic to the
  space $F^n$.
\end{theorem}

\subsection{Representation of Transformations by Matrices}

\begin{theorem}
  Let $V$ be an $n$-dimensional vector space over the field $F$ and $W$ an
  $m$-dimensional vector space over $F$. Let $B$ be an ordered basis for $V$ and
  $B'$ be an ordered basis for $W$. For each linear transformation $T$ from $V$
  into $W$, there is an $m \times n$ matrix $A$ with entries in $F$ such that \[
    [T\alpha]_{B'} = A[\alpha]_B
  \] for every vector $\alpha \in V$. Furthermore, $T \to A$ is a one-one
  correspondence between the set of all linear transformations from $V$ into $W$
  and the set of all $m \times n$ matrices over the field $F$.
\end{theorem}

The matrix $A$ which is associated with $T$ is called the \textbf{matrix of $T$
relative to the ordered bases $B$, $B'$}.

\begin{theorem}
  Let $V$ be an $n$-dimensional vector space over the field $F$ and let $W$ be
  an $m$-dimensional vector space over $F$. For each pair of ordered bases $B$,
  $B'$ for $V$ and $W$ respectively, the function which assigns to a linear
  transformation $T$ its matrix relative to $B$, $B'$ is an isomorphism between
  the space $L(V, W)$ and the space of all $m \times n$ matrices over the field
  $F$.
\end{theorem} 

\begin{theorem}
  Let $V$, $W$ and $Z$ be finite-dimensional vector spaces over the field $F$;
  let $T$ be a linear transformation from $V$ into $W$ and $U$ a linear
  transformation from $W$ into $Z$. If $B$, $B'$, and $B''$ are ordered bases
  for the spaces $V$, $W$, and $Z$, respectively, if $A$ is the matrix of $T$
  relative to the pair $B$, $B'$, and $B$ is the matrix of $U$ relative to the
  pair $B'$, $B''$, then the matrix of the composition $UT$ relative to the pair
  $B$, $B''$ is the product matrix $C = BA$.
\end{theorem}

\begin{theorem}
  Let $V$ be a finite-dimensional vector space over the field $F$ and let \[
    B = \{\alpha_1, \ldots, \alpha_n\}\quad\text{and}\quad
    B' = \{\alpha'_1, \ldots, \alpha'_n\}.
  \] be ordered bases for $V$. Suppose $T$ is a linear operator on $V$. If \[
    P = \begin{bmatrix}
      P_1 & \ldots & P_n
    \end{bmatrix}
  \] is the $n \times n$ matrix with columns $P_j = [\alpha'_j]_B$, then \[
    [T]_{B'} = P^{-1}[T]_BP.
  \] Alternatively, if $U$ is the invertible operator on $V$ defined by
  $U\alpha_j = \alpha'_j$, $j = 1, \ldots, n$, then \[
    [T]_{B'} = [U]^{-1}_B [T]_B [U]_B.
  \]
\end{theorem}

\paragraph{Similarity.} Let $A$ and $B$ be $n \times n$ (square) matrices over
the field $F$. We say that \textbf{$B$ is similar to $A$ over $F$} if there is
an invertible $n \times n$ matrix $P$ over $F$ such that $B = P^{-1}AP$.

\end{document}
